#!/usr/bin/env python3
"""
web_osint_passive.py

Passive OSINT for URLs and emails (defensive use only).

Features:
- URL header fetch (GET)
- DNS lookups (A/AAAA/MX/NS/TXT)
- SPF extraction
- DMARC lookup
- Gravatar existence check
- Optional username->profile HEAD probes on common sites (--probe-usernames)
- Optional Shodan host lookup (requires python-shodan and SHODAN_API_KEY)
- Writes JSON output to file

Usage example:
  python web_osint_passive.py --url https://example.com --email alice@example.com --probe-usernames --out result.json
"""

import argparse
import json
import logging
import socket
import os
import hashlib
import time
from urllib.parse import urlparse

import requests
import dns.resolver

# Optional shodan
try:
    import shodan
except Exception:
    shodan = None

# ---------- config ----------
DEFAULT_TIMEOUT = 8
RATE_LIMIT_SLEEP = 0.25  # between HEAD probes
SOCIAL_SITES = [
    "https://twitter.com/{u}",
    "https://github.com/{u}",
    "https://www.facebook.com/{u}",
    "https://www.instagram.com/{u}",
    "https://www.linkedin.com/in/{u}",
    "https://www.reddit.com/user/{u}",
    "https://www.tiktok.com/@{u}",
    "https://gitlab.com/{u}"
]
SHODAN_API_KEY = os.getenv("SHODAN_API_KEY")

# ---------- logging ----------
logger = logging.getLogger("web_osint_passive")
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
ch.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
logger.addHandler(ch)

# ---------- helpers ----------
def safe_get(url, timeout=DEFAULT_TIMEOUT):
    try:
        r = requests.get(url, timeout=timeout, allow_redirects=True)
        return {"status_code": r.status_code, "headers": dict(r.headers)}
    except Exception as e:
        return {"error": str(e)}

def safe_head(url, timeout=DEFAULT_TIMEOUT):
    try:
        r = requests.head(url, timeout=timeout, allow_redirects=True)
        return {"status_code": r.status_code, "headers": dict(r.headers)}
    except Exception as e:
        return {"error": str(e)}

def dns_lookup(domain):
    out = {"A": [], "AAAA": [], "MX": [], "NS": [], "TXT": []}
    try:
        answers = dns.resolver.resolve(domain, "A")
        out["A"] = [r.address for r in answers]
    except Exception as e:
        out["A_error"] = str(e)
    try:
        answers = dns.resolver.resolve(domain, "AAAA")
        out["AAAA"] = [r.address for r in answers]
    except Exception:
        pass
    try:
        answers = dns.resolver.resolve(domain, "MX")
        out["MX"] = [str(r.exchange).rstrip(".") for r in answers]
    except Exception:
        pass
    try:
        answers = dns.resolver.resolve(domain, "NS")
        out["NS"] = [str(r.target).rstrip(".") for r in answers]
    except Exception:
        pass
    try:
        answers = dns.resolver.resolve(domain, "TXT")
        # TXT records can come back as lists of byte-strings
        txts = []
        for r in answers:
            try:
                txt = b"".join(r.strings).decode("utf-8", errors="ignore")
            except Exception:
                try:
                    # fallback: join as str
                    txt = " ".join([t.decode("utf-8", errors="ignore") if isinstance(t, bytes) else str(t) for t in r.strings])
                except Exception:
                    txt = str(r)
            txts.append(txt)
        out["TXT"] = txts
    except Exception:
        pass
    return out

def extract_spf_from_txt(txt_records):
    for t in txt_records:
        if isinstance(t, str) and t.lower().startswith("v=spf1"):
            return t
    return None

def dmarc_lookup(domain):
    try:
        answers = dns.resolver.resolve(f"_dmarc.{domain}", "TXT")
        vals = []
        for r in answers:
            try:
                v = b"".join(r.strings).decode("utf-8", errors="ignore")
            except Exception:
                v = str(r)
            vals.append(v)
        return vals
    except Exception as e:
        return {"error": str(e)}

def gravatar_exists(email):
    e = (email or "").strip().lower()
    if not e or "@" not in e:
        return {"error": "invalid email"}
    h = hashlib.md5(e.encode("utf-8")).hexdigest()
    url = f"https://www.gravatar.com/avatar/{h}?d=404"
    try:
        r = requests.get(url, timeout=DEFAULT_TIMEOUT)
        return {"status_code": r.status_code, "exists": r.status_code == 200, "url": url}
    except Exception as e:
        return {"error": str(e)}

def shodan_lookup(ip):
    if not SHODAN_API_KEY:
        return {"error": "SHODAN_API_KEY not set"}
    if shodan is None:
        return {"error": "python-shodan not installed"}
    try:
        api = shodan.Shodan(SHODAN_API_KEY)
        host = api.host(ip)
        # Keep output readable
        return {
            "ip": host.get("ip_str", ip),
            "org": host.get("org"),
            "os": host.get("os"),
            "ports": host.get("ports"),
            "vulns": host.get("vulns"),
            "raw_count": len(host.get("data", []))
        }
    except Exception as e:
        return {"error": str(e)}

def probe_usernames(localpart, sites=SOCIAL_SITES, timeout=DEFAULT_TIMEOUT):
    results = []
    u = localpart
    for template in sites:
        url = template.format(u=u)
        res = safe_head(url, timeout=timeout)
        results.append({"site": template, "url": url, "result": res})
        time.sleep(RATE_LIMIT_SLEEP)
    return results

# ---------- main gather functions ----------
def gather_main_osint_info(url):
    logger.info("[OSINT] Gathering headers/DNS/Shodan for: %s", url)
    out = {"url": url, "headers": {}, "dns": {}, "ip": None, "shodan": None}
    # headers
    try:
        r = requests.get(url, timeout=DEFAULT_TIMEOUT)
        out["headers"] = dict(r.headers)
    except Exception as e:
        out["headers_error"] = str(e)
    # domain extraction
    parsed = urlparse(url)
    domain = parsed.netloc or parsed.path
    if ":" in domain:
        domain = domain.split(":")[0]
    out["dns"] = dns_lookup(domain)
    # resolve ip
    try:
        ip = socket.gethostbyname(domain)
        out["ip"] = ip
    except Exception as e:
        out["ip_error"] = str(e)
    # shodan
    if out.get("ip"):
        out["shodan"] = shodan_lookup(out["ip"])
    return out

def gather_email_osint(email, probe_usernames_flag=False):
    logger.info("[EMAIL-OSINT] Gathering info for: %s", email)
    out = {"email": email, "mx": [], "dns_domain": {}, "spf": None, "dmarc": None, "gravatar": None, "username_probes": []}
    if "@" not in (email or ""):
        out["error"] = "invalid email"
        return out
    localpart, domain = email.split("@", 1)
    # MX and other DNS for domain
    out["dns_domain"] = dns_lookup(domain)
    out["mx"] = out["dns_domain"].get("MX", [])
    out["spf"] = extract_spf_from_txt(out["dns_domain"].get("TXT", []))
    out["dmarc"] = dmarc_lookup(domain)
    # gravatar
    out["gravatar"] = gravatar_exists(email)
    # optional username probing
    if probe_usernames_flag:
        out["username_probes"] = probe_usernames(localpart)
    # shodan lookup for MX hosts' IPs (if desired)
    mx_shodan = {}
    for mx in out["mx"]:
        try:
            ip = socket.gethostbyname(mx)
            mx_shodan[mx] = {"ip": ip, "shodan": shodan_lookup(ip)}
        except Exception as e:
            mx_shodan[mx] = {"error": str(e)}
    out["mx_shodan"] = mx_shodan
    return out

# ---------- CLI ----------
def main():
    parser = argparse.ArgumentParser(description="Passive OSINT CLI for URLs and emails (defensive)")
    parser.add_argument("--url", help="Target URL (include scheme e.g. https://example.com)")
    parser.add_argument("--email", help="Email address to investigate (passive checks only)")
    parser.add_argument("--probe-usernames", action="store_true", help="Try HEAD requests for common profile URLs derived from email local-part")
    parser.add_argument("--out", help="Output JSON file path (default: timestamped file)", default=None)
    parser.add_argument("--log", choices=["debug","info","warning","error"], default="info", help="Console log level")
    args = parser.parse_args()

    # set log level
    ch.setLevel(getattr(logging, args.log.upper(), logging.INFO))

    results = {"meta": {"tool": "web_osint_passive"}, "main": None, "email": None}

    if args.url:
        results["main"] = gather_main_osint_info(args.url)
    if args.email:
        results["email"] = gather_email_osint(args.email, probe_usernames_flag=args.probe_usernames)

    if not args.out:
        import datetime
        host = (urlparse(args.url).netloc.replace(":", "_") if args.url else "no-url")
        args.out = f"web_osint_passive_{host}_{datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json"
    try:
        with open(args.out, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2)
        logger.info("Wrote output to %s", args.out)
    except Exception as e:
        logger.error("Failed to write output: %s", e)

if __name__ == "__main__":
    main()
