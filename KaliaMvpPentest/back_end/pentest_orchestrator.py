import json
import uuid
from datetime import datetime
from typing import Dict, Any, List, Union

import openai

#from G_L_L.l_region.Aggregation_area import correlator
from Network_Pentest.network.utils.cidr_helper import normalize_targets
from back_end.database import DatabaseManager
from back_end.mini_transformer.attack_surface_translator import AttackSurfaceTranslator
from back_end.mini_transformer.attack_surfaces import ATTACK_SURFACE_MAP
from back_end.utils.colored_logger import get_logger
from back_end.pentest_research import PentestResearchRAG

logger = get_logger(__name__, component="ORCHESTRATOR", region="CONTROLLER")
translator = AttackSurfaceTranslator()


class PentestOrchestrator:
    """
    AI-Orchestrator (Controller)
    ----------------------------
    - Dispatches network, web, or hybrid scans.
    - Expands and normalizes user requests.
    - For network scans, delegates execution to NetworkPipeline.
    - Performs RAG-based enrichment and contextualization.
    """

    def __init__(self,bus,network_pipeline):
        self.bus = bus
        self.db = DatabaseManager()
        self.rag = PentestResearchRAG()
        self.network_pipeline = network_pipeline
        self.regions = ["network", "web"]
        self.completed_tasks = []
        logger.info("[Orchestrator] Initialized. Ready for regions: %s", self.regions)

    # ------------------------------------------------------------
    #  Attack surface expansion utilities
    # ------------------------------------------------------------
    def _expand_full_surfaces(self, raw: List[str]) -> List[str]:
        """Expand 'full' to all known surfaces."""
        if any(x in ("full", "all_surfaces") for x in raw):
            try:
                return list(ATTACK_SURFACE_MAP.keys())
            except Exception:
                return [m["attack_surface"] for m in self.db.capability_matrix]
        return raw



    # ------------------------------------------------------------
    #  Main entry point
    # ------------------------------------------------------------
    async def submit_scan_request(
            self,
            region: str,
            scan_type: str,
            scope: str,
            target: Union[str, List[str]],
            #cross_domain_insight: bool = False,
            **kwargs,
    ) -> Dict[str, Any]:
        """
        Entry point from UI or CLI.
        Handles normalization, RAG enrichment, and dispatch to pipelines.
        """

        if region not in self.regions:
            raise ValueError(f"Unknown region '{region}'. Supported: {self.regions}")
        if not target:
            raise ValueError("Target required.")

        # Normalize target (single IP, range, or CIDR)
        subnetmask_enabled = kwargs.get("subnet_mask", False)
        explicit, retained = normalize_targets(target, subnetmask_enabled=subnetmask_enabled)
        target = explicit + retained
        ipv6_enabled = any(":" in t for t in target)

        # --- Scope & RAG reasoning ---
        research_notes = {"note": "Black-box minimal reconnaissance."}
        if scope == "white":
            research_notes = self.rag.research(f"What do you know about this codebase?") or research_notes
        elif scope == "grey":
            research_notes = self.rag.research(f"Grey-box {region} pentest for {target}") or research_notes

        # --- Surface expansion ---
        raw_surfaces = kwargs.get("attack_surface", ["full"])
        #expanded = self._expand_aspects_to_surfaces(self._expand_full_surfaces(raw_surfaces))

        # Enrich surfaces
        resolved_surface = []
        # for surf in expanded:
        #     matrix_def = next((m for m in self.db.capability_matrix if m.get("attack_surface") == surf), None)
        #     enrichment = self.rag.research(f"Explain risks and CVEs for {surf}.") if self.rag else {}
        #     resolved_surface.append({"surface": surf, "definition": matrix_def, "enrichment": enrichment})

        # --- Build task metadata ---
        task_id = str(uuid.uuid4())

        mode = kwargs.get("mode")
        if not mode:
            if "train" in (scan_type or "").lower() or kwargs.get("deploy_honeypot"):
                mode = "train"
            else:
                mode = "live"

        hints = {
            "scope": scope,
            "attack_surface": resolved_surface,
            "ipv6_enabled": ipv6_enabled,
            "explicit_surfaces": raw_surfaces != ["full"],  # marks if user manually picked surfaces
            "cross_domain_info": {},
            "mode": mode
        }

        task = {
            "id": task_id,
            "region": region,
            "action": f"initial_{region}_scan",
            "target": target,
            "scan_type": scan_type,
            "scope": scope,
            "research": research_notes,
            "hints": hints,
            "created_at": datetime.now().isoformat(),
        }

        # Save initial task record
        self.db.standard_db.insert_one({
            "_id": task_id,
            "task": task,
            "status": "queued",
            "created_at": datetime.now()
        })

        # Optional: correlate cross-domain learning
        #if cross_domain_insight:
           # events = list(self.db.learning_data.find({}))
            #hints["cross_domain_info"] = correlator.compute_cross_domain_potential(events)

        # --- Dispatch ---
        logger.info("[Orchestrator] Dispatching %s (%s targets)", task_id, len(target))
        if region == "network":
            await self._dispatch_network_pipeline(task, target)
        else:
            logger.warning("[Orchestrator] Region '%s' not yet implemented.", region)

        return {
            "status": "accepted",
            "task_id": task_id,
            "targets": target,
            "surfaces": [s["surface"] for s in resolved_surface],
            "message": f"Network pipeline initiated with {len(target)} targets."
        }

    # ------------------------------------------------------------
    # Network pipeline dispatch logic
    # ------------------------------------------------------------
    async def _dispatch_network_pipeline(self, task: Dict[str, Any], targets: List[str]):
        """Decide whether to skip recon or run full mapping."""
        hints = task["hints"]
        task_id = task["id"]

        logger.info(f"[Orchestrator] Dispatching NetworkPipeline for task {task_id}")
        # Explicit surfaces means user knows what to test → skip recon
        if hints.get("explicit_surfaces"):
            logger.info(f"[Orchestrator] Task {task_id}: explicit surfaces defined → skipping discovery.")
            await self.network_pipeline.run(targets, task_id=task_id, hints=hints)
            return

        # Full CIDR or multi-host → perform recon and mapping
        logger.info(f"[Orchestrator] Task {task_id}: initiating full recon + mapping pipeline.")
        await self.network_pipeline.run(targets, task_id=task_id, hints=hints)

    # ------------------------------------------------------------
    # Completion callback
    # ------------------------------------------------------------
    def handle_completed_task(self, task_result: Dict[str, Any]):
        """Triggered when downstream components signal completion."""
        self.completed_tasks.append(task_result)
        logger.info("[Orchestrator] Task %s completed.", task_result.get("id"))

async def submit_ui_request(self, ui_payload: dict):
        """
        Entry point for UI interaction.
        Handles both MANUAL mode and LLM (free-text) mode.
        Converts everything into a structured request compatible with
        submit_scan_request().
        """

        mode = ui_payload.get("mode", "manual")   # "manual" or "llm"

        if mode == "manual":
            # The UI already provided a structured request
            structured = ui_payload["structured_request"]

        else:
            # LLM must interpret the user's natural text
            user_text = ui_payload.get("user_text")
            if not user_text:
                raise ValueError("LLM mode requested but no 'user_text' provided.")

            structured = self.interpret_user_request(user_text)

        # Now the orchestrator continues with your existing standardized flow:
        return await self.submit_scan_request(
            region=structured["region"],
            scan_type=structured["scan_type"],
            scope=structured["scope"],
            target=structured["target"],
            attack_surface=structured.get("attack_surface", ["full"]),
            subnet_mask=structured.get("subnet_mask", False),
            mode=structured.get("mode", "live")
        )

def interpret_user_request(self, text: str) -> dict:
    """
    Use OpenAI to convert natural language into a structured pentest request.
    The LLM must return STRICT JSON.
    """

    system_prompt = """
        You are a pentest request interpreter. Your job is to convert NATURAL LANGUAGE
        into a STRUCTURED pentesting task.

        OUTPUT STRICT JSON ONLY. No explanation, no comments.

        SCHEMA:
        {
            "region": "network" | "web",
            "scan_type": "external" | "internal",
            "scope": "black" | "grey" | "white",
            "target": [ "IP" | "CIDR" | "domain" ],
            "attack_surface": ["full"] | [string,...],
            "subnet_mask": true | false,
            "mode": "live" | "train"
        }

        RULES:
        - If user does not specify, default:
          region = "network"
          scan_type = "external"
          scope = "black"
          attack_surface = ["full"]
          subnet_mask = false
          mode = "live"

        - Convert ranges like "10.0.0.5-10" into CIDR or list of IPs.
        - If user mentions "internal", or "inside the LAN", set scan_type="internal".
        - If user provides credentials or AD info → scope="grey".
        - If user mentions source code → scope="white".
        - If user mentions a single IP → target=[IP].
        - If user provides multiple IPs → return a list.
        """

    user_prompt = f"User request:\n{text}\n\nReturn STRICT JSON only."

    # ----------------------------
    # Try up to 3 times (LLM sometimes returns invalid JSON)
    # ----------------------------
    for _ in range(3):
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=300
            )

            raw = response.choices[0].message["content"]

            # Ensure strict JSON parsing
            structured = json.loads(raw)

            # Return validated JSON
            return structured

        except json.JSONDecodeError:
            # Force retry with a JSON repair instruction
            user_prompt = f"""
                The previous response was not valid JSON.

                FIX IT. Output STRICT VALID JSON ONLY.
                User request:
                {text}
                """

    # ----------------------------
    # If still invalid after retries → safe fallback
    # ----------------------------
    return {
        "region": "network",
        "scan_type": "external",
        "scope": "black",
        "target": ["8.8.8.8"],
        "attack_surface": ["full"],
        "subnet_mask": False,
        "mode": "live"
    }
