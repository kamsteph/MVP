# back_end/Regions/l_region/local_learner.py
import logging
import time
from typing import List, Dict, Any

from G_L_L.l_region.local_learner_helper import LocalHelper
from back_end.utils.colored_logger import get_logger
from back_end.database import DatabaseManager

logger = get_logger(__name__, component="LEARNER",region="LOCAL")
logger.setLevel(logging.INFO)

# Tunables
RECENT_WINDOW_SECONDS = 60 * 60 * 6  # keep events within last 6 hours for dedupe
DUPLICATE_WINDOW_SECONDS = 60 * 5    # 5 minutes dedupe window
MIN_INTEREST_SCORE = 0.3             # mini-predictor threshold to escalate

class LocalLearner:
    """
    Local learner for a specific region (Web, Network, etc.)
    - Noise filtering: dedupe/repeat suppression.
    - Context enrichment: add timestamp, region, severity heuristics.
    - Mini-predictors: compute quick interest score before sending to global learner.
    """
    def __init__(self, region: str, bus):
        self.region = region
        self.db=DatabaseManager()
        self.bus = bus
        self.local_knowledge = {}  # {target: {vuln_id: exploit_result}}
        self.recent_events = []    # small ring of recent events for dedupe checks
        self.helper=LocalHelper(db_manager=self.db)
        logger.info(f"[LocalLearner-{self.region}] Initialized.")
        # Subscribe to exploitation results pushed by DL/Exploitation Manager
        bus.subscribe("learn_from_exploitation", self._on_learn_from_exploitation)

    def _now_ts(self) -> float:
        return time.time()

    # ----------------- public handler -----------------
    def _on_learn_from_exploitation(self, exploit_results: List[Dict[str, Any]]):
        """
        Entry point called by the eventbus. This wraps the pipeline:
        normalize -> filter -> enrich -> mini_predict -> forward new_for_global
        """
        if not exploit_results:
            return

        normalized = self._normalize_input(exploit_results)
        filtered = self._filter_noise(normalized)
        enriched = [self._enrich_event(e) for e in filtered]

        # split: those that are interesting -> escalate; others we store locally
        to_escalate = []
        for e in enriched:
            score = self._mini_predictor(e)
            e["_interest_score"] = score
            if score >= MIN_INTEREST_SCORE:
                to_escalate.append(e)
            else:
                # update local knowledge (may still be useful)
                self._update_local_knowledge(e)

        if to_escalate:
            logger.info(f"[LocalLearner-{self.region}] Escalating {len(to_escalate)} events to global learner.")
            # assuming you pass a LocalHelper into LocalLearner on init
            enriched = [self._enrich_event(e) for e in filtered]
            enriched = [self.helper.enrich_with_adb_flag(e) for e in enriched]

            # publish to learning region (global learner subscribed)
            # use a generic event name; your system already listens for region 'learning'
            self.bus.publish_task({
                "region": "learning",
                "source_region": self.region,
                "action": "local_new_events",
                "events": to_escalate
            })

    # ----------------- normalization -----------------
    def _normalize_input(self, raw: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Normalize input shape: result might be a dict with 'target'/'vulnerabilities' or nested.
        Return list of minimal event dicts: {target, vuln, status, timestamp, raw}
        """
        out = []
        now = self._now_ts()
        for item in raw:
            # if already a batch event with multiple vulnerabilities, split per vuln
            target = item.get("target") or item.get("host") or item.get("ip") or item.get("target_ip")
            vulns = item.get("vulnerabilities") or []
            status = item.get("status", "unknown")
            ts = item.get("timestamp", now)

            if isinstance(vulns, list) and vulns:
                for v in vulns:
                    out.append({
                        "target": target,
                        "vuln": v,              # vuln is expected to be dict with id/alert/etc.
                        "status": status,
                        "timestamp": ts,
                        "raw": item
                    })
            else:
                out.append({
                    "target": target,
                    "vuln": item.get("vuln") or {},
                    "status": status,
                    "timestamp": ts,
                    "raw": item
                })
        return out

    # ----------------- noise filtering -----------------
    def _is_duplicate(self, e: Dict[str, Any]) -> bool:
        """
        Simple duplicate detection based on (target, vuln.id) within DUPLICATE_WINDOW_SECONDS.
        """
        now = self._now_ts()
        vid = e.get("vuln", {}).get("id") or e.get("vuln", {}).get("alert")
        tgt = e.get("target")
        for r in self.recent_events:
            age = now - r["timestamp"]
            if age > DUPLICATE_WINDOW_SECONDS:
                continue
            if r.get("target") == tgt:
                rvid = r.get("vuln", {}).get("id") or r.get("vuln", {}).get("alert")
                if rvid and vid and rvid == vid:
                    return True
        return False

    def _filter_noise(self, events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Remove duplicates and very old events; keep a sliding window of recents.
        """
        now = self._now_ts()
        filtered = []
        # prune recent_events to RECENT_WINDOW_SECONDS
        self.recent_events = [r for r in self.recent_events if now - r["timestamp"] <= RECENT_WINDOW_SECONDS]

        for e in events:
            # drop if extremely old
            if e.get("timestamp", now) < now - RECENT_WINDOW_SECONDS:
                continue
            if self._is_duplicate(e):
                logger.debug(f"[LocalLearner-{self.region}] Dropping duplicate event for {e.get('target')}.")
                continue
            # accept
            filtered.append(e)
            # add to recent_events for future dedupe
            self.recent_events.append(e)
        return filtered

    # ----------------- enrichment -----------------
    def _enrich_event(self, e: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add context fields: region, severity estimate, human friendly message.
        """
        vuln = e.get("vuln", {}) or {}
        alert = vuln.get("alert") or vuln.get("id") or "unknown"
        # rough severity heuristic: map strings or CVSS if present
        severity = vuln.get("cvss") if isinstance(vuln.get("cvss"), (int, float)) else None
        if severity is None:
            # string heuristics
            a = str(alert).lower()
            if any(x in a for x in ("critical", "rce", "remote code", "privilege")):
                severity = 9.0
            elif any(x in a for x in ("sql", "sqli", "xss", "csrf")):
                severity = 7.0
            else:
                severity = 4.0

        e_enriched = dict(e)  # shallow copy
        e_enriched.update({
            "region": self.region,
            "severity": float(severity),
            "received_at": self._now_ts(),
            "normalized_alert": alert,
            "human_summary": f"{self.region} event {alert} on {e.get('target')}"
        })
        return e_enriched

    # ----------------- mini predictor -----------------
    def _mini_predictor(self, e: Dict[str, Any]) -> float:
        """
        Very fast heuristic scoring to decide if event is interesting:
         - higher if severity high
         - higher if vuln previously unseen in local_knowledge
         - higher if evidence shows chained events in raw (e.g., raw contains 'rce' + 'auth')
        Returns float 0..1
        """
        score = 0.0
        sev = min(10.0, float(e.get("severity", 0)))
        score += (sev / 10.0) * 0.6  # severity weighted 60%

        # new vulnerability for this target increases score
        tgt = e.get("target")
        vid = e.get("vuln", {}).get("id") or e.get("vuln", {}).get("alert")
        if tgt and vid:
            known = self.local_knowledge.get(tgt, {})
            if vid not in known:
                score += 0.25

        # raw evidence keywords
        raw = e.get("raw", {})
        rawstr = str(raw).lower()
        if "rce" in rawstr or "auth_bypass" in rawstr or "privilege" in rawstr:
            score += 0.15

        # clamp
        return max(0.0, min(1.0, score))

    # ----------------- update local knowledge -----------------
    def _update_local_knowledge(self, e: Dict[str, Any]) -> None:
        tgt = e.get("target")
        vid = e.get("vuln", {}).get("id") or e.get("vuln", {}).get("alert")
        status = e.get("status")
        if not tgt or not vid:
            return
        if tgt not in self.local_knowledge:
            self.local_knowledge[tgt] = {}
        self.local_knowledge[tgt][vid] = {"status": status, "last_seen": e.get("timestamp")}

    def handle_exploitation_results(self, results):
        return self._on_learn_from_exploitation(results)