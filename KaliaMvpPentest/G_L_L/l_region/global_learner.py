from datetime import datetime

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import logging
from typing import List, Dict, Any

from back_end.database import DatabaseManager

logger = logging.getLogger("GNN_RLAgent")
logger.setLevel(logging.INFO)

class ExploitGNN(nn.Module):
    """
    Simple GCN to encode vulnerabilities, targets, exploits.
    """
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

class RLAgentGNN:
    """
    RL Agent using GNNs to manage exploitation, KVE generation, and vulnerability discovery.
    """
    def __init__(self, in_channels: int = 16, hidden_channels: int = 32, out_channels: int = 2):
        self.gnn = ExploitGNN(in_channels, hidden_channels, out_channels)
        self.optimizer = torch.optim.Adam(self.gnn.parameters(), lr=0.001)
        self.knowledge_graph = {}  # Stores KVEs, targets, exploits
        self.aggregator = None
        logger.info("[RLAgentGNN] Initialized.")

    def process_staging_to_adb(self, db: DatabaseManager):
        """
        Fetch raw learning data from staging area, aggregate it,
        and push structured vulnerabilities+exploits into ADB.
        """
        # 1) Fetch staging events
        staging_events = list(db.learning_data.find({}))
        if not staging_events:
            logger.info("[RLAgentGNN] No new staging events to process.")
            return

        logger.info(f"[RLAgentGNN] Retrieved {len(staging_events)} staging events.")

        # 2) Run aggregator
        aggregated = self.run_aggregation(staging_events)
        if not aggregated:
            logger.warning("[RLAgentGNN] Aggregator produced no output.")
            return

        # 3) Normalize into ADB schema
        for entry in aggregated:
            adb_doc = {
                "vulnerability": {
                    "cve_id": entry.get("cve") or entry.get("vuln_id", "UNKNOWN"),
                    "name": entry.get("name", "Unnamed Vulnerability"),
                    "severity": entry.get("severity", "medium"),
                    "cvss_score": entry.get("cvss", None),
                    "attack_vector": entry.get("vector", "unknown"),
                    "description": entry.get("description", "")
                },
                "exploits": entry.get("exploits", []),
                "tools": entry.get("tools", []),
                "source": "stage_learning_area",
                "region": entry.get("region", "unknown"),
                "processed": True,
                "created_at": datetime.now()
            }

            # 4) Save to ADB
            db.adb_collection.insert_one(adb_doc)
            logger.info(f"[RLAgentGNN] Stored vuln {adb_doc['vulnerability']} in ADB.")

    def attach_aggregator(self, aggregator):
        self.aggregator = aggregator

    def run_aggregation(self, events: List[Dict[str, Any]]):
        if not self.aggregator:
            logger.warning("[RLAgentGNN] No aggregator attached.")
            return None
        agg_result = self.aggregator.aggregate(events)
        logger.info("[RLAgentGNN] Aggregated events -> %s", agg_result)
        return agg_result

    # --- a. Learn from exploitation results ---
    def learn_from_exploitation(self, exploit_results: List[Dict[str, Any]]):
        """
        Construct a graph from exploitation results and train GNN.
        """
        if not exploit_results:
            return

        # Convert exploit results into nodes/features and edges
        nodes, edges, labels = self._build_graph_from_results(exploit_results)
        data = Data(x=torch.tensor(nodes, dtype=torch.float),
                    edge_index=torch.tensor(edges, dtype=torch.long),
                    y=torch.tensor(labels, dtype=torch.long))

        self.gnn.train()
        self.optimizer.zero_grad()
        out = self.gnn(data.x, data.edge_index)
        loss = F.nll_loss(out, data.y)
        loss.backward()
        self.optimizer.step()
        logger.info("[RLAgentGNN] Trained GNN on exploitation results.")

    # def _build_graph_from_results(self, results):
    #     # Each node: target + vulnerability + exploit outcome
    #     nodes = [[1.0] * 16 for _ in results]  # dummy features for now
    #     labels = [0 if r.get("status") == "success" else 1 for r in results]
    #
    #     # Build simple chain graph
    #     edge_list = []
    #     for i in range(len(results) - 1):
    #         edge_list.append([i, i + 1])
    #         edge_list.append([i + 1, i])
    #     edges = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
    #
    #     return nodes, edges, labels
    #
    # def _graph_from_kve(self, target_features):
    #     # Dummy graph for prediction
    #     nodes = [[1.0] * 16 for _ in range(5)]
    #     edge_list = [[0, 1], [1, 2], [2, 3], [3, 4]]
    #     edges = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
    #
    #     data = Data(
    #         x=torch.tensor(nodes, dtype=torch.float),
    #         edge_index=edges
    #     )
    #     return data

# --- b. Generate new exploits from known KVEs ---
# def predict_exploit(self, target_features: Dict[str, Any]) -> str:
#     """
#     Predict the next best exploit for a target using known KVEs.
#
#     The GNN produces per-node outputs (shape: [num_nodes, out_channels]).
#     We need a single graph-level action. Strategy:
#       - average the node logits across nodes to get a single vector of
#         class scores (graph-level).
#       - pick argmax on that vector to get the action index.
#     Returns a string identifier mapped from the action index.
#     """
#     self.gnn.eval()
#     graph_data = self._graph_from_kve(target_features)
#     with torch.no_grad():
#         out = self.gnn(graph_data.x, graph_data.edge_index)  # shape: [N, C]
#         # Aggregate per-node logits to get a graph-level score vector (C,)
#         graph_scores = out.mean(dim=0)             # shape: [C]
#         action_idx = int(graph_scores.argmax().item())
#     return f"KVE_exploit_{action_idx}"  # Map index to a concrete exploit
#
# def plan_discovery(self, target_features: Dict[str, Any]) -> List[str]:
#     """
#     Use GNN predictions and historical patterns to prioritize fuzzing or PoC.
#
#     Again, GNN returns per-node logits. We:
#       - average node logits -> graph-level scores
#       - take top-k class indices from the graph-level scores
#       - map indices to discovery action names
#
#     Returns up to 3 ordered string actions (may be fewer if out_channels < 3).
#     """
#     self.gnn.eval()
#     graph_data = self._graph_from_kve(target_features)
#     with torch.no_grad():
#         out = self.gnn(graph_data.x, graph_data.edge_index)  # [N, C]
#         graph_scores = out.mean(dim=0)                       # [C]
#         k = min(3, graph_scores.numel())
#         topk = graph_scores.topk(k).indices.tolist()         # list of top indices
#     return [f"discovery_action_{i}" for i in topk]

