# tests/unit/test_rlagent_gnn.py
"""
Unit tests for RLAgentGNN (back_end.GraphNN / RLAgentGNN)

Covers:
 - process_staging_to_adb (no events / with events + aggregator)
 - run_aggregation behavior (no aggregator vs aggregator attached)
 - learn_from_exploitation (training on a tiny dataset)
 - predict_exploit and plan_discovery outputs

Requirements:
    torch, torch_geometric
Run:
    pip install pytest pytest-mock torch torch_geometric  # as needed in your env
    pytest -q tests/unit/test_rlagent_gnn.py
"""

import pytest
from unittest.mock import MagicMock

import torch

from global_local_learner.l_region import RLAgentGNN


# Import the class under test



@pytest.fixture
def simple_db():
    """Return a fake DB object with the collections we need mocked."""
    fake_db = MagicMock()
    # learning_data.find should return an iterable (list)
    fake_db.learning_data = MagicMock()
    fake_db.adb_collection = MagicMock()
    return fake_db


def test_run_aggregation_without_aggregator_logs_warning(monkeypatch):
    """
    If no aggregator attached, run_aggregation should return None.
    """
    agent = RLAgentGNN()
    # ensure no aggregator
    agent.aggregator = None

    res = agent.run_aggregation([{"a": 1}])
    assert res is None


def test_attach_aggregator_and_run_aggregation(monkeypatch):
    """
    Attach a fake aggregator that returns a known aggregated list.
    run_aggregation should call aggregator.aggregate and return its result.
    """
    agent = RLAgentGNN()

    class FakeAgg:
        def aggregate(self, events):
            # return normalized aggregated entries
            return [{"cve": "CVE-FAKE", "name": "FakeVuln", "exploits": [], "tools": [], "region": "net"}]

    fake_agg = FakeAgg()
    agent.attach_aggregator(fake_agg)

    aggregated = agent.run_aggregation([{"x": 1}, {"x": 2}])
    assert isinstance(aggregated, list)
    assert aggregated[0]["cve"] == "CVE-FAKE"


def test_process_staging_to_adb_no_events(simple_db, monkeypatch):
    """
    When there are no staging events, process_staging_to_adb should early-return
    and not call adb_collection.insert_one.
    """
    agent = RLAgentGNN()
    # learning_data.find returns empty list
    simple_db.learning_data.find.return_value = []
    agent.process_staging_to_adb(simple_db)
    simple_db.adb_collection.insert_one.assert_not_called()


def test_process_staging_to_adb_with_events_and_aggregator(simple_db, monkeypatch):
    """
    With staged events and an attached aggregator -> process_staging_to_adb should:
      - call run_aggregation
      - write normalized documents into adb_collection.insert_one
    """
    agent = RLAgentGNN()

    # Prepare fake staged events and aggregator
    staged = [{"raw": "one"}, {"raw": "two"}]
    simple_db.learning_data.find.return_value = staged

    class FakeAgg:
        def aggregate(self, events):
            # produce two aggregated entries similar to expected schema
            return [
                {"cve": "CVE-1", "name": "Vuln1", "severity": "high", "cvss": 9.0, "exploits": [], "tools": [], "region": "network"},
                {"vuln_id": "V-2", "name": "Vuln2", "severity": "medium", "exploits": [], "tools": [], "region": "web"}
            ]

    agent.attach_aggregator(FakeAgg())

    # Execute
    agent.process_staging_to_adb(simple_db)

    # Should insert two docs into adb_collection
    assert simple_db.adb_collection.insert_one.call_count == 2
    # Inspect call args to ensure document shape includes expected keys
    first_doc = simple_db.adb_collection.insert_one.call_args_list[0][0][0]
    assert "vulnerability" in first_doc
    assert "source" in first_doc and first_doc["source"] == "stage_learning_area"


@pytest.mark.slow  # optional marker to separate slightly heavier GNN training test
def test_learn_from_exploitation_trains_on_small_batch(monkeypatch):
    """
    Provide a tiny batch of exploitation results (2 items so edges are created).
    The method should run without exception and perform an optimizer step.
    We check that parameters remain tensors and the method completes.
    """
    agent = RLAgentGNN(in_channels=16, hidden_channels=8, out_channels=2)

    # Build two fake exploit results to create a small graph with edges
    results = [
        {"status": "success", "meta": {}},
        {"status": "failed", "meta": {}}
    ]

    # Execute training - should not raise
    agent.learn_from_exploitation(results)

    # After training, ensure model parameters exist and are tensors
    params = list(agent.gnn.parameters())
    assert len(params) > 0
    # simple sanity: ensure parameters are torch tensors (via .data)
    assert all(hasattr(p, "data") for p in params)


def test_predict_and_plan_discovery_return_expected_types(monkeypatch):
    """
    predict_exploit should return a string exploit identifier,
    plan_discovery should return a list of strings.
    We monkeypatch internal _graph_from_kve to use a deterministic tiny graph
    so test is stable.
    """
    agent = RLAgentGNN(in_channels=16, hidden_channels=8, out_channels=3)

    # Monkeypatch _graph_from_kve to produce a small fixed Data object
    def fake_graph_from_kve(target_features):
        nodes = [[1.0] * 16 for _ in range(5)]
        edge_list = [[0, 1], [1, 2], [2, 3], [3, 4]]
        edges = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        data = type("D", (), {})()
        data.x = torch.tensor(nodes, dtype=torch.float)
        data.edge_index = edges
        return data

    monkeypatch.setattr(agent, "_graph_from_kve", fake_graph_from_kve)

    pred = agent.predict_exploit({"some": "features"})
    assert isinstance(pred, str) and pred.startswith("KVE_exploit_")

    plan = agent.plan_discovery({"some": "features"})
    assert isinstance(plan, list)
    assert all(isinstance(p, str) for p in plan)
    # plan_discovery returns up to 3 items; ensure it returns a list (possible empty)
    assert len(plan) <= 3


def test__build_graph_from_results_shapes_and_labels():
    """
    Validate the internal _build_graph_from_results builds node features, edges, and labels with expected
    sizes for a small input.
    """
    agent = RLAgentGNN(in_channels=16, hidden_channels=8, out_channels=2)
    results = [
        {"status": "success"},
        {"status": "failed"},
        {"status": "success"},
    ]
    nodes, edges, labels = agent._build_graph_from_results(results)

    # nodes -> list of length == len(results), each feature vector of length 16
    assert isinstance(nodes, list) and len(nodes) == len(results)
    assert all(len(n) == 16 for n in nodes)

    # edges -> torch tensor of shape [2, E], for 3 nodes E should be 4 edges (chain back/forth)
    assert hasattr(edges, "shape")
    assert edges.dim() == 2
    assert edges.shape[0] == 2

    # labels length equals results length
    assert isinstance(labels, list) and len(labels) == len(results)
    assert set(labels) <= {0, 1}
