# compute_semantic_temporal.py
import os
from datetime import datetime
import torch
from pymongo import UpdateOne
from sentence_transformers import SentenceTransformer

from back_end.database import DatabaseManager

MODEL_NAME = "all-MiniLM-L6-v2"
PROJECT_DIM = 64   # Project SBERT 384 -> 64 dims for GNN use

def parse_iso_to_ts(s):
    try:
        if not s:
            return None
        if s.endswith("Z"):
            s = s.replace("Z", "+00:00")
        return datetime.fromisoformat(s).timestamp()
    except Exception:
        try:
            return float(s)
        except Exception:
            return None

def compute_and_store(project_dim=PROJECT_DIM, batch_size=256):
    dbm = DatabaseManager()               # Unified DB connection
    col = dbm.adb_collection              # ADB = augmented_database
    model = SentenceTransformer(MODEL_NAME)

    projector = torch.nn.Linear(384, project_dim)
    projector_path = "./sbert_projector.pt"

    if os.path.exists(projector_path):
        projector.load_state_dict(torch.load(projector_path))

    cursor = col.find({}, {"summary": 1, "publishedDate": 1, "lastModifiedDate": 1})
    batch_docs, ids = [], []

    for doc in cursor:
        ids.append(doc["_id"])
        batch_docs.append(doc)
        if len(batch_docs) >= batch_size:
            _process_batch(batch_docs, ids, model, projector, col)
            ids, batch_docs = [], []

    if batch_docs:
        _process_batch(batch_docs, ids, model, projector, col)

    torch.save(projector.state_dict(), projector_path)
    print("Done computing embeddings & temporal features.")

def _process_batch(batch_docs, ids, model, projector, col):
    texts = [d.get("summary") or "" for d in batch_docs]
    emb = model.encode(texts, show_progress_bar=False)
    emb = torch.tensor(emb, dtype=torch.float32)
    projected = projector(emb).detach().numpy()

    updates = []
    for i, doc in enumerate(batch_docs):
        summary_emb = projected[i].tolist()
        published_ts = parse_iso_to_ts(doc.get("publishedDate"))
        modified_ts = parse_iso_to_ts(doc.get("lastModifiedDate"))
        now_ts = datetime.now().timestamp()

        days_since_published = None
        if published_ts:
            days_since_published = (now_ts - published_ts) / (24 * 3600)

        recency = 1.0
        if days_since_published is not None:
            recency = max(0.0, min(1.0, 1.0 - days_since_published / (10 * 365)))

        update = {
            "$set": {
                "summary_emb": summary_emb,
                "summary_emb_dim": len(summary_emb),
                "published_ts": published_ts,
                "last_modified_ts": modified_ts,
                "days_since_published": days_since_published,
                "recency": recency,
                "semantic_ingested_at": datetime.now()
            }
        }
        updates.append(UpdateOne({"_id": ids[i]}, update))

    if updates:
        col.bulk_write(updates, ordered=False)
        print(f"Updated {len(updates)} documents with embeddings.")
