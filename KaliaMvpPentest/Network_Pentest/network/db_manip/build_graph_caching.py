# build_graph_caching.py
import os
import math
import numpy as np
import torch
from pymongo import MongoClient
from tqdm import tqdm
from torch_geometric.data import Data
from urllib.parse import urlparse

from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
from back_end.database import DatabaseManager


def domain_from_url(url):
    try:
        p = urlparse(url)
        return p.netloc
    except Exception:
        return None

def build_graph_cache(vendor_mgr: VendorEmbeddingManager,
                      k_nn: int = 8,
                      sim_threshold: float = 0.25):
    dbm = DatabaseManager()               # Unified DB connection
    col = dbm.adb_collection              # ADB = augmented_database
    cursor = col.find({"summary_emb": {"$exists": True}})
    docs = list(cursor)
    N = len(docs)
    if N == 0:
        raise RuntimeError("No docs with summary_emb found. Run semantic pipeline first.")

    # 1) build per-node vectors
    node_vecs = []
    doc_ids = []
    domains_index = {}  # map domain -> list(node_idx)
    vendor_index = {}   # vendor -> list(node_idx)
    published_ts = []

    for i, d in enumerate(tqdm(docs, desc="build nodes")):
        doc_ids.append(d["_id"])
        # numeric features
        cvss = float(d.get("cvss") or 0.0)
        cvss_norm = cvss / 10.0
        has_exploit = 1.0 if d.get("has_exploit") else 0.0
        disputed = 1.0 if d.get("disputed") else 0.0
        recency = float(d.get("recency") or 0.0)
        ref_cnt = min(1.0, math.log1p(len(d.get("references") or [])) / 5.0)
        cwe_count = min(1.0, len(d.get("weaknesses") or []) / 5.0)
        # summary embedding (projected) stored already
        summary_emb = np.array(d.get("summary_emb") or [0.0]*64, dtype=np.float32)
        # vendor embedding lookup
        vendor_name = str(d.get("vendor") or "").lower()
        vendor_idx = vendor_mgr.get_index(vendor_name)
        vendor_vec = vendor_mgr.emb_table.weight[vendor_idx].detach().cpu().numpy()
        # final node vector (numeric + summary + vendor)
        node_vec = np.concatenate([
            np.array([cvss_norm, has_exploit, disputed, recency, ref_cnt, cwe_count], dtype=np.float32),
            summary_emb,
            vendor_vec.astype(np.float32)
        ], axis=0)
        node_vecs.append(node_vec)
        # domains
        for r in (d.get("references") or []):
            if isinstance(r, dict):
                url = r.get("url", "")
            elif isinstance(r, str):
                url = r
            else:
                continue
            dom = domain_from_url(url)
            if dom:
                domains_index.setdefault(dom, []).append(i)
        # vendor index
        vendor_index.setdefault(vendor_name, []).append(i)
        # published ts
        published_ts.append(float(d.get("published_ts") or 0.0))

    node_arr = np.stack(node_vecs, axis=0)  # shape (N, F)
    # 2) Build edges
    edges = set()
    edge_types = []  # optional track type ids: 0=same_vendor,1=shared_domain,2=temporal,3=knn

    # same_vendor edges
    for v, idxs in vendor_index.items():
        if len(idxs) < 2:
            continue
        for i in idxs:
            for j in idxs:
                if i == j: continue
                edges.add((i, j, 0))

    # shared reference domain edges
    for dom, idxs in domains_index.items():
        if len(idxs) < 2:
            continue
        for i in idxs:
            for j in idxs:
                if i == j: continue
                edges.add((i, j, 1))

    # temporal proximity: published within +/- 30 days
    sorted_idx = sorted(range(N), key=lambda k: published_ts[k] or 0)
    for a in range(N):
        for b in range(a+1, min(a+50, N)):  # local window to limit complexity
            i = sorted_idx[a]; j = sorted_idx[b]
            ta = published_ts[i]; tb = published_ts[j]
            if ta == 0 or tb == 0: continue
            if abs(ta - tb) <= 30*24*3600:
                edges.add((i, j, 2)); edges.add((j, i, 2))

    # embedding k-NN (cosine)
    X = node_arr.astype(np.float32)
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    norms[norms==0] = 1.0
    Xn = X / norms
    sim_mat = Xn @ Xn.T
    for i in range(N):
        topk = np.argsort(-sim_mat[i])[: k_nn+1]
        for j in topk:
            if j == i: continue
            if sim_mat[i,j] >= sim_threshold:
                edges.add((i, j, 3))

    # convert edges to tensors (we will encode edge_type as separate tensor)
    edge_index_list = []
    edge_type_list = []
    for (i,j,t) in edges:
        edge_index_list.append([i,j])
        edge_type_list.append(t)

    if len(edge_index_list) == 0:
        edge_index = torch.empty((2,0), dtype=torch.long)
        edge_type = torch.empty((0,), dtype=torch.long)
    else:
        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()
        edge_type = torch.tensor(edge_type_list, dtype=torch.long)

    # Node features tensor
    x = torch.tensor(node_arr, dtype=torch.float)

    # Option: build labels if you have supervisory signal (cvss bucket)
    # For example: high severity if cvss >= 7
    labels = []
    for d in docs:
        cvss = float(d.get("cvss") or 0.0)
        labels.append(1 if cvss >= 7.0 else 0)
    y = torch.tensor(labels, dtype=torch.long)

    data = Data(x=x, edge_index=edge_index, y=y)
    # attach edge_type metadata if desired
    data.edge_type = edge_type

    # cache in memory - simple approach: return data and mapping
    graph_cache = {
        "data": data,
        "doc_ids": doc_ids,
        "node_index": {i: doc_ids[i] for i in range(len(doc_ids))}
    }
    print(f"Built graph with {x.shape[0]} nodes and {edge_index.shape[1]} edges.")
    return graph_cache
