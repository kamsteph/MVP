import math
import numpy as np
import torch
import faiss
from tqdm import tqdm
from torch_geometric.data import Data
from urllib.parse import urlparse
import gc # Import garbage collector

from back_end.database import DatabaseManager


# Assuming VendorEmbeddingManager and DatabaseManager are available
# from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
# from back_end.database import DatabaseManager

def domain_from_url(url):
    try:
        p = urlparse(url)
        return p.netloc
    except Exception:
        return None

# New helper function to yield edges, replacing the large in-memory 'edges' set
def _generate_edges(vendor_index, domains_index, published_ts, N, node_arr_normalized, k_nn, sim_threshold):
    # 0. Same Vendor Edges (Type 0)
    for v, idxs in vendor_index.items():
        if len(idxs) < 2: continue
        for i in idxs:
            for j in idxs:
                if i != j: yield (i, j, 0)

    # 1. Shared Reference Domain Edges (Type 1)
    for dom, idxs in domains_index.items():
        if len(idxs) < 2: continue
        for i in idxs:
            for j in idxs:
                if i != j: yield (i, j, 1)

    # 2. Temporal Proximity Edges (Type 2)
    sorted_idx = sorted(range(N), key=lambda k: published_ts[k] or 0)
    for a in range(N):
        for b in range(a + 1, min(a + 50, N)):
            i = sorted_idx[a]; j = sorted_idx[b]
            ta = published_ts[i]; tb = published_ts[j]
            if ta == 0 or tb == 0: continue
            if abs(ta - tb) <= 30 * 24 * 3600:
                yield (i, j, 2); yield (j, i, 2)

    # 3. Embedding k-NN Edges (Type 3) - Uses FAISS
    for edge in build_knn_edges(node_arr_normalized, k_nn, sim_threshold):
        yield edge

    # Explicitly clear large temporary FAISS data
    del node_arr_normalized
    gc.collect()

def build_knn_edges(Xn, k_nn=8, sim_threshold=0.25):
    N, D = Xn.shape
    index = faiss.IndexFlatIP(D)  # Inner Product index
    index.add(Xn)

    # FAISS search is fast but uses memory for Dists/Neighbors arrays
    Dists, Neighbors = index.search(Xn, k_nn + 1)

    # Use a generator to yield edges without building an intermediate set
    for i in range(N):
        for j, score in zip(Neighbors[i], Dists[i]):
            if i != j and score >= sim_threshold:
                yield (i, j, 3)

    del index # Explicitly remove FAISS index
    del Dists
    del Neighbors
    gc.collect() # Trigger garbage collection

def build_graph_cache_optimized(vendor_mgr,
                                k_nn: int = 8,
                                sim_threshold: float = 0.25):
    """
    Construit un objet Data PyTorch Geometic de manière optimisée en mémoire.
    Filtre sur les CVEs 'network-related' et utilise NumPy pour la déduplication des arêtes.
    """
    dbm = DatabaseManager()
    col = dbm.adb_collection

    # Filtre MongoDB: CVEs réseau avec embeddings
    query = {
        "summary_emb": {"$exists": True},
        "is_network_related": True
    }

    # 1. Déterminer N et pré-allouer
    N = col.count_documents(query)
    if N == 0:
        raise RuntimeError("No network-related docs with summary_emb found. Run flag pipeline and semantic pipeline first.")

    vendor_dim = vendor_mgr.emb_table.weight.shape[1]
    feature_dim = 6 + 64 + vendor_dim

    # Pré-allocation du tableau de caractéristiques final
    node_arr = np.empty((N, feature_dim), dtype=np.float32)
    doc_ids = []

    # Métadonnées en mémoire
    domains_index = {}
    vendor_index = {}
    published_ts = []
    cvss_scores = []

    # 2. Itérer (streaming) et remplir
    cursor = col.find(query)

    for i, d in enumerate(tqdm(cursor, total=N, desc="build network nodes (streaming)")):
        doc_ids.append(d["_id"])

        # --- Calcul des caractéristiques ---
        cvss = float(d.get("cvss") or 0.0)
        cvss_scores.append(cvss)

        cvss_norm = cvss / 10.0
        has_exploit = 1.0 if d.get("has_exploit") else 0.0
        disputed = 1.0 if d.get("disputed") else 0.0
        recency = float(d.get("recency") or 0.0)
        ref_cnt = min(1.0, math.log1p(len(d.get("references") or [])) / 5.0)
        cwe_count = min(1.0, len(d.get("weaknesses") or []) / 5.0)
        summary_emb = np.array(d.get("summary_emb") or [0.0]*64, dtype=np.float32)
        vendor_name = str(d.get("vendor") or "").lower()
        vendor_idx = vendor_mgr.get_index(vendor_name)
        vendor_vec = vendor_mgr.emb_table.weight[vendor_idx].detach().cpu().numpy()

        node_vec = np.concatenate([
            np.array([cvss_norm, has_exploit, disputed, recency, ref_cnt, cwe_count], dtype=np.float32),
            summary_emb,
            vendor_vec.astype(np.float32)
        ], axis=0)

        node_arr[i] = node_vec

        # --- Construction des index d'arêtes (pour la génération des arêtes) ---
        for r in (d.get("references") or []):
            url = r.get("url", "") if isinstance(r, dict) else r if isinstance(r, str) else None
            dom = domain_from_url(url)
            if dom:
                domains_index.setdefault(dom, []).append(i)

        vendor_index.setdefault(vendor_name, []).append(i)
        published_ts.append(float(d.get("published_ts") or 0.0))

    del cursor
    gc.collect()

    # 3. Normaliser node_arr pour k-NN
    X = node_arr.astype(np.float32)
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    norms[norms==0] = 1.0
    Xn = X / norms

    # 4. Générer toutes les arêtes (y compris les doublons)
    edge_generator = _generate_edges(vendor_index, domains_index, published_ts, N, Xn, k_nn, sim_threshold)

    # Collecte de toutes les arêtes générées dans une liste (prélude à NumPy)
    all_edges_list = []
    for i, j, t in tqdm(edge_generator, desc="collecting ALL generated edges"):
        all_edges_list.append([i, j, t])

    # Nettoyage des grandes structures intermédiaires après la génération
    del domains_index
    del vendor_index
    del published_ts
    del Xn
    gc.collect()

    # 5. Conversion en NumPy Array et Déduplication (Méthode NumPy optimisée)
    if not all_edges_list:
        edge_index = torch.empty((2,0), dtype=torch.long)
        edge_type = torch.empty((0,), dtype=torch.long)
    else:
        # 5a. Convertir en NumPy et libérer la liste Python
        edge_arr = np.array(all_edges_list, dtype=np.int32)
        del all_edges_list
        gc.collect()

        # 5b. Déduplication par tri et np.unique (plus efficace en mémoire que le set Python)
        unique_edge_arr = np.unique(edge_arr, axis=0)
        del edge_arr
        gc.collect()

        # 5c. Séparer et convertir en PyTorch tensors
        edge_index_data = unique_edge_arr[:, :2]
        edge_type_data = unique_edge_arr[:, 2]

        edge_index = torch.tensor(edge_index_data, dtype=torch.long).t().contiguous()
        edge_type = torch.tensor(edge_type_data, dtype=torch.long)

        del unique_edge_arr
        gc.collect()

    # 6. Finalisation du graphe
    x = torch.tensor(node_arr, dtype=torch.float)
    del node_arr
    gc.collect()

    # Construction des labels 'y'
    labels = [1 if cvss >= 7.0 else 0 for cvss in cvss_scores]
    y = torch.tensor(labels, dtype=torch.long)
    del cvss_scores

    data = Data(x=x, edge_index=edge_index, y=y)
    data.edge_type = edge_type

    graph_cache = {
        "data": data,
        "doc_ids": doc_ids,
        "node_index": {i: doc_ids[i] for i in range(len(doc_ids))}
    }
    print(f"Built NETWORK graph with {x.shape[0]} nodes and {edge_index.shape[1]} edges.")
    return graph_cache
# import os
# import math
# import numpy as np
# import torch
# import faiss
# from pymongo import MongoClient
# from tqdm import tqdm
# from torch_geometric.data import Data
# from urllib.parse import urlparse
#
# from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
# from back_end.database import DatabaseManager
#
#
# def domain_from_url(url):
#     try:
#         p = urlparse(url)
#         return p.netloc
#     except Exception:
#         return None

# def build_graph_cache(vendor_mgr: VendorEmbeddingManager,
#                       k_nn: int = 8,
#                       sim_threshold: float = 0.25):
#     dbm = DatabaseManager()               # Unified DB connection
#     col = dbm.adb_collection              # ADB = augmented_database
#     cursor = col.find({"summary_emb": {"$exists": True}})
#     docs = list(cursor)
#     N = len(docs)
#     if N == 0:
#         raise RuntimeError("No docs with summary_emb found. Run semantic pipeline first.")
#
#     # 1) build per-node vectors
#     node_vecs = []
#     doc_ids = []
#     domains_index = {}  # map domain -> list(node_idx)
#     vendor_index = {}   # vendor -> list(node_idx)
#     published_ts = []
#
#     for i, d in enumerate(tqdm(docs, desc="build nodes")):
#         doc_ids.append(d["_id"])
#         # numeric features
#         cvss = float(d.get("cvss") or 0.0)
#         cvss_norm = cvss / 10.0
#         has_exploit = 1.0 if d.get("has_exploit") else 0.0
#         disputed = 1.0 if d.get("disputed") else 0.0
#         recency = float(d.get("recency") or 0.0)
#         ref_cnt = min(1.0, math.log1p(len(d.get("references") or [])) / 5.0)
#         cwe_count = min(1.0, len(d.get("weaknesses") or []) / 5.0)
#         # summary embedding (projected) stored already
#         summary_emb = np.array(d.get("summary_emb") or [0.0]*64, dtype=np.float32)
#         # vendor embedding lookup
#         vendor_name = str(d.get("vendor") or "").lower()
#         vendor_idx = vendor_mgr.get_index(vendor_name)
#         vendor_vec = vendor_mgr.emb_table.weight[vendor_idx].detach().cpu().numpy()
#         # final node vector (numeric + summary + vendor)
#         node_vec = np.concatenate([
#             np.array([cvss_norm, has_exploit, disputed, recency, ref_cnt, cwe_count], dtype=np.float32),
#             summary_emb,
#             vendor_vec.astype(np.float32)
#         ], axis=0)
#         node_vecs.append(node_vec)
#         # domains
#         for r in (d.get("references") or []):
#             if isinstance(r, dict):
#                 url = r.get("url", "")
#             elif isinstance(r, str):
#                 url = r
#             else:
#                 continue
#             dom = domain_from_url(url)
#             if dom:
#                 domains_index.setdefault(dom, []).append(i)
#         # vendor index
#         vendor_index.setdefault(vendor_name, []).append(i)
#         # published ts
#         published_ts.append(float(d.get("published_ts") or 0.0))
#
#     node_arr = np.stack(node_vecs, axis=0)  # shape (N, F)
#     # 2) Build edges
#     edges = set()
#     edge_types = []  # optional track type ids: 0=same_vendor,1=shared_domain,2=temporal,3=knn
#
#     # same_vendor edges
#     for v, idxs in vendor_index.items():
#         if len(idxs) < 2:
#             continue
#         for i in idxs:
#             for j in idxs:
#                 if i == j: continue
#                 edges.add((i, j, 0))
#
#     # shared reference domain edges
#     for dom, idxs in domains_index.items():
#         if len(idxs) < 2:
#             continue
#         for i in idxs:
#             for j in idxs:
#                 if i == j: continue
#                 edges.add((i, j, 1))
#
#     # temporal proximity: published within +/- 30 days
#     sorted_idx = sorted(range(N), key=lambda k: published_ts[k] or 0)
#     for a in range(N):
#         for b in range(a+1, min(a+50, N)):  # local window to limit complexity
#             i = sorted_idx[a]; j = sorted_idx[b]
#             ta = published_ts[i]; tb = published_ts[j]
#             if ta == 0 or tb == 0: continue
#             if abs(ta - tb) <= 30*24*3600:
#                 edges.add((i, j, 2)); edges.add((j, i, 2))
#
#     # embedding k-NN (cosine)
#     X = node_arr.astype(np.float32)
#     norms = np.linalg.norm(X, axis=1, keepdims=True)
#     norms[norms==0] = 1.0
#     Xn = X / norms
#     # sim_mat = Xn @ Xn.T
#     # for i in range(N):
#     #     topk = np.argsort(-sim_mat[i])[: k_nn+1]
#     #     for j in topk:
#     #         if j == i: continue
#     #         if sim_mat[i,j] >= sim_threshold:
#     #             edges.add((i, j, 3))
#
#     edges.update(build_knn_edges(Xn, k_nn, sim_threshold))
#
#     # convert edges to tensors (we will encode edge_type as separate tensor)
#     edge_index_list = []
#     edge_type_list = []
#     for (i,j,t) in edges:
#         edge_index_list.append([i,j])
#         edge_type_list.append(t)
#
#     if len(edge_index_list) == 0:
#         edge_index = torch.empty((2,0), dtype=torch.long)
#         edge_type = torch.empty((0,), dtype=torch.long)
#     else:
#         edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()
#         edge_type = torch.tensor(edge_type_list, dtype=torch.long)
#
#     # Node features tensor
#     x = torch.tensor(node_arr, dtype=torch.float)
#
#     # Option: build labels if you have supervisory signal (cvss bucket)
#     # For example: high severity if cvss >= 7
#     labels = []
#     for d in docs:
#         cvss = float(d.get("cvss") or 0.0)
#         labels.append(1 if cvss >= 7.0 else 0)
#     y = torch.tensor(labels, dtype=torch.long)
#
#     data = Data(x=x, edge_index=edge_index, y=y)
#     # attach edge_type metadata if desired
#     data.edge_type = edge_type
#
#     # cache in memory - simple approach: return data and mapping
#     graph_cache = {
#         "data": data,
#         "doc_ids": doc_ids,
#         "node_index": {i: doc_ids[i] for i in range(len(doc_ids))}
#     }
#     print(f"Built graph with {x.shape[0]} nodes and {edge_index.shape[1]} edges.")
#     return graph_cache
#
# def build_knn_edges(Xn, k_nn=8, sim_threshold=0.25):
#     N, D = Xn.shape
#     index = faiss.IndexFlatIP(D)  # Inner Product index (cosine if normalized)
#     index.add(Xn)
#     Dists, Neighbors = index.search(Xn, k_nn + 1)
#
#     edges = set()
#     for i in range(N):
#         for j, score in zip(Neighbors[i], Dists[i]):
#             if i == j:
#                 continue
#             if score >= sim_threshold:
#                 edges.add((i, j, 3))
#     return edges