# ==============================
# MAIN RUNNER (Avec BATCH_SIZE r√©duit et flush=True)
# ==============================
import os
import sys
import torch
from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
import math
from tqdm import tqdm

# --- ASSUMPTION ---
# Assurez-vous que 'build_graph_cache_partial' est correctement import√©
try:
    from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache_partial as build_graph_function
except ImportError:
    print("FATAL ERROR: 'build_graph_cache_partial' n'a pas pu √™tre import√©.", flush=True)
    sys.exit(1)


# Cette ligne ajoute le r√©pertoire parent au chemin d'importation.
sys.path.append("/MVP/KaliaMvpPentest")


if __name__ == "__main__":

    print("=== BUILD GRAPH CACHE STARTED ===", flush=True)

    # Param√®tres de construction du graphe
    # -----------------------------------------------------------------
    # CORRECTION CRUCIALE : R√©duction du BATCH_SIZE pour √©viter l'OOM Killer
    BATCH_SIZE = 2500
    # -----------------------------------------------------------------
    K_NN = 8
    SIM_THRESHOLD = 0.25

    # Cr√©ation du dossier de cache (avant de commencer)
    BATCHES_DIR = "./cache/batches"
    os.makedirs(BATCHES_DIR, exist_ok=True)

    # Variables de la boucle et compteurs globaux
    start_index = 0
    batch_count = 0
    total_nodes = 0
    total_edges = 0

    # 1. Initialisation du Vendor Manager
    vendor_mgr = VendorEmbeddingManager(
        initial_vendor_list="./data/vendors.txt",
        emb_dim=16,
        device="cuda"
    )

    # 2. Boucle de construction du graphe par lots (Mini-Batching de construction)
    while True:
        print(f"\n--- üèóÔ∏è Processing Batch {batch_count + 1}: Starting at index {start_index} ---", flush=True)

        # Le skip et limit de MongoDB g√®rent le mini-batching ici
        graph_cache = build_graph_function(
            vendor_mgr=vendor_mgr,
            k_nn=K_NN,
            sim_threshold=SIM_THRESHOLD,
            skip=start_index,
            limit=BATCH_SIZE
        )

        if graph_cache is None:
            # La fonction retourne None quand il n'y a plus de documents
            break

        # 3. Sauvegarde imm√©diate du lot (Lib√©ration de la m√©moire)

        data = graph_cache["data"]

        # Mise √† jour des statistiques globales avant la sauvegarde
        nodes_in_batch = data.num_nodes
        edges_in_batch = data.num_edges
        total_nodes += nodes_in_batch
        total_edges += edges_in_batch

        # Chemin de sauvegarde unique
        cache_path = os.path.join(BATCHES_DIR, f"graph_batch_{batch_count}.pt")

        try:
            torch.save(graph_cache, cache_path)
            # Ajout de flush=True
            print(f"  -> ‚úÖ Batch {batch_count + 1} saved to: {cache_path} ({nodes_in_batch} nodes, {edges_in_batch} edges)", flush=True)
        except Exception as e:
            # Ajout de flush=True
            print(f"  -> ‚ùå ERROR saving batch {batch_count + 1}: {e}", flush=True)


        # 4. Pr√©paration pour le prochain lot

        # Mettre √† jour l'index et le compteur pour le prochain lot
        start_index += BATCH_SIZE
        batch_count += 1


    # 5. Traitement des r√©sultats finaux
    print(f"\n=== BUILD GRAPH CACHE COMPLETED: {batch_count} batches processed ===", flush=True)

    if batch_count > 0:
        print("\n=============================================", flush=True)
        print(f"‚úÖ Total Graphs Built: {batch_count}", flush=True)
        print(f"‚úÖ TOTAL Nodes Processed: {total_nodes}", flush=True)
        print(f"‚úÖ TOTAL Edges Constructed: {total_edges}", flush=True)
        print("=============================================", flush=True)

    else:
        print("‚ùå No data found to build any graph batch.", flush=True)