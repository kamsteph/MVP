# ==============================
# MAIN RUNNER (Avec Boucle de Batching)
# ==============================
import os
import sys
import torch
from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
import math

# --- ASSUMPTION ---
# J'assume que vous avez renommé la fonction dans 'build_graph_caching.py'
# ou que vous avez importé la nouvelle fonction build_graph_cache_partial.

# Si vous utilisez le nom de fichier original, modifiez l'importation comme suit:
try:
    from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache_partial as build_graph_function
except ImportError:
    # Fallback pour s'assurer que le code fonctionne si le nom de l'importation est différent
    from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache_partial as build_graph_function
    # NOTE: Vous devez manuellement vous assurer que la fonction importée a les paramètres (skip, limit).


sys.path.append("/MVP/KaliaMvpPentest")


if __name__ == "__main__":

    print("=== BUILD GRAPH CACHE STARTED ===")

    # Paramètres de construction du graphe
    BATCH_SIZE = 10000
    K_NN = 8
    SIM_THRESHOLD = 0.25

    # Variables de la boucle
    start_index = 0
    batch_count = 0
    all_graph_batches = []

    # 1. Initialisation du Vendor Manager
    vendor_mgr = VendorEmbeddingManager(
        initial_vendor_list="./data/vendors.txt",
        emb_dim=16,
        device="cuda"
    )

    # 2. Boucle de construction du graphe par lots (Mini-Batching de construction)
    while True:
        print(f"\n--- Processing Batch {batch_count + 1}: Starting at index {start_index} ---")

        # Le skip et limit de MongoDB gèrent le mini-batching ici
        graph_cache = build_graph_function(
            vendor_mgr=vendor_mgr,
            k_nn=K_NN,
            sim_threshold=SIM_THRESHOLD,
            skip=start_index,
            limit=BATCH_SIZE
        )

        if graph_cache is None:
            # La fonction build_graph_cache_partial retourne None quand il n'y a plus de documents
            break

            # Ajouter le lot (batch) à la liste
        all_graph_batches.append(graph_cache)

        # Mettre à jour l'index et le compteur pour le prochain lot
        start_index += BATCH_SIZE
        batch_count += 1


    # 3. Traitement des résultats finaux
    print(f"\n=== BUILD GRAPH CACHE COMPLETED: {batch_count} batches processed ===")

    if all_graph_batches:
        # NOTE IMPORTANTE: Normalement, vous devriez maintenant fusionner
        # ces lots en un seul graphe ou les sauvegarder séparément pour
        # l'entraînement par grappes (clustering/Metis).

        # Pour cet exemple, nous allons simplement sauvegarder le dernier lot
        # et afficher les statistiques globales.

        # Nous allons sauvegarder TOUS les lots (mémoire potentiellement élevée)
        # Il est plus sûr de les sauvegarder un par un dans une boucle.

        os.makedirs("./cache/batches", exist_ok=True)
        total_nodes = 0
        total_edges = 0

        for i, cache in enumerate(all_graph_batches):
            cache_path = f"./cache/batches/graph_batch_{i}.pt"
            torch.save(cache, cache_path)

            data = cache["data"]
            total_nodes += data.num_nodes
            total_edges += data.num_edges

            print(f"  -> Batch {i+1} saved to: {cache_path} ({data.num_nodes} nodes)")


        print("\n=============================================")
        print(f"✅ Total Graphs Built: {batch_count}")
        print(f"✅ TOTAL Nodes Processed: {total_nodes}")
        print(f"✅ TOTAL Edges Constructed: {total_edges}")
        print("=============================================")

    else:
        print("❌ No data found to build any graph batch.")