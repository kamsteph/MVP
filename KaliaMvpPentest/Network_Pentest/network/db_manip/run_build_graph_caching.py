# ==============================
# MAIN RUNNER (Avec Boucle de Batching et Sauvegarde Immediat)
# ==============================
import os
import sys
import torch
from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
import math
from tqdm import tqdm # Ajout de tqdm pour une meilleure barre de progression si nécessaire

# --- ASSUMPTION ---
# Assurez-vous que 'build_graph_cache_partial' est correctement importé
try:
    from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache_partial as build_graph_function
except ImportError:
    # Fallback ou gestion d'erreur si l'importation échoue
    print("FATAL ERROR: 'build_graph_cache_partial' n'a pas pu être importé.")
    sys.exit(1)


# Cette ligne ajoute le répertoire parent au chemin d'importation.
# Assurez-vous que ce chemin est correct pour votre structure de projet.
sys.path.append("/MVP/KaliaMvpPentest")


if __name__ == "__main__":

    print("=== BUILD GRAPH CACHE STARTED ===")

    # Paramètres de construction du graphe
    BATCH_SIZE = 10000
    K_NN = 8
    SIM_THRESHOLD = 0.25

    # Création du dossier de cache (avant de commencer)
    BATCHES_DIR = "./cache/batches"
    os.makedirs(BATCHES_DIR, exist_ok=True)

    # Variables de la boucle et compteurs globaux
    start_index = 0
    batch_count = 0
    total_nodes = 0
    total_edges = 0

    # 1. Initialisation du Vendor Manager
    # Garder ce manager actif tout au long de la boucle assure la cohérence des embeddings.
    vendor_mgr = VendorEmbeddingManager(
        initial_vendor_list="./data/vendors.txt",
        emb_dim=16,
        device="cuda"
    )

    # 2. Boucle de construction du graphe par lots (Mini-Batching de construction)
    while True:
        print(f"\n--- 🏗️ Processing Batch {batch_count + 1}: Starting at index {start_index} ---")

        # Le skip et limit de MongoDB gèrent le mini-batching ici
        graph_cache = build_graph_function(
            vendor_mgr=vendor_mgr,
            k_nn=K_NN,
            sim_threshold=SIM_THRESHOLD,
            skip=start_index,
            limit=BATCH_SIZE
        )

        if graph_cache is None:
            # La fonction retourne None quand il n'y a plus de documents
            break

        # 3. Sauvegarde immédiate du lot (Libération de la mémoire)

        data = graph_cache["data"]

        # Mise à jour des statistiques globales avant la sauvegarde
        nodes_in_batch = data.num_nodes
        edges_in_batch = data.num_edges
        total_nodes += nodes_in_batch
        total_edges += edges_in_batch

        # Chemin de sauvegarde unique
        cache_path = os.path.join(BATCHES_DIR, f"graph_batch_{batch_count}.pt")

        try:
            torch.save(graph_cache, cache_path)
            print(f"  -> ✅ Batch {batch_count + 1} saved to: {cache_path} ({nodes_in_batch} nodes, {edges_in_batch} edges)")
        except Exception as e:
            print(f"  -> ❌ ERROR saving batch {batch_count + 1}: {e}")


        # 4. Préparation pour le prochain lot

        # Mettre à jour l'index et le compteur pour le prochain lot
        start_index += BATCH_SIZE
        batch_count += 1


    # 5. Traitement des résultats finaux
    print(f"\n=== BUILD GRAPH CACHE COMPLETED: {batch_count} batches processed ===")

    if batch_count > 0:
        print("\n=============================================")
        print(f"✅ Total Graphs Built: {batch_count}")
        print(f"✅ TOTAL Nodes Processed: {total_nodes}")
        print(f"✅ TOTAL Edges Constructed: {total_edges}")
        print("=============================================")

    else:
        print("❌ No data found to build any graph batch.")