import os
import sys
import torch

from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache_10_batches
from Network_Pentest.network.utils.dynamic_vendor_embedding import VendorEmbeddingManager


if __name__ == "__main__":
    print("=== BUILD GRAPH CACHE STARTED ===", flush=True)

    # ---------------------------------------------------------
    # PARAMETERS
    # ---------------------------------------------------------
    BATCH_SIZE = 10          # documents per batch
    TOTAL_LIMIT = 100        # 10 batches × 10 docs
    K_NN = 8
    SIM_THRESHOLD = 0.25
    CACHE_DIR = "./cache/batches"

    os.makedirs(CACHE_DIR, exist_ok=True)

    # ---------------------------------------------------------
    # INITIALIZE VENDOR MANAGER
    # ---------------------------------------------------------
    vendor_mgr = VendorEmbeddingManager(
        initial_vendor_list="./data/vendors.txt",
        emb_dim=16,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # ---------------------------------------------------------
    # BUILD GRAPH CACHE (all batches at once)
    # ---------------------------------------------------------
    print(f"→ Building up to {TOTAL_LIMIT} nodes in batches of {BATCH_SIZE}...", flush=True)

    batch_list = build_graph_cache_10_batches(
        vendor_mgr=vendor_mgr,
        total_limit=TOTAL_LIMIT,
        batch_size=BATCH_SIZE,
        k_nn=K_NN,
        sim_threshold=SIM_THRESHOLD
    )

    if not batch_list:
        print("⚠️ No batches returned — possibly no matching documents.", flush=True)
        sys.exit(0)

    # ---------------------------------------------------------
    # SAVE EACH BATCH
    # ---------------------------------------------------------
    total_nodes, total_edges = 0, 0

    for idx, graph_cache in enumerate(batch_list):
        if not isinstance(graph_cache, dict) or "data" not in graph_cache:
            print(f"⚠️ Skipping batch {idx + 1}: invalid format.", flush=True)
            continue

        data = graph_cache["data"]
        num_nodes = getattr(data, "num_nodes", 0)
        num_edges = getattr(data, "num_edges", 0)

        total_nodes += num_nodes
        total_edges += num_edges

        save_path = os.path.join(CACHE_DIR, f"graph_batch_{idx + 1}.pt")
        try:
            torch.save(graph_cache, save_path)
            print(f"  ✅ Saved batch {idx + 1}: {num_nodes} nodes, {num_edges} edges", flush=True)
        except Exception as e:
            print(f"  ❌ Error saving batch {idx + 1}: {e}", flush=True)

    # ---------------------------------------------------------
    # SUMMARY
    # ---------------------------------------------------------
    print("\n=== BUILD GRAPH CACHE COMPLETED ===", flush=True)
    print("=============================================", flush=True)
    print(f" Total Batches Saved:     {len(batch_list)}")
    print(f" Total Nodes Processed:   {total_nodes}")
    print(f" Total Edges Constructed: {total_edges}")
    print(f" Cache Directory:         {os.path.abspath(CACHE_DIR)}")
    print("=============================================", flush=True)
