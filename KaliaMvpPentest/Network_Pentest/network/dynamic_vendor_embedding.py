# vendor_embedding.py
from typing import Dict, Optional
import torch
import torch.nn as nn
from sentence_transformers import SentenceTransformer

class VendorEmbeddingManager:
    """
    Maintains a vendor->index mapping and a torch.nn.Embedding table that can
    be initialized using SBERT projections and expanded at runtime.
    - sb_model (optional): SentenceTransformer model used to seed embeddings.
    - emb_dim: final embedding dimension used by the GNN.
    - proj_dim: SBERT dims (384) may be projected down to emb_dim via a linear projector.
    """

    def __init__(self,
                 initial_vendor_list=None,
                 emb_dim: int = 16,
                 sb_model_name: Optional[str] = "all-MiniLM-L6-v2",
                 device: str = "cpu"):
        self.device = torch.device(device)
        self.emb_dim = emb_dim
        self.vendor_to_idx: Dict[str, int] = {}
        self.idx_to_vendor: Dict[int, str] = {}
        self.sb = None
        self.sb_name = sb_model_name
        if sb_model_name:
            try:
                self.sb = SentenceTransformer(sb_model_name)
            except Exception:
                self.sb = None
        # linear projector from SBERT dim -> emb_dim (trainable)
        sb_dim = 384 if self.sb else 0
        self.proj = nn.Linear(sb_dim, emb_dim).to(self.device) if self.sb else None

        # initialize embedding table empty; will be created once vendors known
        self.emb_table = None

        if initial_vendor_list:
            self._build_from_list(initial_vendor_list)

    def _build_from_list(self, vendor_list):
        unique = sorted({v.lower() for v in vendor_list if v})
        self.vendor_to_idx = {v: i for i, v in enumerate(unique)}
        self.idx_to_vendor = {i: v for v, i in self.vendor_to_idx.items()}
        self._init_embedding_table(seed_with_sbert=True)

    def _init_embedding_table(self, seed_with_sbert: bool = True):
        n = len(self.vendor_to_idx)
        if n == 0:
            self.emb_table = nn.Embedding(1, self.emb_dim).to(self.device)
            self.emb_table.weight.data.uniform_(-0.01, 0.01)
            return

        self.emb_table = nn.Embedding(n, self.emb_dim).to(self.device)
        if seed_with_sbert and self.sb:
            names = [self.idx_to_vendor[i] for i in range(n)]
            embeddings = self.sb.encode(names, show_progress_bar=False)
            embeddings = torch.tensor(embeddings, dtype=torch.float32, device=self.device)
            projected = self.proj(embeddings)  # (n, emb_dim)
            # normalize scale
            projected = projected / (projected.norm(dim=1, keepdim=True) + 1e-6)
            self.emb_table.weight.data[:n] = projected
        else:
            # random init
            self.emb_table.weight.data.uniform_(-0.05, 0.05)

    def get_index(self, vendor: str) -> int:
        v = (vendor or "").lower()
        if v in self.vendor_to_idx:
            return self.vendor_to_idx[v]
        # auto-expand for unseen vendor
        return self.add_vendor(v)

    def add_vendor(self, vendor: str, init_with_sbert: bool = True) -> int:
        """
        Expand the embedding table with a new vendor entry and return index.
        This preserves existing weights and creates a new larger embedding.
        """
        v = vendor.lower()
        if v in self.vendor_to_idx:
            return self.vendor_to_idx[v]

        new_idx = len(self.vendor_to_idx)
        self.vendor_to_idx[v] = new_idx
        self.idx_to_vendor[new_idx] = v

        # create new embedding table
        old_table = self.emb_table
        old_n = 0 if old_table is None else old_table.num_embeddings
        new_n = old_n + 1

        new_table = nn.Embedding(new_n, self.emb_dim).to(self.device)
        # copy old weights if exist
        if old_table is not None:
            new_table.weight.data[:old_n] = old_table.weight.data.clone()
        # init new row
        if init_with_sbert and self.sb:
            emb = self.sb.encode([v], show_progress_bar=False)
            emb = torch.tensor(emb, dtype=torch.float32, device=self.device)
            new_row = self.proj(emb)  # (1, emb_dim)
            new_row = new_row / (new_row.norm() + 1e-6)
            new_table.weight.data[old_n:new_n] = new_row
        else:
            new_table.weight.data[old_n:new_n].uniform_(-0.05, 0.05)

        # replace table
        self.emb_table = new_table
        return new_idx

    def embed_index(self, idx: int) -> torch.Tensor:
        return self.emb_table.weight[idx].to(self.device)

    def forward_vendor(self, vendor: str) -> torch.Tensor:
        idx = self.get_index(vendor)
        return self.emb_table(torch.tensor(idx, device=self.device)).unsqueeze(0)  # (1, emb_dim)
