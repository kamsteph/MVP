import os
import torch
from tqdm import tqdm
from torch_geometric.loader import DataLoader as PyGDataLoader

from Network_Pentest.network.Brain.training.eval_epoch import eval_epoch
from Network_Pentest.network.Brain.training.losses_t import FocalLoss
from Network_Pentest.network.Brain.training.utils_t import compute_class_weights_from_labels
from Network_Pentest.network.utils.metrics import MetricsRecorder
from back_end.database import DatabaseManager
from Network_Pentest.network.Brain.gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
from Network_Pentest.network.Brain.training.config_t import DEFAULT_EPOCHS, DEFAULT_BATCH_SIZE, DEFAULT_NUM_NEIGHBORS, BATCHES_DIR


class UnifiedTrainer:
    def __init__(self, device="cpu"):
        self.device = torch.device(device)
        self.db = DatabaseManager()
        try:
            self.metrics = MetricsRecorder(db_logging=True)
        except Exception:
            self.metrics = None
        self.BATCHES_DIR = "C:\\Users\\2331025\\Downloads\\MVP\\KaliaMvpPentest\\Network_Pentest\\network\\db_manip\\cache\\batches_vuln"
    # ------------------------------------------------------------
    # Load batches and return PyG DataLoaders
    # ------------------------------------------------------------
    def build_cache_and_loaders(
            self,
            all_batches=None,
            batch_size=DEFAULT_BATCH_SIZE,
            train_ratio=0.7,
            val_ratio=0.15,
            random_seed=2025
    ):
        """
        Load cached batches (if all_batches is None) or use provided all_batches list,
        then produce (data_list, train_loader, val_loader, test_loader).

        Accepts .pt files saved as:
          - dict with "data" key: {"data": Data(...), ...}
          - plain Data / Batch object saved directly
          - dict with "data_list" key: {"data_list": [Data,...]}
          - list/tuple of Data objects
        """
        # Step A: load all_batches from disk if not provided
        if all_batches is None:
            all_batches = []
            if not os.path.isdir(self.BATCHES_DIR):
                raise FileNotFoundError(f"Batch directory not found: {self.BATCHES_DIR}")
            batch_files = sorted([f for f in os.listdir(self.BATCHES_DIR) if f.endswith(".pt")])
            print(f"[build_cache_and_loaders] Loading {len(batch_files)} graph batches from {self.BATCHES_DIR}", flush=True)

            accepted = 0
            skipped = 0
            for f in tqdm(batch_files, desc=f"Loading {len(batch_files)} graph batches"):
                path = os.path.join(self.BATCHES_DIR, f)
                try:
                    obj = torch.load(path, map_location=self.device,weights_only=False)
                except Exception as e:
                    print(f"[build_cache_and_loaders] Warning: could not load {f}: {e}", flush=True)
                    skipped += 1
                    continue

                # Normalize into a dict-like batch_cache with key "data" or "data_list"
                batch_cache = None

                # Case 1: dict with "data"
                if isinstance(obj, dict) and "data" in obj and hasattr(obj["data"], "x"):
                    batch_cache = obj
                # Case 2: dict with "data_list"
                elif isinstance(obj, dict) and "data_list" in obj and isinstance(obj["data_list"], (list, tuple)) and len(obj["data_list"])>0 and hasattr(obj["data_list"][0], "x"):
                    # wrap it so downstream code expects {"data": ...} per batch
                    # we'll create one batch_cache per element to keep semantics consistent
                    for d in obj["data_list"]:
                        all_batches.append({"data": d})
                    accepted += len(obj["data_list"])
                    continue
                # Case 3: plain Data or Batch object saved directly
                elif hasattr(obj, "x") and hasattr(obj, "edge_index"):
                    batch_cache = {"data": obj}
                # Case 4: list/tuple of Data objects saved directly
                elif isinstance(obj, (list, tuple)) and len(obj) > 0 and hasattr(obj[0], "x"):
                    for d in obj:
                        all_batches.append({"data": d})
                    accepted += len(obj)
                    continue
                else:
                    # unexpected format
                    print(f"[build_cache_and_loaders] Warning: {f} has unsupported type {type(obj)}; skipping.", flush=True)
                    skipped += 1
                    continue

                # Validate batch_cache["data"]
                if batch_cache is not None:
                    d = batch_cache.get("data")
                    if d is None or not hasattr(d, "x"):
                        print(f"[build_cache_and_loaders] Warning: {f} contains invalid 'data'; skipping.", flush=True)
                        skipped += 1
                        continue
                    all_batches.append(batch_cache)
                    accepted += 1

            print(f"[build_cache_and_loaders] Loaded: accepted={accepted}, skipped={skipped}", flush=True)

            if not all_batches:
                raise RuntimeError("No valid graph batches loaded.")

        # Step B: flatten all_batches -> data_list
        data_list = []
        for b in all_batches:
            if isinstance(b, dict) and "data" in b and hasattr(b["data"], "x"):
                data_list.append(b["data"])
            else:
                # safety: try to extract Data-like objects
                if hasattr(b, "x"):
                    data_list.append(b)
                else:
                    # skip weird entries (shouldn't happen because of loading validation)
                    continue

        if not data_list:
            raise RuntimeError("After normalization, no Data objects found in batches.")

        # Step C: Shuffle & split into train/val/test
        N = len(data_list)
        perm = torch.randperm(N, generator=torch.Generator().manual_seed(random_seed))
        n_train = int(train_ratio * N)
        n_val = int(val_ratio * N)

        train_list = [data_list[i] for i in perm[:n_train]]
        val_list = [data_list[i] for i in perm[n_train:n_train+n_val]]
        test_list = [data_list[i] for i in perm[n_train+n_val:]]

        print(f"[build_cache_and_loaders] Dataset split: Train={len(train_list)}, Val={len(val_list)}, Test={len(test_list)}", flush=True)

        # Step D: Create loaders (graph-level DataLoader)
        train_loader = PyGDataLoader(train_list, batch_size=batch_size, shuffle=True)
        val_loader = PyGDataLoader(val_list, batch_size=batch_size)
        test_loader = PyGDataLoader(test_list, batch_size=batch_size)

        return data_list, train_loader, val_loader, test_loader


    # ------------------------------------------------------------
    # Generic training loop
    # ------------------------------------------------------------
    def train_model(self, model, train_loader, val_loader, test_loader, model_name: str, epochs=DEFAULT_EPOCHS, use_focal=False):
        model = model.to(self.device)

        # Compute class weights
        y_all = []
        for batch in train_loader:
            if hasattr(batch, "y"):
                y_all.append(batch.y.view(-1).cpu())
        if y_all:
            y_all = torch.cat(y_all, dim=0)
            num_classes = int(y_all.max().item()) + 1
            weights = compute_class_weights_from_labels(y_all, num_classes)
            if weights is not None:
                weights = weights.to(self.device)
        else:
            weights = None

        # Loss selection
        if "exploit" in model_name.lower():
            pos_weight = torch.tensor([3.0], device=self.device)
            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        else:
            criterion = FocalLoss() if use_focal else (torch.nn.CrossEntropyLoss(weight=weights) if weights is not None else torch.nn.CrossEntropyLoss())

        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", factor=0.5, patience=3)

        best_metric = -1.0
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            model.train()
            total_loss = 0.0
            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()
                batch_data = getattr(batch, "batch", None)
                if batch_data is not None and batch_data.max().item() == 0:
                    batch_data = None
                out = model(batch.x, batch.edge_index, batch_data)

                if isinstance(out, dict) and "success_logits" in out:
                    logits = out["success_logits"]
                    y = batch.y.float().view(-1)
                    loss = criterion(logits.view(-1), y)
                else:
                    y = batch.y.view(-1)
                    loss = criterion(out, y) if not isinstance(criterion, FocalLoss) else criterion(out, y, weights=weights)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
                optimizer.step()
                total_loss += float(loss.item())

            avg_loss = total_loss / len(train_loader)

            # Validation
            val_stats = eval_epoch(model, val_loader, self.device)
            val_metric = val_stats.get("f1", val_stats.get("auc", 0.0))
            scheduler.step(val_metric)

            print(f"[{model_name}] Epoch {epoch} | Train Loss: {avg_loss:.4f} | Val Metric: {val_metric:.4f}")

            if val_metric > best_metric:
                best_metric = val_metric
                patience_counter = 0
                torch.save(model.state_dict(), f"best_{model_name}.pt")
            else:
                patience_counter += 1
                if patience_counter >= 10:
                    print(f"Early stopping at epoch {epoch}")
                    break

        model.load_state_dict(torch.load(f"best_{model_name}.pt", map_location=self.device))
        test_stats = eval_epoch(model, test_loader, self.device)
        print(f"[{model_name}] FINAL TEST: {test_stats}")
        return model, test_stats

    # ------------------------------------------------------------
    # Specific trainers
    # ------------------------------------------------------------

    def train_network_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, use_focal=False):
        # Load cached batches (.pt files)
        all_batches = []
        if not os.path.isdir(self.BATCHES_DIR):
            raise FileNotFoundError(f"Batch directory not found: {self.BATCHES_DIR}")

        for f in sorted(os.listdir(self.BATCHES_DIR)):
            if f.endswith(".pt"):
                path = os.path.join(self.BATCHES_DIR, f)
                try:
                    batch_cache = torch.load(path, map_location=self.device, weights_only=False)
                    all_batches.append(batch_cache)
                except Exception as e:
                    print(f"Warning: could not load {path}: {e}")
                    continue

        if not all_batches:
            raise RuntimeError("No graph batches found in cache directory.")

        # Create loaders (train/val/test)
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(
            all_batches,
            batch_size=batch_size
        )

        # Model initialization
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = NetworkManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)

        # Train
        return self.train_model(model, train_loader, val_loader, test_loader, "network_gnn", epochs=epochs, use_focal=use_focal)


    def train_vuln_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, use_focal=False):
        # Load cached batches (.pt files)
        all_batches = []
        if not os.path.isdir(self.BATCHES_DIR):
            raise FileNotFoundError(f"Batch directory not found: {self.BATCHES_DIR}")

        for f in sorted(os.listdir(self.BATCHES_DIR)):
            if f.endswith(".pt"):
                path = os.path.join(self.BATCHES_DIR, f)
                try:
                    batch_cache = torch.load(path, map_location=self.device, weights_only=False)
                    all_batches.append(batch_cache)
                except Exception as e:
                    print(f"Warning: could not load {path}: {e}")
                    continue

        if not all_batches:
            raise RuntimeError("No graph batches found in cache directory.")

        # Create loaders (train/val/test)
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(
            all_batches,
            batch_size=batch_size
        )

        # Model initialization
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = VulnerabilityManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)

        # Train
        return self.train_model(model, train_loader, val_loader, test_loader, "vuln_gnn", epochs=epochs, use_focal=use_focal)

    def train_exploit_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, val_ratio=0.15)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = ExploitManagerGNN(in_channels=in_channels, hidden=48, heads=6)
        return self.train_model(model, train_loader, val_loader, test_loader, "exploit_gnn", epochs=epochs, use_focal=use_focal)
