from torch_geometric.loader import DataLoader as PyGDataLoader

from Network_Pentest.network.Brain.training.eval_epoch import eval_epoch
from Network_Pentest.network.Brain.training.losses_t import FocalLoss
from Network_Pentest.network.Brain.training.utils_t import compute_class_weights_from_labels
from Network_Pentest.network.utils.metrics import MetricsRecorder
from back_end.database import DatabaseManager
from config_t import DEFAULT_BATCH_SIZE, DEFAULT_NUM_NEIGHBORS, BATCHES_DIR
import os
import torch
from tqdm import tqdm
from Network_Pentest.network.Brain.gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
from Network_Pentest.network.Brain.training.config_t import DEFAULT_EPOCHS, DEFAULT_BATCH_SIZE, DEFAULT_NUM_NEIGHBORS


class UnifiedTrainer:
    def __init__(self, device="cpu"):
        self.device = torch.device(device)
        self.db = DatabaseManager()
        try:
            self.metrics = MetricsRecorder(db_logging=True)
        except Exception:
            self.metrics = None
        self.BATCHES_DIR = BATCHES_DIR

# ------------------------------------------------------------
# Load cached batches and return PyG DataLoaders
# ------------------------------------------------------------
    def build_cache_and_loaders(
            self,
            all_batches,
            batch_size: int = 200,
            train_ratio: float = 0.7,
            val_ratio: float = 0.15,
            random_seed: int = 2025
    ):
        """
        Given a list of graph caches (as returned by build_graph_cache_10_batches),
        split them into train/val/test and create PyG DataLoaders.

        Parameters:
            all_batches : list
                List of graph caches, each containing a 'data' key.
            batch_size : int
                Batch size for PyG DataLoaders.
            train_ratio : float
                Fraction of data used for training.
            val_ratio : float
                Fraction of data used for validation.
            random_seed : int
                Random seed for reproducible shuffling.
        """
        if not all_batches:
            raise RuntimeError("No graph batches provided.")

        # Flatten all batches into a single list of Data objects
        data_list = [b["data"] for b in all_batches]

        # Shuffle indices reproducibly
        N = len(data_list)
        perm = torch.randperm(N, generator=torch.Generator().manual_seed(random_seed))

        n_train = int(train_ratio * N)
        n_val = int(val_ratio * N)

        train_list = [data_list[i] for i in perm[:n_train]]
        val_list = [data_list[i] for i in perm[n_train:n_train+n_val]]
        test_list = [data_list[i] for i in perm[n_train+n_val:]]

        print(f"Dataset split: Train={len(train_list)}, Val={len(val_list)}, Test={len(test_list)}")

        # Create PyG DataLoaders
        train_loader = PyGDataLoader(train_list, batch_size=batch_size, shuffle=True)
        val_loader = PyGDataLoader(val_list, batch_size=batch_size)
        test_loader = PyGDataLoader(test_list, batch_size=batch_size)

        return data_list, train_loader, val_loader, test_loader


    # def build_cache_and_loaders(self, batch_size=DEFAULT_BATCH_SIZE, k_fold_seed=2025):
    #     data_list = []
    #
    #     if not os.path.isdir(self.BATCHES_DIR):
    #         raise FileNotFoundError(f"Batch directory not found: {self.BATCHES_DIR}")
    #
    #     batch_files = sorted([f for f in os.listdir(self.BATCHES_DIR) if f.endswith(".pt")])
    #     for f in tqdm(batch_files, desc=f"Loading {len(batch_files)} graph batches"):
    #         path = os.path.join(self.BATCHES_DIR, f)
    #         try:
    #             batch_cache = torch.load(path, map_location=self.device)
    #             if "data" in batch_cache and hasattr(batch_cache["data"], "x"):
    #                 data_list.append(batch_cache["data"])
    #             else:
    #                 print(f"Warning: {f} does not contain valid 'data'.")
    #         except Exception as e:
    #             print(f"Warning: could not load {f}: {e}")
    #             continue
    #
    #     if not data_list:
    #         raise RuntimeError("No valid graph batches loaded.")
    #
    #     # Shuffle & split (Train/Val/Test)
    #     N = len(data_list)
    #     perm = torch.randperm(N, generator=torch.Generator().manual_seed(k_fold_seed))
    #     n_train = int(0.7 * N)
    #     n_val = int(0.15 * N)
    #
    #     train_list = [data_list[i] for i in perm[:n_train]]
    #     val_list = [data_list[i] for i in perm[n_train:n_train + n_val]]
    #     test_list = [data_list[i] for i in perm[n_train + n_val:]]
    #
    #     print(f"Dataset split: Train={len(train_list)}, Val={len(val_list)}, Test={len(test_list)}")
    #
    #     # DataLoaders
    #     train_loader = PyGDataLoader(train_list, batch_size=batch_size, shuffle=True)
    #     val_loader = PyGDataLoader(val_list, batch_size=batch_size)
    #     test_loader = PyGDataLoader(test_list, batch_size=batch_size)
    #
    #     return data_list, train_loader, val_loader, test_loader

    # ------------------------------------------------------------
    # Generalized training method for any GNN
    # ------------------------------------------------------------
    def train_model(self, model, train_loader, val_loader, test_loader, model_name: str, epochs=30, use_focal=False):
        model = model.to(self.device)

        # Compute class weights (for classification GNNs)
        y_all = []
        for batch in train_loader:
            if hasattr(batch, "y"):
                y_all.append(batch.y.view(-1).cpu())
        if y_all:
            y_all = torch.cat(y_all, dim=0)
            num_classes = int(y_all.max().item()) + 1
            weights = compute_class_weights_from_labels(y_all, num_classes)
            if weights is not None:
                weights = weights.to(self.device)
        else:
            weights = None

        # Loss selection
        if "exploit" in model_name.lower():
            pos_weight = torch.tensor([3.0], device=self.device)
            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        else:
            criterion = FocalLoss() if use_focal else (torch.nn.CrossEntropyLoss(weight=weights) if weights is not None else torch.nn.CrossEntropyLoss())

        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", factor=0.5, patience=3)

        best_metric = -1.0
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            model.train()
            total_loss = 0.0
            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()
                batch_data = getattr(batch, "batch", None)
                if batch_data is not None and batch_data.max().item() == 0:
                    batch_data = None
                out = model(batch.x, batch.edge_index, batch_data)

                if isinstance(out, dict) and "success_logits" in out:
                    logits = out["success_logits"]
                    y = batch.y.float().view(-1)
                    loss = criterion(logits.view(-1), y)
                else:
                    y = batch.y.view(-1)
                    loss = criterion(out, y) if not isinstance(criterion, FocalLoss) else criterion(out, y, weights=weights)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
                optimizer.step()
                total_loss += float(loss.item())

            avg_loss = total_loss / len(train_loader)

            # Evaluation
            val_stats = eval_epoch(model, val_loader, self.device)
            val_metric = val_stats.get("f1", val_stats.get("auc", 0.0))
            scheduler.step(val_metric)

            print(f"[{model_name}] Epoch {epoch} | Train Loss: {avg_loss:.4f} | Val Metric: {val_metric:.4f}")

            # Early stopping & checkpoint
            if val_metric > best_metric:
                best_metric = val_metric
                patience_counter = 0
                torch.save(model.state_dict(), f"best_{model_name}.pt")
            else:
                patience_counter += 1
                if patience_counter >= 10:
                    print(f"Early stopping at epoch {epoch}")
                    break

        # Load best model & evaluate on test set
        model.load_state_dict(torch.load(f"best_{model_name}.pt", map_location=self.device))
        test_stats = eval_epoch(model, test_loader, self.device)
        print(f"[{model_name}] FINAL TEST: {test_stats}")

        return model, test_stats

    def train_network_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = NetworkManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self.train_model(model, train_loader, val_loader, test_loader, "network_gnn", epochs=epochs, use_focal=use_focal)

    def train_vuln_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = VulnerabilityManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self.train_model(model, train_loader, val_loader, test_loader, "vuln_gnn", epochs=epochs, use_focal=use_focal)

    def train_exploit_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = ExploitManagerGNN(in_channels=in_channels, hidden=48, heads=6)
        return self.train_model(model, train_loader, val_loader, test_loader, "exploit_gnn", epochs=epochs, use_focal=use_focal)
