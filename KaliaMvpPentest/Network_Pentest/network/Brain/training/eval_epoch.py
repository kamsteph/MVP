import torch
import numpy as np
from typing import Dict

def eval_epoch(model, loader, device) -> Dict[str, float]:
    """
    Evaluate a GNN model (graph-level or node-level) on a DataLoader.
    Returns a metrics dictionary: precision, recall, f1, auc (or accuracy fallback).
    """
    model.eval()
    all_true = []
    all_scores = []

    with torch.no_grad():
        for batch in loader:
            if hasattr(batch, "to"):
                batch = batch.to(device)

                # batch vector for graph-level pooling
                batch_data = getattr(batch, "batch", None)
                if batch_data is not None and batch_data.max().item() == 0:
                    batch_data = None

                out = model(batch.x, batch.edge_index, batch_data)

                # Handle node-level vs graph-level outputs
                if isinstance(out, dict) and "success_logits" in out:
                    # ExploitManagerGNN (BCE)
                    logits = out["success_logits"].detach().cpu().flatten()
                    scores = torch.sigmoid(logits).numpy()
                else:
                    # Classification (CrossEntropy)
                    if out.dim() == 1:
                        # binary logits?
                        scores = torch.sigmoid(out.detach().cpu()).numpy()
                    else:
                        logits = out.detach().cpu().numpy()
                        if logits.shape[1] > 1:
                            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
                            scores = probs[:, 1]  # positive class
                        else:
                            scores = 1.0 / (1.0 + np.exp(-logits[:, 0]))

                y_true = batch.y.detach().cpu().numpy().astype(int)
                all_true.append(y_true)
                all_scores.append(scores)

    if not all_true:
        return {}

    y_true = np.concatenate(all_true, axis=0)
    y_scores = np.concatenate(all_scores, axis=0)

    # Compute metrics
    try:
        from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
        y_pred = (y_scores >= 0.5).astype(int)
        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
        auc = 0.0
        try:
            auc = float(roc_auc_score(y_true, y_scores))
        except Exception:
            pass
        return {"precision": float(prec), "recall": float(rec), "f1": float(f1), "auc": float(auc)}
    except ImportError:
        # fallback if sklearn missing
        y_pred = (y_scores >= 0.5).astype(int)
        acc = (y_pred == y_true).mean()
        return {"accuracy": float(acc), "precision": 0.0, "recall": 0.0, "f1": 0.0, "auc": 0.0}
