#!/usr/bin/env python3
"""
enrich_adb_from_local_parallel.py

Bulk ingest CVE JSON files (MITRE v5 and NVD v1.1) into MongoDB adb_collection.

Features:
- Fully recursive walk (no assumptions about subfolder naming).
- Extracts year and numeric-range from path when possible (e.g., 2025/7xxx -> 7000-7999).
- Concurrent JSON parsing using ThreadPoolExecutor.
- Batched bulk_upsert (UpdateOne with upsert) to Mongo for speed.
- Adds metadata fields: cve_year, cve_range, source_file, ingested_at.
- Robust to malformed files: logs and continues.

Usage:
    python -m module.enrich_adb_from_local_parallel /path/to/cve_root \
        --batch 500 --workers 8 --dry-run False

Requires:
    - pymongo installed and accessible via your DatabaseManager
    - back_end.database.DatabaseManager present (as you provided)

Author: Generated for your repo
"""

import os
import sys
import json
import re
import time
from datetime import datetime
from typing import Optional, Dict, Any, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed

from pymongo import UpdateOne
from pymongo.errors import BulkWriteError

from back_end.database import DatabaseManager

# ---------- Configuration ----------
DEFAULT_BATCH_SIZE = 500
DEFAULT_WORKERS = 8
# -----------------------------------

DBM = DatabaseManager()
COL = DBM.adb_collection  # your target collection (adb_collection)
# If you want another collection, change COL assignment.


# ----------------- Helpers -----------------
def extract_year_and_range_from_path(path: str) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[str]]:
    """
    Try to extract year and numeric range from a path.
    Returns: (year, start_range, end_range, range_str) or (None, None, None, None)
    Example:
      /.../2025/7xxx/  -> (2025, 7000, 7999, "7000-7999")
      /.../2021/13xxx/ -> (2021, 13000, 13999, "13000-13999")
    Approach:
      - Look for a 4-digit year token in the path (1999-2099).
      - Look for a token that ends with 'xxx' (case-insensitive) and numeric prefix.
    """
    parts = path.replace("\\", "/").split("/")
    year = None
    range_token = None
    for p in parts:
        if p.isdigit() and len(p) == 4:
            y = int(p)
            if 1970 <= y <= 2100:
                year = y
        # token like '7xxx' or '07xxx' or '130xxx' etc.
        if p.lower().endswith("xxx"):
            range_token = p.lower()

    if year is None or range_token is None:
        return None, None, None, None

    prefix = range_token[:-3]
    try:
        k = int(prefix)
        start = k * 1000
        end = k * 1000 + 999
        return year, start, end, f"{start}-{end}"
    except ValueError:
        return year, None, None, None


def cve_id_to_parts(cve_id: str) -> Tuple[Optional[int], Optional[int]]:
    """
    Parse CVE ID like 'CVE-2025-7234' -> (2025, 7234)
    Returns (None, None) on parse failure.
    """
    if not cve_id or not isinstance(cve_id, str):
        return None, None
    m = re.match(r"(?i)cve-(\d{4})-(\d+)$", cve_id.strip())
    if not m:
        return None, None
    try:
        year = int(m.group(1))
        num = int(m.group(2))
        return year, num
    except Exception:
        return None, None


# ----------------- Parsers -----------------
def parse_mitre_v5_cve(data: Dict[str, Any], source_file: str = None) -> Optional[Dict[str, Any]]:
    """
    Parse MITRE/CVE v5 JSON record (one CVE per file).
    Returns normalized doc or None if cannot find cve id.
    """
    meta = data.get("cveMetadata", {}) or {}
    cve_id = meta.get("cveId") or meta.get("CVE_ID")
    if not cve_id:
        return None

    cna = data.get("containers", {}).get("cna", {}) or {}
    descs = cna.get("descriptions", []) or []
    description = ""
    if descs and isinstance(descs, list):
        # pick the first english description if available
        for d in descs:
            if isinstance(d, dict) and d.get("lang", "").lower().startswith("en"):
                description = d.get("value", "") or description
        if not description:
            description = descs[0].get("value", "") if descs[0] else ""

    refs = []
    try:
        for r in cna.get("references", []) or []:
            if isinstance(r, dict) and r.get("url"):
                refs.append(r.get("url"))
    except Exception:
        refs = []

    affected = []
    try:
        for a in cna.get("affected", []) or []:
            vendor = a.get("vendor", "") or ""
            for ver in a.get("versions", []) or []:
                ver_str = ver.get("version") or ""
                affected.append(" ".join([vendor, ver_str]).strip())
    except Exception:
        affected = []

    tags = cna.get("tags", []) or []
    disputed = any("disputed" == t.lower() for t in tags if isinstance(t, str))

    # attempt to extract CVSS from containers.cna.metrics
    cvss_val = 0.0
    try:
        metrics = cna.get("metrics", []) or []
        for m in metrics:
            if isinstance(m, dict):
                for _, v in m.items():
                    if isinstance(v, dict) and "baseScore" in v:
                        cvss_val = float(v.get("baseScore") or 0.0)
                        break
                if cvss_val:
                    break
    except Exception:
        cvss_val = 0.0

    # identify exploit refs heuristically
    exploit_urls = [u for u in refs if "exploit" in (u or "").lower() or "exploit-db" in (u or "").lower()]

    doc = {
        "cve": cve_id,
        "summary": description,
        "publishedDate": meta.get("datePublished"),
        "lastModifiedDate": meta.get("dateUpdated"),
        "references": refs,
        "exploit_urls": exploit_urls,
        "has_exploit": bool(exploit_urls),
        "affected": affected,
        "disputed": disputed,
        "cvss": cvss_val,
        "source": meta.get("assignerShortName") or "mitre",
        "source_file": source_file
    }
    return doc


def parse_nvd_v1_item(item: Dict[str, Any], source_file: str = None) -> Optional[Dict[str, Any]]:
    """
    Parse NVD-style CVE item (legacy bulk files with "CVE_Items").
    """
    global cve_obj
    try:
        cve_obj = item.get("cve", {}) or {}
        meta = cve_obj.get("CVE_data_meta", {}) or {}
        cve_id = meta.get("ID") or cve_obj.get("id")
    except Exception:
        cve_id = None

    if not cve_id:
        return None

    # CVSS extraction
    cvss_val = 0.0
    try:
        impact = item.get("impact", {}) or {}
        if "baseMetricV3" in impact:
            cvss_val = float(impact["baseMetricV3"]["cvssV3"].get("baseScore", 0.0))
        elif "baseMetricV2" in impact:
            cvss_val = float(impact["baseMetricV2"]["cvssV2"].get("baseScore", 0.0))
    except Exception:
        cvss_val = 0.0

    desc = ""
    try:
        desc_entries = cve_obj.get("description", {}).get("description_data", []) or []
        if desc_entries:
            for d in desc_entries:
                if d.get("lang", "").lower().startswith("en"):
                    desc = d.get("value") or ""
                    break
            if not desc and desc_entries:
                desc = desc_entries[0].get("value", "")
    except Exception:
        desc = ""


    try:
        ref_entries = cve_obj.get("references", {}).get("reference_data", []) or []
        refs = [r.get("url") for r in ref_entries if r.get("url")]
    except Exception:
        refs = []

    doc = {
        "cve": cve_id,
        "summary": desc,
        "references": refs,
        "cvss": cvss_val,
        "source_file": source_file,
        "source": "nvd"
    }
    return doc


def normalize_and_tag(doc: Dict[str, Any], parent_dir: str) -> Dict[str, Any]:
    """
    Add standardized fields and tag with year/range if available.
    """
    year, s, e, range_str = extract_year_and_range_from_path(parent_dir)
    if year:
        doc["cve_year"] = year
    if range_str:
        doc["cve_range"] = range_str
        if s is not None and e is not None:
            doc["range_start"] = s
            doc["range_end"] = e
    doc["ingested_at"] = datetime.now()
    return doc


# --------------- File processing worker ----------------
def process_file(path: str) -> Optional[Dict[str, Any]]:
    """
    Parse a single JSON file and return a normalized document dict ready for DB upsert.
    Returns None if parsing fails or CVE not found.
    """
    fname = os.path.basename(path)
    parent = os.path.dirname(path)
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            data = json.load(f)
    except Exception as e:
        # malformed JSON
        return {"_error": f"json_load_error: {e}", "_source_file": fname}

    # detect MITRE style
    if isinstance(data, dict) and "cveMetadata" in data:
        doc = parse_mitre_v5_cve(data, source_file=fname)
        if not doc:
            return {"_error": "no_cve_in_mitre", "_source_file": fname}
        doc = normalize_and_tag(doc, parent)
        # validate range if possible
        year_tag, num = cve_id_to_parts(doc.get("cve"))
        ytag, start, end, _ = extract_year_and_range_from_path(parent)
        if ytag and start is not None and num is not None:
            if year_tag != ytag or not (start <= num <= end):
                doc["_range_mismatch"] = True
        return doc

    # detect NVD bulk (CVE_Items)
    if isinstance(data, dict) and "CVE_Items" in data:
        # return a doc per item -> marking with special key so caller splits
        docs = []
        items = data.get("CVE_Items", []) or []
        for it in items:
            p = parse_nvd_v1_item(it, source_file=fname)
            if p:
                p = normalize_and_tag(p, parent)
                # range check
                year_tag, num = cve_id_to_parts(p.get("cve"))
                ytag, start, end, _ = extract_year_and_range_from_path(parent)
                if ytag and start is not None and num is not None:
                    if year_tag != ytag or not (start <= num <= end):
                        p["_range_mismatch"] = True
                docs.append(p)
        return {"_bulk_docs": docs}

    # detect list-of-dicts (maybe many small JSONs in one file)
    if isinstance(data, list):
        docs = []
        for entry in data:
            if not isinstance(entry, dict):
                continue
            # many formats: try MITRE-like, NVD-lite, or direct fields
            if "cveMetadata" in entry:
                p = parse_mitre_v5_cve(entry, source_file=fname)
            elif "cve" in entry and isinstance(entry.get("cve"), dict):
                p = parse_nvd_v1_item(entry, source_file=fname)
            else:
                # fallback: direct flat fields
                cve_id = entry.get("cve") or entry.get("CVE") or entry.get("id")
                if not cve_id:
                    continue
                p = {
                    "cve": cve_id,
                    "summary": entry.get("summary") or entry.get("description") or "",
                    "cvss": entry.get("cvss", 0.0),
                    "references": entry.get("references", []),
                    "source_file": fname
                }
            if p:
                p = normalize_and_tag(p, parent)
                docs.append(p)
        return {"_bulk_docs": docs}

    # fallback for single dict that may contain nested vulnerabilities
    if isinstance(data, dict):
        # try to find a top-level CVE ID keys
        keys = list(data.keys())
        cve_id = None
        for k in keys:
            if isinstance(k, str) and k.upper().startswith("CVE-"):
                cve_id = k
                break
        if cve_id:
            # make simple doc
            doc = {"cve": cve_id, "summary": str(data.get(cve_id) or ""), "source_file": fname}
            doc = normalize_and_tag(doc, parent)
            return doc

    return {"_error": "unrecognized_format", "_source_file": fname}


# --------------- Bulk upsert helper ----------------
def bulk_upsert(docs: List[Dict[str, Any]], batch_size: int = DEFAULT_BATCH_SIZE):
    """
    Perform bulk upsert into COL using UpdateOne with upsert=True.
    docs: list of normalized docs (each must have 'cve' key)
    """
    if not docs:
        return 0
    ops = []
    count = 0
    for doc in docs:
        if not doc or not isinstance(doc, dict):
            continue
        cve_id = doc.get("cve")
        if not cve_id:
            continue
        # remove local-only fields before writing?
        # We'll keep fields like _range_mismatch or _error for debugging.
        update_doc = {k: v for k, v in doc.items() if not k.startswith("_")}
        ops.append(UpdateOne({"cve": cve_id}, {"$set": update_doc}, upsert=True))
        count += 1
        if len(ops) >= batch_size:
            try:
                COL.bulk_write(ops, ordered=False)
            except BulkWriteError as bwe:
                # log and continue
                print("Bulk write error:", bwe.details)
            ops = []
    if ops:
        try:
            COL.bulk_write(ops, ordered=False)
        except BulkWriteError as bwe:
            print("Bulk write error:", bwe.details)
    return count


# --------------- Main ingestion routine ----------------
def ingest_root(root_dir: str, batch_size: int = DEFAULT_BATCH_SIZE, max_workers: int = DEFAULT_WORKERS, dry_run: bool = False):
    """
    Walk root_dir recursively, parse all JSON files concurrently, and upsert to Mongo in batches.
    """
    start_time = time.time()
    file_paths = []
    for dirpath, _, filenames in os.walk(root_dir):
        for fname in filenames:
            if fname.lower().endswith(".json"):
                file_paths.append(os.path.join(dirpath, fname))

    total_files = len(file_paths)
    print(f"Found {total_files} JSON files under {root_dir}. Using {max_workers} workers, batch_size={batch_size}.")

    parsed_docs_buffer: List[Dict[str, Any]] = []
    total_parsed = 0
    total_inserted = 0
    errors = 0

    # Threaded parsing
    with ThreadPoolExecutor(max_workers=max_workers) as exe:
        futures = {exe.submit(process_file, p): p for p in file_paths}
        for i, fut in enumerate(as_completed(futures), 1):
            path = futures[fut]
            try:
                res = fut.result()
            except Exception as e:
                errors += 1
                print(f"[ERROR] parsing {path}: {e}")
                continue

            # res may be single doc, dict with _bulk_docs or error dict
            if not res:
                continue

            if isinstance(res, dict) and res.get("_bulk_docs"):
                docs = res.get("_bulk_docs") or []
                for d in docs:
                    parsed_docs_buffer.append(d)
            elif isinstance(res, dict) and res.get("_error"):
                errors += 1
                # store error doc for later inspection (optional)
                parsed_docs_buffer.append(res)
            elif isinstance(res, dict):
                parsed_docs_buffer.append(res)
            else:
                # unexpected type
                errors += 1

            total_parsed += 1

            # flush buffer in batches
            if len(parsed_docs_buffer) >= batch_size * 2:  # keep memory reasonable
                if dry_run:
                    total_inserted += len(parsed_docs_buffer)
                    parsed_docs_buffer = []
                else:
                    inserted = bulk_upsert(parsed_docs_buffer, batch_size=batch_size)
                    total_inserted += inserted
                    parsed_docs_buffer = []
            # periodic progress
            if i % 200 == 0 or i == total_files:
                elapsed = time.time() - start_time
                print(f"[{i}/{total_files}] parsed_files={total_parsed} buffer={len(parsed_docs_buffer)} inserted={total_inserted} errors={errors} elapsed={elapsed:.1f}s")

    # final flush
    if parsed_docs_buffer:
        if dry_run:
            total_inserted += len(parsed_docs_buffer)
        else:
            inserted = bulk_upsert(parsed_docs_buffer, batch_size=batch_size)
            total_inserted += inserted

    elapsed_total = time.time() - start_time
    print(f"Done. files_seen={total_files} parsed_files={total_parsed} inserted_docs={total_inserted} errors={errors} time={elapsed_total:.1f}s")


# -------------- CLI -----------------
def print_usage():
    print("Usage: python -m new_module.enrich_adb_from_local_parallel <root_dir> [--batch N] [--workers K] [--dry-run True|False]")
    print("Example: python -m new_module.enrich_adb_from_local_parallel /data/cve --batch 500 --workers 8")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print_usage()
        sys.exit(1)
    root = sys.argv[1]
    batch = DEFAULT_BATCH_SIZE
    workers = DEFAULT_WORKERS
    dry_run = False
    # simple arg parsing
    for arg in sys.argv[2:]:
        if arg.startswith("--batch"):
            batch = int(arg.split("=")[-1]) if "=" in arg else batch
        elif arg.startswith("--workers"):
            workers = int(arg.split("=")[-1]) if "=" in arg else workers
        elif arg.startswith("--dry-run"):
            v = arg.split("=")[-1] if "=" in arg else "True"
            dry_run = v.lower() in ("1", "true", "yes")

    ingest_root(root, batch_size=batch, max_workers=workers, dry_run=dry_run)

#python -m new_module.enrich_adb_from_local_parallel /path/to/unzipped_cve_root --batch=500 --workers=12
