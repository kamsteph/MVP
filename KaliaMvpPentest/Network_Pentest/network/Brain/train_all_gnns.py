"""
Improved trainer with:
 - class weighting / focal loss
 - LR scheduler (ReduceLROnPlateau) + weight decay
 - early stopping + best checkpoint saving
 - metrics logging (CSV)
 - BatchPrep (NeighborLoader) integration -> Remplacé par PyG DataLoader pour les lots
"""

import os
import csv
import time
import argparse
import random
import numpy as np
from datetime import datetime
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.tensorboard import SummaryWriter
from torch_geometric.loader import DataLoader as PyGDataLoader

# Try user imports (adjust if your repository layout differs)
try:
    from Network_Pentest.network.Brain.gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
    from Network_Pentest.network.utils.metrics import MetricsRecorder
    from back_end.database import DatabaseManager
    from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
except Exception as e:
    print("Import warning (adjust paths if needed):", e)
    from torch_geometric.nn import GCNConv  # fallback only to avoid hard crash
    class DatabaseManager: pass
    class MetricsRecorder:
        def __init__(self, db_logging): pass

# ----------------- Hyperparameters (tweak these) -----------------
DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DEFAULT_EPOCHS = 30
# MODIFICATION CLÉ : Chaque lot (2500 nœuds) est maintenant considéré comme le "batch"
DEFAULT_BATCH_SIZE = 1
DEFAULT_NUM_NEIGHBORS = [10, 5]  # Conservé pour l'API, non utilisé pour le chargement PyG DataLoader
DEFAULT_LR = 1e-3
DEFAULT_WEIGHT_DECAY = 1e-5
EARLY_STOPPING_PATIENCE = 6
LR_REDUCE_FACTOR = 0.5
LR_REDUCE_PATIENCE = 3
CSV_LOG_DIR = "./training_logs"
os.makedirs(CSV_LOG_DIR, exist_ok=True)
# -----------------------------------------------------------------

# ---------- Utilities ----------
def set_global_seed(seed: int = 42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_global_seed(2025)

def compute_class_weights_from_labels(y_tensor: torch.Tensor, num_classes: int):
    """
    Compute class weights inversely proportional to frequency.
    y_tensor: 1-D tensor of integer class labels
    """
    if y_tensor.numel() == 0:
        return None
    counts = torch.bincount(y_tensor.cpu())
    # if classes missing, pad
    if counts.numel() < num_classes:
        counts = torch.cat([counts, torch.zeros(num_classes - counts.numel(), dtype=counts.dtype)])
    weights = 1.0 / (counts.float() + 1e-6)
    weights = weights / weights.sum() * num_classes
    return weights

class FocalLoss(nn.Module):
    def __init__(self, gamma: float = 2.0, alpha: float = 1.0, reduction="mean"):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction

    def forward(self, inputs, targets):
        """
        inputs: raw logits (N, C)
        targets: long labels (N,)
        """
        ce = nn.functional.cross_entropy(inputs, targets, reduction="none")
        pt = torch.exp(-ce)
        loss = self.alpha * (1 - pt) ** self.gamma * ce
        if self.reduction == "mean":
            return loss.mean()
        elif self.reduction == "sum":
            return loss.sum()
        return loss

def safe_metrics(y_true, y_pred_probs, pos_label=1):
    """
    Compute precision, recall, f1, auc if sklearn available; otherwise basics.
    y_true: 1-D numpy array ints
    y_pred_probs: 1-D numpy array floats (or logits) for positive class
    """
    try:
        from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
        y_scores = np.array(y_pred_probs)
        if y_scores.ndim > 1:
            if y_scores.shape[1] > 1:
                y_scores_pos = y_scores[:, 1]
            else:
                y_scores_pos = y_scores[:, 0]
        else:
            y_scores_pos = y_scores

        y_pred = (y_scores_pos >= 0.5).astype(int)
        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
        auc = 0.0
        try:
            auc = float(roc_auc_score(y_true, y_scores_pos))
        except Exception:
            auc = 0.0
        return {"precision": float(prec), "recall": float(rec), "f1": float(f1), "auc": float(auc)}
    except Exception:
        y_pred = (np.array(y_pred_probs) >= 0.5).astype(int)
        acc = (y_pred == np.array(y_true)).mean() if y_pred.size else 0.0
        return {"accuracy": float(acc), "precision": 0.0, "recall": 0.0, "f1": 0.0, "auc": 0.0}

# ----------------- Training primitives -----------------
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    n = 0
    for batch in loader:
        if hasattr(batch, "to"):
            batch = batch.to(device)
            optimizer.zero_grad()

            # GNNs attendent (x, edge_index, batch.batch)
            # Pour un DataLoader batch_size=1, batch.batch sera torch.tensor([0, 0, ...])
            # Laisser 'None' si la variable batch.batch n'est pas nécessaire et n'est pas présente.
            batch_data = getattr(batch, "batch", None)
            if batch_data is not None and batch_data.max().item() == 0:
                batch_data = None # Simplification si c'est un seul graphe

            out = model(batch.x, batch.edge_index, batch_data)

            loss = None
            if out is None:
                continue

            if isinstance(out, dict) and "success_logits" in out:
                # Modèles d'Exploit (BCEWithLogitsLoss)
                logits = out["success_logits"]
                y = batch.y.float().view(-1)
                loss = criterion(logits.view(-1), y)
            else:
                # Modèles de Classification (CrossEntropyLoss)
                y = batch.y.view(-1)
                loss = criterion(out, y)
        else:
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
        optimizer.step()

        total_loss += float(loss.item())
        n += 1
    return total_loss / max(1, n)

def eval_epoch(model, loader, device):
    model.eval()
    all_true = []
    all_scores = []
    with torch.no_grad():
        for batch in loader:
            if hasattr(batch, "to"):
                batch = batch.to(device)

                # Gestion du batch_data comme dans train_epoch
                batch_data = getattr(batch, "batch", None)
                if batch_data is not None and batch_data.max().item() == 0:
                    batch_data = None

                out = model(batch.x, batch.edge_index, batch_data)

                # Récupération des scores (probabilités de classe positive)
                if isinstance(out, dict) and "success_logits" in out:
                    # Exploit Model (BCEWithLogits)
                    logits = out["success_logits"].detach().cpu().numpy().flatten()
                    scores = 1.0 / (1.0 + np.exp(-logits))  # sigmoid
                else:
                    # Classification Model (CrossEntropy)
                    if out.dim() == 1:
                        scores = 1.0 / (1.0 + np.exp(-out.detach().cpu().numpy()))
                    else:
                        logits = out.detach().cpu().numpy()
                        if logits.shape[1] > 1:
                            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
                            scores = probs[:, 1] # Score de la classe 1
                        else:
                            scores = 1.0 / (1.0 + np.exp(-logits[:, 0]))

                y_true = batch.y.detach().cpu().numpy().astype(int)
                all_true.append(y_true)
                all_scores.append(scores)
            else:
                continue

    if not all_true:
        return {}
    y_true = np.concatenate(all_true, axis=0)
    y_scores = np.concatenate(all_scores, axis=0)
    return safe_metrics(y_true, y_scores)

# ------------------ Unified Trainer ------------------
class UnifiedTrainer:
    def __init__(self, device=DEFAULT_DEVICE):
        self.device = torch.device(device)
        self.db = DatabaseManager()
        try:
            self.metrics = MetricsRecorder(db_logging=True)
        except Exception:
            self.metrics = None
        # Dossier où les lots (batchs) de graphes sont sauvegardés
        self.BATCHES_DIR = "./cache/batches"

    def build_cache_and_loaders(self, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, k_fold_seed=2025):
        """
        Charge les lots de graphes sauvegardés, les divise en ensembles train/val/test
        et crée les DataLoaders correspondants (approche Cluster-GCN).
        Returns: (data_list, train_loader, val_loader, test_loader)
        """

        print(f"Loading graph batches from {self.BATCHES_DIR}...", flush=True)

        # 1. Charger tous les lots de graphes sauvegardés
        data_list = []
        if not os.path.isdir(self.BATCHES_DIR):
            raise FileNotFoundError(f"Batch directory not found: {self.BATCHES_DIR}. Please run the main construction script first.")

        batch_files = sorted([f for f in os.listdir(self.BATCHES_DIR) if f.endswith(".pt")])

        for f in tqdm(batch_files, desc=f"Loading {len(batch_files)} graph batches"):
            cache_path = os.path.join(self.BATCHES_DIR, f)
            try:
                # Chaque fichier PT contient { "data": PyG Data, "doc_ids": [...] }
                graph_cache = torch.load(cache_path, map_location=self.device, weights_only=False)
                if "data" in graph_cache and hasattr(graph_cache["data"], 'x'):
                    data_list.append(graph_cache["data"])
                else:
                    print(f"Warning: Batch {f} did not contain a valid PyG Data object in 'data' key.")
            except Exception as e:
                print(f"Warning: Could not load batch {f}: {e}")
                continue

        if not data_list:
            raise RuntimeError("No valid graph batches loaded. Check if the construction script ran successfully.")

        # 2. Split (division) en ensembles Train/Val/Test
        N_graphs = len(data_list)
        perm = torch.randperm(N_graphs, generator=torch.Generator().manual_seed(k_fold_seed))

        n_train = int(0.7 * N_graphs)
        n_val = int(0.15 * N_graphs)

        train_list = [data_list[i] for i in perm[:n_train]]
        val_list = [data_list[i] for i in perm[n_train:n_train + n_val]]
        test_list = [data_list[i] for i in perm[n_train + n_val:]]

        print(f"Dataset Split (by graph batch): Train={len(train_list)}, Val={len(val_list)}, Test={len(test_list)}", flush=True)

        # 3. Création des DataLoaders (Cluster-GCN mode: batch_size=1 pour le sous-graphe entier)
        # On utilise le batch_size fourni par l'utilisateur (ou le DEFAULT_BATCH_SIZE=1)
        train_loader = PyGDataLoader(train_list, batch_size=batch_size, shuffle=True)
        val_loader = PyGDataLoader(val_list, batch_size=batch_size, shuffle=False)
        test_loader = PyGDataLoader(test_list, batch_size=batch_size, shuffle=False)

        return data_list, train_loader, val_loader, test_loader

    def _train_model_with_advanced_features(
            self,
            model,
            train_loader,
            val_loader,
            test_loader,
            model_name: str,
            epochs: int = DEFAULT_EPOCHS,
            lr: float = DEFAULT_LR,
            weight_decay: float = DEFAULT_WEIGHT_DECAY,
            use_focal: bool = False,
            device: str = None
    ):
        device = self.device if device is None else torch.device(device)
        model = model.to(device)

        # Compute class weights
        train_labels = []
        for batch in train_loader:
            if hasattr(batch, "y"):
                train_labels.append(batch.y.view(-1).cpu())

        if train_labels:
            y_all = torch.cat(train_labels, dim=0)
            num_classes = int(y_all.max().item()) + 1 if y_all.numel() else 2
            weights = compute_class_weights_from_labels(y_all, num_classes)
            if weights is not None:
                weights = weights.to(device)
        else:
            weights = None

        # Loss function selection
        if "exploit" in model_name.lower():
            pos_weight = torch.tensor([3.0], device=device)
            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        else:
            if use_focal:
                criterion = FocalLoss(gamma=2.0, alpha=1.0)
            elif weights is not None:
                criterion = nn.CrossEntropyLoss(weight=weights)
            else:
                criterion = nn.CrossEntropyLoss()

        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode="max", factor=LR_REDUCE_FACTOR, patience=LR_REDUCE_PATIENCE
        )

        # ---- CSV Logger ----
        csv_path = os.path.join(
            CSV_LOG_DIR,
            f"{model_name}_training_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}.csv"
        )
        csv_fields = ["epoch", "train_loss", "val_precision", "val_recall",
                      "val_f1", "val_auc", "lr", "time_epoch_s"]
        with open(csv_path, "w", newline="") as csvfile:
            csv_writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
            csv_writer.writeheader()

        # ---- TensorBoard Logger ----
        tb_writer = SummaryWriter(
            log_dir=f"./runs/{model_name}_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}"
        )

        best_metric = -1.0
        patience_counter = 0

        # ---- Training Loop ----
        for epoch in range(1, epochs + 1):
            t0 = time.time()
            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
            val_stats = eval_epoch(model, val_loader, device)
            epoch_time = time.time() - t0

            val_metric = val_stats.get("f1", None) or val_stats.get("auc", None) or val_stats.get("precision", 0.0)

            # TensorBoard logging
            tb_writer.add_scalar("Loss/train", train_loss, epoch)
            tb_writer.add_scalar("Val/F1", val_stats.get("f1", 0.0), epoch)
            tb_writer.add_scalar("Val/AUC", val_stats.get("auc", 0.0), epoch)
            tb_writer.add_scalar("LR", optimizer.param_groups[0]['lr'], epoch)
            tb_writer.add_scalar("Time/EpochSeconds", epoch_time, epoch)

            # Scheduler
            try:
                scheduler.step(val_metric if val_metric is not None else 0.0)
            except Exception:
                pass

            # Checkpoint + Early stopping
            if val_metric is not None and val_metric > best_metric:
                best_metric = float(val_metric)
                patience_counter = 0
                torch.save(model.state_dict(), f"best_{model_name}.pt")
                print(f"✅ Epoch {epoch}: New best val metric {best_metric:.4f} — saved best_{model_name}.pt", flush=True)
            else:
                patience_counter += 1
                if patience_counter >= EARLY_STOPPING_PATIENCE:
                    print(f"⏹️ Early stopping at epoch {epoch} (no improvement for {EARLY_STOPPING_PATIENCE} epochs).", flush=True)
                    break

            # CSV logging
            current_lr = optimizer.param_groups[0]["lr"]
            csv_line = {
                "epoch": epoch,
                "train_loss": train_loss,
                "val_precision": val_stats.get("precision", 0.0),
                "val_recall": val_stats.get("recall", 0.0),
                "val_f1": val_stats.get("f1", 0.0),
                "val_auc": val_stats.get("auc", 0.0),
                "lr": current_lr,
                "time_epoch_s": round(epoch_time, 2)
            }
            with open(csv_path, "a", newline="") as csvfile:
                csv_writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
                csv_writer.writerow(csv_line)

            print(f"[{model_name}] Epoch {epoch:03d} | train_loss {train_loss:.4f} | "
                  f"val_f1 {val_stats.get('f1', 0.0):.4f} | lr {current_lr:.6f} | "
                  f"epoch_time {epoch_time:.1f}s", flush=True)

            if epoch % 10 == 0:
                os.makedirs("./checkpoints", exist_ok=True)
                torch.save(model.state_dict(), f"./checkpoints/{model_name}_epoch_{epoch}.pth")

        # ---- Final test ----
        try:
            model.load_state_dict(torch.load(f"best_{model_name}.pt", map_location=device))
        except Exception:
            pass
        test_stats = eval_epoch(model, test_loader, device)
        print(f"[{model_name}] FINAL TEST: {test_stats}", flush=True)
        tb_writer.close()
        return model, test_stats

    # -------------- public train methods --------------
    def train_network_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = NetworkManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "network_gnn", epochs=epochs, use_focal=use_focal)

    def train_vuln_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = VulnerabilityManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "vuln_gnn", epochs=epochs, use_focal=use_focal)

    def train_exploit_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = ExploitManagerGNN(in_channels=in_channels, hidden=48, heads=6)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "exploit_gnn", epochs=epochs, use_focal=use_focal)

# ----------------- CLI -----------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--device", default=DEFAULT_DEVICE)
    p.add_argument("--epochs", type=int, default=DEFAULT_EPOCHS)
    p.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE)
    p.add_argument("--num-neighbors", nargs="+", type=int, default=DEFAULT_NUM_NEIGHBORS)
    p.add_argument("--train-network", action="store_true")
    p.add_argument("--train-vuln", action="store_true")
    p.add_argument("--train-exploit", action="store_true")
    p.add_argument("--use-focal", action="store_true")
    return p.parse_args()

def main():
    args = parse_args()
    # Ajout du seed pour la reproductibilité du split Train/Val/Test
    set_global_seed(2025)
    trainer = UnifiedTrainer(device=args.device)

    train_all = not (args.train_network or args.train_vuln or args.train_exploit)
    if train_all or args.train_network:
        trainer.train_network_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
    if train_all or args.train_vuln:
        trainer.train_vuln_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
    if train_all or args.train_exploit:
        trainer.train_exploit_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)

if __name__ == "__main__":
    main()