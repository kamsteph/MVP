"""
Improved trainer with:
 - class weighting / focal loss
 - LR scheduler (ReduceLROnPlateau) + weight decay
 - early stopping + best checkpoint saving
 - metrics logging (CSV)
 - BatchPrep (NeighborLoader) integration -> Remplacé par PyG DataLoader pour les lots
"""

import os
import csv
import time
import argparse
import random
import numpy as np
from datetime import datetime
from tqdm import tqdm # Ajouté pour l'affichage de la progression du chargement

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.tensorboard import SummaryWriter
from torch_geometric.loader import DataLoader as PyGDataLoader # Ajouté pour charger les lots

# Try user imports (adjust if your repository layout differs)
try:
    from Network_Pentest.network.Brain.gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
    from Network_Pentest.network.utils.metrics import MetricsRecorder
    from back_end.database import DatabaseManager
    # from Network_Pentest.network.Brain.batch_preparation import BatchPrep # N'est plus utilisé pour le Cluster-GCN
    # from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache # N'est plus utilisé
    from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
except Exception as e:
    # best-effort fallbacks (you may need to edit import paths)
    print("Import warning (adjust paths if needed):", e)
    from torch_geometric.nn import GCNConv  # fallback only to avoid hard crash
    # Définition minimaliste des classes non trouvées pour éviter un crash complet
    class DatabaseManager: pass
    class MetricsRecorder:
        def __init__(self, db_logging): pass
    # La suite des imports essentiels est supposée exister dans l'environnement de l'utilisateur.

# ----------------- Hyperparameters (tweak these) -----------------
DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DEFAULT_EPOCHS = 30
DEFAULT_BATCH_SIZE = 128
DEFAULT_NUM_NEIGHBORS = [10, 5]  # NeighborLoader fanout per hop (plus utilisé directement)
DEFAULT_LR = 1e-3
DEFAULT_WEIGHT_DECAY = 1e-5
EARLY_STOPPING_PATIENCE = 6
LR_REDUCE_FACTOR = 0.5
LR_REDUCE_PATIENCE = 3
CSV_LOG_DIR = "./training_logs"
os.makedirs(CSV_LOG_DIR, exist_ok=True)
# -----------------------------------------------------------------

# ---------- Utilities ----------
def set_global_seed(seed: int = 42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_global_seed(2025)

def compute_class_weights_from_labels(y_tensor: torch.Tensor, num_classes: int):
    """
    Compute class weights inversely proportional to frequency.
    y_tensor: 1-D tensor of integer class labels
    """
    if y_tensor.numel() == 0:
        return None
    counts = torch.bincount(y_tensor.cpu())
    # if classes missing, pad
    if counts.numel() < num_classes:
        counts = torch.cat([counts, torch.zeros(num_classes - counts.numel(), dtype=counts.dtype)])
    weights = 1.0 / (counts.float() + 1e-6)
    weights = weights / weights.sum() * num_classes
    return weights

class FocalLoss(nn.Module):
    def __init__(self, gamma: float = 2.0, alpha: float = 1.0, reduction="mean"):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction

    def forward(self, inputs, targets):
        """
        inputs: raw logits (N, C)
        targets: long labels (N,)
        """
        ce = nn.functional.cross_entropy(inputs, targets, reduction="none")
        pt = torch.exp(-ce)
        loss = self.alpha * (1 - pt) ** self.gamma * ce
        if self.reduction == "mean":
            return loss.mean()
        elif self.reduction == "sum":
            return loss.sum()
        return loss

def safe_metrics(y_true, y_pred_probs, pos_label=1):
    """
    Compute precision, recall, f1, auc if sklearn available; otherwise basics.
    y_true: 1-D numpy array ints
    y_pred_probs: 1-D numpy array floats (or logits) for positive class
    """
    try:
        from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
        y_scores = np.array(y_pred_probs)
        # if probs are logits or multi-class, convert appropriately
        if y_scores.ndim > 1:
            # assume shape (N, C) -> take positive class as index 1 if exists
            if y_scores.shape[1] > 1:
                y_scores_pos = y_scores[:, 1]
            else:
                y_scores_pos = y_scores[:, 0]
        else:
            y_scores_pos = y_scores

        # build hard preds at 0.5
        y_pred = (y_scores_pos >= 0.5).astype(int)
        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
        auc = 0.0
        try:
            auc = float(roc_auc_score(y_true, y_scores_pos))
        except Exception:
            auc = 0.0
        return {"precision": float(prec), "recall": float(rec), "f1": float(f1), "auc": float(auc)}
    except Exception:
        # fallback simple accuracy
        y_pred = (np.array(y_pred_probs) >= 0.5).astype(int)
        acc = (y_pred == np.array(y_true)).mean() if y_pred.size else 0.0
        return {"accuracy": float(acc), "precision": 0.0, "recall": 0.0, "f1": 0.0, "auc": 0.0}

# ----------------- Training primitives -----------------
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    n = 0
    for batch in loader:
        # Batch est maintenant un objet Data (sous-graphe) si DataLoader(list, batch_size=1) est utilisé
        if hasattr(batch, "to"):
            batch = batch.to(device)
            optimizer.zero_grad()

            # GNNs attendent (x, edge_index, batch.batch)
            # Pour un DataLoader batch_size=1, batch.batch peut être None ou torch.tensor([0, 0, ...])
            out = model(batch.x, batch.edge_index, getattr(batch, "batch", None))

            loss = None
            if out is None:
                continue

            # Logique pour les différents types de sortie
            if isinstance(out, dict) and "success_logits" in out:
                # Modèles d'Exploit (BCEWithLogitsLoss)
                logits = out["success_logits"]
                y = batch.y.float().view(-1)
                loss = criterion(logits.view(-1), y)
            else:
                # Modèles de Classification (CrossEntropyLoss)
                y = batch.y.view(-1)
                loss = criterion(out, y)
        else:
            # Fallback (devrait rarement être atteint avec PyG DataLoader)
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
        optimizer.step()

        total_loss += float(loss.item())
        n += 1
    return total_loss / max(1, n)

def eval_epoch(model, loader, device):
    model.eval()
    all_true = []
    all_scores = []
    with torch.no_grad():
        for batch in loader:
            if hasattr(batch, "to"):
                batch = batch.to(device)
                out = model(batch.x, batch.edge_index, getattr(batch, "batch", None))

                # Récupération des scores (probabilités de classe positive)
                if isinstance(out, dict) and "success_logits" in out:
                    # Exploit Model (BCEWithLogits)
                    logits = out["success_logits"].detach().cpu().numpy().flatten()
                    scores = 1.0 / (1.0 + np.exp(-logits))  # sigmoid
                else:
                    # Classification Model (CrossEntropy)
                    if out.dim() == 1:
                        scores = 1.0 / (1.0 + np.exp(-out.detach().cpu().numpy()))
                    else:
                        logits = out.detach().cpu().numpy()
                        if logits.shape[1] > 1:
                            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
                            scores = probs[:, 1] # Score de la classe 1
                        else:
                            scores = 1.0 / (1.0 + np.exp(-logits[:, 0]))

                y_true = batch.y.detach().cpu().numpy().astype(int)
                all_true.append(y_true)
                all_scores.append(scores)
            else:
                continue

    if not all_true:
        return {}
    y_true = np.concatenate(all_true, axis=0)
    y_scores = np.concatenate(all_scores, axis=0)
    return safe_metrics(y_true, y_scores)

# ------------------ Unified Trainer ------------------
class UnifiedTrainer:
    def __init__(self, device=DEFAULT_DEVICE):
        self.device = torch.device(device)
        self.db = DatabaseManager()
        try:
            self.metrics = MetricsRecorder(db_logging=True)
        except Exception:
            self.metrics = None
        # Dossier où les lots (batchs) de graphes sont sauvegardés
        self.BATCHES_DIR = "./cache/batches"

    def build_cache_and_loaders(self, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, k_fold_seed=2025):
        """
        Charge les lots de graphes sauvegardés, les divise en ensembles train/val/test
        et crée les DataLoaders correspondants (approche Cluster-GCN).
        Returns: (data_list, train_loader, val_loader, test_loader)
        """

        print(f"Loading graph batches from {self.BATCHES_DIR}...")

        # 1. Charger tous les lots de graphes sauvegardés
        data_list = []
        if not os.path.isdir(self.BATCHES_DIR):
            raise FileNotFoundError(f"Batch directory not found: {self.BATCHES_DIR}. Please run the main construction script first.")

        batch_files = sorted([f for f in os.listdir(self.BATCHES_DIR) if f.endswith(".pt")])

        for f in tqdm(batch_files, desc=f"Loading {len(batch_files)} graph batches"):
            cache_path = os.path.join(self.BATCHES_DIR, f)
            try:
                # Chaque fichier PT contient { "data": PyG Data, "doc_ids": [...] }
                graph_cache = torch.load(cache_path, map_location="cpu")
                # Assurez-vous que c'est bien un objet Data qui est ajouté
                if "data" in graph_cache and hasattr(graph_cache["data"], 'x'):
                    data_list.append(graph_cache["data"])
                else:
                    print(f"Warning: Batch {f} did not contain a valid PyG Data object in 'data' key.")
            except Exception as e:
                print(f"Warning: Could not load batch {f}: {e}")
                continue

        if not data_list:
            raise RuntimeError("No valid graph batches loaded. Check if the construction script ran successfully.")

        # 2. Split (division) en ensembles Train/Val/Test
        # Le split se fait au niveau des GRAPHE (Batch)
        N_graphs = len(data_list)
        perm = torch.randperm(N_graphs, generator=torch.Generator().manual_seed(k_fold_seed))

        n_train = int(0.7 * N_graphs)
        n_val = int(0.15 * N_graphs)

        train_list = [data_list[i] for i in perm[:n_train]]
        val_list = [data_list[i] for i in perm[n_train:n_train + n_val]]
        test_list = [data_list[i] for i in perm[n_train + n_val:]]

        print(f"Dataset Split (by graph batch): Train={len(train_list)}, Val={len(val_list)}, Test={len(test_list)}")

        # 3. Création des DataLoaders (Cluster-GCN mode: batch_size=1 pour le sous-graphe entier)
        # Note: Le batch_size=1 ici signifie 1 sous-graphe entier par itération.
        train_loader = PyGDataLoader(train_list, batch_size=1, shuffle=True)
        val_loader = PyGDataLoader(val_list, batch_size=1, shuffle=False)
        test_loader = PyGDataLoader(test_list, batch_size=1, shuffle=False)

        return data_list, train_loader, val_loader, test_loader

    def _train_model_with_advanced_features(
            self,
            model,
            train_loader,
            val_loader,
            test_loader,
            model_name: str,
            epochs: int = DEFAULT_EPOCHS,
            lr: float = DEFAULT_LR,
            weight_decay: float = DEFAULT_WEIGHT_DECAY,
            use_focal: bool = False,
            device: str = None
    ):
        device = self.device if device is None else torch.device(device)
        model = model.to(device)

        # Compute class weights from training loader if possible
        train_labels = []
        for batch in train_loader:
            if hasattr(batch, "y"):
                train_labels.append(batch.y.view(-1).cpu())

        if train_labels:
            y_all = torch.cat(train_labels, dim=0)
            num_classes = int(y_all.max().item()) + 1 if y_all.numel() else 2
            weights = compute_class_weights_from_labels(y_all, num_classes)
            if weights is not None:
                weights = weights.to(device)
        else:
            weights = None


        if "exploit" in model_name.lower():
            pos_weight = torch.tensor([3.0], device=device)  # adjust this ratio if class imbalance changes
            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

        else:
            if use_focal:
                criterion = FocalLoss(gamma=2.0, alpha=1.0)
            elif weights is not None:
                criterion = nn.CrossEntropyLoss(weight=weights)
            else:
                criterion = nn.CrossEntropyLoss()

        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", factor=LR_REDUCE_FACTOR, patience=LR_REDUCE_PATIENCE)

        # CSV logger
        csv_path = os.path.join(CSV_LOG_DIR, f"{model_name}_training_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}.csv")
        csv_fields = ["epoch", "train_loss", "val_precision", "val_recall", "val_f1", "val_auc", "lr", "time_epoch_s"]
        with open(csv_path, "w", newline="") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
            writer.writeheader()

        best_metric = -1.0
        patience = EARLY_STOPPING_PATIENCE
        patience_counter = 0

        writer = SummaryWriter(log_dir=f"./runs/{model_name}_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}")

        for epoch in range(1, epochs + 1):
            t0 = time.time()
            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
            val_stats = eval_epoch(model, val_loader, device)  # dict: precision/recall/f1/auc
            epoch_time = time.time() - t0

            # choose validation metric to monitor: f1 preferred, fall back to auc or precision
            val_metric = val_stats.get("f1", None) or val_stats.get("auc", None) or val_stats.get("precision", 0.0)

            writer.add_scalar("Loss/train", train_loss, epoch)
            writer.add_scalar("Val/F1", val_stats.get("f1", 0.0), epoch)
            writer.add_scalar("Val/AUC", val_stats.get("auc", 0.0), epoch)
            writer.add_scalar("LR", optimizer.param_groups[0]['lr'], epoch)


            # scheduler step
            try:
                scheduler.step(val_metric if val_metric is not None else 0.0)
            except Exception:
                pass

            # checkpointing & early stopping
            if val_metric is not None and val_metric > best_metric:
                best_metric = float(val_metric)
                patience_counter = 0
                torch.save(model.state_dict(), f"best_{model_name}.pt")
                print(f"✅ Epoch {epoch}: New best val metric {best_metric:.4f} — saved best_{model_name}.pt")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"⏹️ Early stopping at epoch {epoch} (no improvement for {patience} epochs).")
                    break

            # write CSV line
            current_lr = optimizer.param_groups[0]["lr"]
            csv_line = {
                "epoch": epoch,
                "train_loss": train_loss,
                "val_precision": val_stats.get("precision", 0.0),
                "val_recall": val_stats.get("recall", 0.0),
                "val_f1": val_stats.get("f1", 0.0),
                "val_auc": val_stats.get("auc", 0.0),
                "lr": current_lr,
                "time_epoch_s": round(epoch_time, 2)
            }
            with open(csv_path, "a", newline="") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
                writer.writerow(csv_line)

            print(f"[{model_name}] Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_f1 {val_stats.get('f1',0.0):.4f} | lr {current_lr:.6f} | epoch_time {epoch_time:.1f}s")
            print(f"⏱️  Epoch runtime: {epoch_time:.2f}s | Best F1 so far: {best_metric:.4f}")
            writer.add_scalar("Time/EpochSeconds", epoch_time, epoch)

            if epoch % 10 == 0:
                os.makedirs("./checkpoints", exist_ok=True)
                torch.save(model.state_dict(), f"./checkpoints/{model_name}_epoch_{epoch}.pth")

        # final test evaluation with best model if saved
        try:
            model.load_state_dict(torch.load(f"best_{model_name}.pt", map_location=device))
        except Exception:
            pass
        test_stats = eval_epoch(model, test_loader, device)
        print(f"[{model_name}] FINAL TEST: {test_stats}")
        writer.close()
        return model, test_stats

    # -------------- public train methods --------------
    def train_network_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        # NOTE: num_neighbors n'est plus utilisé pour le chargement, mais conservé pour l'API
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        # infer in_channels from the first loaded subgraph
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = NetworkManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "network_gnn", epochs=epochs, use_focal=use_focal)

    def train_vuln_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = VulnerabilityManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "vuln_gnn", epochs=epochs, use_focal=use_focal)

    def train_exploit_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        data_list, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = data_list[0]
        in_channels = data.x.shape[1]
        model = ExploitManagerGNN(in_channels=in_channels, hidden=48, heads=6)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "exploit_gnn", epochs=epochs, use_focal=use_focal)

# ----------------- CLI -----------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--device", default=DEFAULT_DEVICE)
    p.add_argument("--epochs", type=int, default=DEFAULT_EPOCHS)
    p.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE)
    p.add_argument("--num-neighbors", nargs="+", type=int, default=DEFAULT_NUM_NEIGHBORS)
    p.add_argument("--train-network", action="store_true")
    p.add_argument("--train-vuln", action="store_true")
    p.add_argument("--train-exploit", action="store_true")
    p.add_argument("--use-focal", action="store_true")
    return p.parse_args()

def main():
    args = parse_args()
    set_global_seed(2025)
    trainer = UnifiedTrainer(device=args.device)

    train_all = not (args.train_network or args.train_vuln or args.train_exploit)
    if train_all or args.train_network:
        trainer.train_network_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
    if train_all or args.train_vuln:
        trainer.train_vuln_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
    if train_all or args.train_exploit:
        trainer.train_exploit_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)

if __name__ == "__main__":
    main()
    
# """
# Improved trainer with:
#  - class weighting / focal loss
#  - LR scheduler (ReduceLROnPlateau) + weight decay
#  - early stopping + best checkpoint saving
#  - metrics logging (CSV)
#  - BatchPrep (NeighborLoader) integration
# """
#
# import os
# import csv
# import time
# import argparse
# import random
# import numpy as np
# from datetime import datetime
#
# import torch
# import torch.nn as nn
# from torch.optim import Adam
# from torch.utils.tensorboard import SummaryWriter
#
# # Try user imports (adjust if your repository layout differs)
# try:
#     from Network_Pentest.network.Brain.gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
#     from Network_Pentest.network.utils.metrics import MetricsRecorder
#     from back_end.database import DatabaseManager
#     from Network_Pentest.network.Brain.batch_preparation import BatchPrep
#     from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache
#     from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
# except Exception as e:
#     # best-effort fallbacks (you may need to edit import paths)
#     print("Import warning (adjust paths if needed):", e)
#     from torch_geometric.nn import GCNConv  # fallback only to avoid hard crash
#
# # ----------------- Hyperparameters (tweak these) -----------------
# DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# DEFAULT_EPOCHS = 30
# DEFAULT_BATCH_SIZE = 128
# DEFAULT_NUM_NEIGHBORS = [10, 5]  # NeighborLoader fanout per hop
# DEFAULT_LR = 1e-3
# DEFAULT_WEIGHT_DECAY = 1e-5
# EARLY_STOPPING_PATIENCE = 6
# LR_REDUCE_FACTOR = 0.5
# LR_REDUCE_PATIENCE = 3
# CSV_LOG_DIR = "./training_logs"
# os.makedirs(CSV_LOG_DIR, exist_ok=True)
# # -----------------------------------------------------------------
#
# # ---------- Utilities ----------
# def set_global_seed(seed: int = 42):
#     os.environ["PYTHONHASHSEED"] = str(seed)
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     if torch.cuda.is_available():
#         torch.cuda.manual_seed(seed)
#         torch.cuda.manual_seed_all(seed)
#
# set_global_seed(2025)
#
# def compute_class_weights_from_labels(y_tensor: torch.Tensor, num_classes: int):
#     """
#     Compute class weights inversely proportional to frequency.
#     y_tensor: 1-D tensor of integer class labels
#     """
#     if y_tensor.numel() == 0:
#         return None
#     counts = torch.bincount(y_tensor.cpu())
#     # if classes missing, pad
#     if counts.numel() < num_classes:
#         counts = torch.cat([counts, torch.zeros(num_classes - counts.numel(), dtype=counts.dtype)])
#     weights = 1.0 / (counts.float() + 1e-6)
#     weights = weights / weights.sum() * num_classes
#     return weights
#
# class FocalLoss(nn.Module):
#     def __init__(self, gamma: float = 2.0, alpha: float = 1.0, reduction="mean"):
#         super().__init__()
#         self.gamma = gamma
#         self.alpha = alpha
#         self.reduction = reduction
#
#     def forward(self, inputs, targets):
#         """
#         inputs: raw logits (N, C)
#         targets: long labels (N,)
#         """
#         ce = nn.functional.cross_entropy(inputs, targets, reduction="none")
#         pt = torch.exp(-ce)
#         loss = self.alpha * (1 - pt) ** self.gamma * ce
#         if self.reduction == "mean":
#             return loss.mean()
#         elif self.reduction == "sum":
#             return loss.sum()
#         return loss
#
# def safe_metrics(y_true, y_pred_probs, pos_label=1):
#     """
#     Compute precision, recall, f1, auc if sklearn available; otherwise basics.
#     y_true: 1-D numpy array ints
#     y_pred_probs: 1-D numpy array floats (or logits) for positive class
#     """
#     try:
#         from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
#         y_scores = np.array(y_pred_probs)
#         # if probs are logits or multi-class, convert appropriately
#         if y_scores.ndim > 1:
#             # assume shape (N, C) -> take positive class as index 1 if exists
#             if y_scores.shape[1] > 1:
#                 y_scores_pos = y_scores[:, 1]
#             else:
#                 y_scores_pos = y_scores[:, 0]
#         else:
#             y_scores_pos = y_scores
#
#         # build hard preds at 0.5
#         y_pred = (y_scores_pos >= 0.5).astype(int)
#         prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
#         auc = 0.0
#         try:
#             auc = float(roc_auc_score(y_true, y_scores_pos))
#         except Exception:
#             auc = 0.0
#         return {"precision": float(prec), "recall": float(rec), "f1": float(f1), "auc": float(auc)}
#     except Exception:
#         # fallback simple accuracy
#         y_pred = (np.array(y_pred_probs) >= 0.5).astype(int)
#         acc = (y_pred == np.array(y_true)).mean() if y_pred.size else 0.0
#         return {"accuracy": float(acc), "precision": 0.0, "recall": 0.0, "f1": 0.0, "auc": 0.0}
#
# # ----------------- Training primitives -----------------
# def train_epoch(model, loader, optimizer, criterion, device):
#     model.train()
#     total_loss = 0.0
#     n = 0
#     for batch in loader:
#         # NeighborLoader yields a Batch object or dict-like mapping with .to()
#         if hasattr(batch, "to"):
#             batch = batch.to(device)
#             optimizer.zero_grad()
#             # many of your GNNs expect (x, edge_index, batch.batch)
#             out = model(batch.x, batch.edge_index, getattr(batch, "batch", None))
#             # out may be (N, C) for node-level or graph-level depending on model impl
#             loss = None
#             if out is None:
#                 continue
#             if isinstance(out, dict) and "success_logits" in out:
#                 logits = out["success_logits"]
#                 # assume binary node-level
#                 if logits.dim() == 1:
#                     logits = logits.view(-1, 1)
#                 # for BCEWithLogitsLoss, criterion expects (N,) or (N,1) and labels floats
#                 y = batch.y.float().view(-1)
#                 loss = criterion(logits.view(-1), y)
#             else:
#                 # assume classification logits (N, C)
#                 y = batch.y.view(-1)
#                 loss = criterion(out, y)
#         else:
#             # if loader yields Data objects
#             data = batch.to(device)
#             optimizer.zero_grad()
#             out = model(data.x, data.edge_index, getattr(data, "batch", None))
#             y = data.y.view(-1)
#             loss = criterion(out, y)
#
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
#         optimizer.step()
#
#         total_loss += float(loss.item())
#         n += 1
#     return total_loss / max(1, n)
#
# def eval_epoch(model, loader, device):
#     model.eval()
#     all_true = []
#     all_scores = []
#     with torch.no_grad():
#         for batch in loader:
#             if hasattr(batch, "to"):
#                 batch = batch.to(device)
#                 out = model(batch.x, batch.edge_index, getattr(batch, "batch", None))
#                 # handle dict outputs
#                 if isinstance(out, dict) and "success_logits" in out:
#                     logits = out["success_logits"].detach().cpu().numpy().flatten()
#                     scores = 1.0 / (1.0 + np.exp(-logits))  # sigmoid
#                 else:
#                     # assume (N, C) logits; take prob of class 1
#                     if out.dim() == 1:
#                         scores = 1.0 / (1.0 + np.exp(-out.detach().cpu().numpy()))
#                     else:
#                         logits = out.detach().cpu().numpy()
#                         if logits.shape[1] > 1:
#                             # softmax prob for class 1
#                             probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
#                             scores = probs[:, 1]
#                         else:
#                             scores = 1.0 / (1.0 + np.exp(-logits[:, 0]))
#                 y_true = batch.y.detach().cpu().numpy().astype(int)
#                 all_true.append(y_true)
#                 all_scores.append(scores)
#             else:
#                 data = batch.to(device)
#                 out = model(data.x, data.edge_index, getattr(data, "batch", None))
#                 if out.dim() == 1:
#                     scores = 1.0 / (1.0 + np.exp(-out.detach().cpu().numpy()))
#                 else:
#                     logits = out.detach().cpu().numpy()
#                     if logits.shape[1] > 1:
#                         probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
#                         scores = probs[:, 1]
#                     else:
#                         scores = 1.0 / (1.0 + np.exp(-logits[:, 0]))
#                 y_true = data.y.detach().cpu().numpy().astype(int)
#                 all_true.append(y_true)
#                 all_scores.append(scores)
#     if not all_true:
#         return {}
#     y_true = np.concatenate(all_true, axis=0)
#     y_scores = np.concatenate(all_scores, axis=0)
#     return safe_metrics(y_true, y_scores)
#
# # ------------------ Unified Trainer ------------------
# class UnifiedTrainer:
#     def __init__(self, device=DEFAULT_DEVICE):
#         self.device = torch.device(device)
#         self.db = DatabaseManager()
#         try:
#             self.metrics = MetricsRecorder(db_logging=True)
#         except Exception:
#             self.metrics = None
#
#     def build_cache_and_loaders(self, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, k_fold_seed=2025):
#         """
#         Build graph cache (node features, edges, labels) then create NeighborLoader-based loaders
#         Returns: (graph_cache, train_loader, val_loader, test_loader)
#         """
#         vendor_mgr = VendorEmbeddingManager()
#         graph_cache = build_graph_cache(vendor_mgr, k_nn=num_neighbors[0], sim_threshold=0.25)  # uses your build_graph_cache
#
#         data = graph_cache["data"]
#         # create a simple node-splitting (random) for validation/testing
#         N = data.num_nodes
#         perm = torch.randperm(N, generator=torch.Generator().manual_seed(k_fold_seed))
#         n_train = int(0.7 * N)
#         n_val = int(0.15 * N)
#         train_idx = perm[:n_train].tolist()
#         val_idx = perm[n_train:n_train + n_val].tolist()
#         test_idx = perm[n_train + n_val:].tolist()
#
#         # attach masks to data (NeighborLoader uses `input_nodes` or mask-based indexing)
#         data.train_mask = torch.zeros(N, dtype=torch.bool)
#         data.val_mask = torch.zeros(N, dtype=torch.bool)
#         data.test_mask = torch.zeros(N, dtype=torch.bool)
#         data.train_mask[train_idx] = True
#         data.val_mask[val_idx] = True
#         data.test_mask[test_idx] = True
#
#         # use BatchPrep (NeighborLoader wrapper)
#         bp_train = BatchPrep(data, batch_size=batch_size, num_neighbors=num_neighbors, shuffle=True)
#         bp_val = BatchPrep(data, batch_size=batch_size, num_neighbors=num_neighbors, shuffle=False)
#         bp_test = BatchPrep(data, batch_size=batch_size, num_neighbors=num_neighbors, shuffle=False)
#
#         train_loader = bp_train.get_loader()
#         val_loader = bp_val.get_loader()
#         test_loader = bp_test.get_loader()
#
#         return graph_cache, train_loader, val_loader, test_loader
#
#     def _train_model_with_advanced_features(
#             self,
#             model,
#             train_loader,
#             val_loader,
#             test_loader,
#             model_name: str,
#             epochs: int = DEFAULT_EPOCHS,
#             lr: float = DEFAULT_LR,
#             weight_decay: float = DEFAULT_WEIGHT_DECAY,
#             use_focal: bool = False,
#             device: str = None
#     ):
#         device = self.device if device is None else torch.device(device)
#         model = model.to(device)
#         # Compute class weights from training loader if possible
#         # attempt to gather training labels (might be expensive for huge datasets)
#         train_labels = []
#         for batch in train_loader:
#             if hasattr(batch, "y"):
#                 train_labels.append(batch.y.view(-1).cpu())
#         if train_labels:
#             y_all = torch.cat(train_labels, dim=0)
#             num_classes = int(y_all.max().item()) + 1 if y_all.numel() else 2
#             weights = compute_class_weights_from_labels(y_all, num_classes)
#             if weights is not None:
#                 weights = weights.to(device)
#         else:
#             weights = None
#
#
#         if "exploit" in model_name.lower():
#             pos_weight = torch.tensor([3.0], device=device)  # adjust this ratio if class imbalance changes
#             criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
#
#         else:
#             if use_focal:
#                 criterion = FocalLoss(gamma=2.0, alpha=1.0)
#             elif weights is not None:
#                 criterion = nn.CrossEntropyLoss(weight=weights)
#             else:
#                 criterion = nn.CrossEntropyLoss()
#
#     # if use_focal:
#         #     criterion = FocalLoss(gamma=2.0, alpha=1.0)
#         # else:
#         #     if weights is not None:
#         #         criterion = nn.CrossEntropyLoss(weight=weights)
#         #     else:
#         #         criterion = nn.CrossEntropyLoss()
#
#         optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
#         scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", factor=LR_REDUCE_FACTOR, patience=LR_REDUCE_PATIENCE)
#
#         # CSV logger
#         csv_path = os.path.join(CSV_LOG_DIR, f"{model_name}_training_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}.csv")
#         csv_fields = ["epoch", "train_loss", "val_precision", "val_recall", "val_f1", "val_auc", "lr", "time_epoch_s"]
#         with open(csv_path, "w", newline="") as csvfile:
#             writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
#             writer.writeheader()
#
#         best_metric = -1.0
#         patience = EARLY_STOPPING_PATIENCE
#         patience_counter = 0
#
#         writer = SummaryWriter(log_dir=f"./runs/{model_name}_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}")
#
#         for epoch in range(1, epochs + 1):
#             t0 = time.time()
#             train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
#             val_stats = eval_epoch(model, val_loader, device)  # dict: precision/recall/f1/auc
#             epoch_time = time.time() - t0
#
#             # choose validation metric to monitor: f1 preferred, fall back to auc or precision
#             val_metric = val_stats.get("f1", None) or val_stats.get("auc", None) or val_stats.get("precision", 0.0)
#
#             writer.add_scalar("Loss/train", train_loss, epoch)
#             writer.add_scalar("Val/F1", val_stats.get("f1", 0.0), epoch)
#             writer.add_scalar("Val/AUC", val_stats.get("auc", 0.0), epoch)
#             writer.add_scalar("LR", optimizer.param_groups[0]['lr'], epoch)
#
#
#             # scheduler step
#             try:
#                 scheduler.step(val_metric if val_metric is not None else 0.0)
#             except Exception:
#                 pass
#
#             # checkpointing & early stopping
#             if val_metric is not None and val_metric > best_metric:
#                 best_metric = float(val_metric)
#                 patience_counter = 0
#                 torch.save(model.state_dict(), f"best_{model_name}.pt")
#                 print(f"✅ Epoch {epoch}: New best val metric {best_metric:.4f} — saved best_{model_name}.pt")
#             else:
#                 patience_counter += 1
#                 if patience_counter >= patience:
#                     print(f"⏹️ Early stopping at epoch {epoch} (no improvement for {patience} epochs).")
#                     break
#
#             # write CSV line
#             current_lr = optimizer.param_groups[0]["lr"]
#             csv_line = {
#                 "epoch": epoch,
#                 "train_loss": train_loss,
#                 "val_precision": val_stats.get("precision", 0.0),
#                 "val_recall": val_stats.get("recall", 0.0),
#                 "val_f1": val_stats.get("f1", 0.0),
#                 "val_auc": val_stats.get("auc", 0.0),
#                 "lr": current_lr,
#                 "time_epoch_s": round(epoch_time, 2)
#             }
#             with open(csv_path, "a", newline="") as csvfile:
#                 writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
#                 writer.writerow(csv_line)
#
#             print(f"[{model_name}] Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_f1 {val_stats.get('f1',0.0):.4f} | lr {current_lr:.6f} | epoch_time {epoch_time:.1f}s")
#             print(f"⏱️  Epoch runtime: {epoch_time:.2f}s | Best F1 so far: {best_metric:.4f}")
#             writer.add_scalar("Time/EpochSeconds", epoch_time, epoch)
#
#             if epoch % 10 == 0:
#                 os.makedirs("./checkpoints", exist_ok=True)
#                 torch.save(model.state_dict(), f"./checkpoints/{model_name}_epoch_{epoch}.pth")
#
#     # final test evaluation with best model if saved
#         try:
#             model.load_state_dict(torch.load(f"best_{model_name}.pt", map_location=device))
#         except Exception:
#             pass
#         test_stats = eval_epoch(model, test_loader, device)
#         print(f"[{model_name}] FINAL TEST: {test_stats}")
#         writer.close()
#         return model, test_stats
#
#     # -------------- public train methods --------------
#     def train_network_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
#         graph_cache, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
#         # infer in_channels
#         data = graph_cache["data"]
#         in_channels = data.x.shape[1]
#         model = NetworkManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
#         return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "network_gnn", epochs=epochs, use_focal=use_focal)
#
#     def train_vuln_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
#         graph_cache, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
#         data = graph_cache["data"]
#         in_channels = data.x.shape[1]
#         model = VulnerabilityManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
#         return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "vuln_gnn", epochs=epochs, use_focal=use_focal)
#
#     def train_exploit_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
#         graph_cache, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
#         data = graph_cache["data"]
#         in_channels = data.x.shape[1]
#         model = ExploitManagerGNN(in_channels=in_channels, hidden=48, heads=6)
#         # Exploit uses BCEWithLogitsLoss usually; adapt _train_model... for that if needed
#         return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "exploit_gnn", epochs=epochs, use_focal=use_focal)
#
# # ----------------- CLI -----------------
# def parse_args():
#     p = argparse.ArgumentParser()
#     p.add_argument("--device", default=DEFAULT_DEVICE)
#     p.add_argument("--epochs", type=int, default=DEFAULT_EPOCHS)
#     p.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE)
#     p.add_argument("--num-neighbors", nargs="+", type=int, default=DEFAULT_NUM_NEIGHBORS)
#     p.add_argument("--train-network", action="store_true")
#     p.add_argument("--train-vuln", action="store_true")
#     p.add_argument("--train-exploit", action="store_true")
#     p.add_argument("--use-focal", action="store_true")
#     return p.parse_args()
#
# def main():
#     args = parse_args()
#     set_global_seed(2025)
#     trainer = UnifiedTrainer(device=args.device)
#
#     train_all = not (args.train_network or args.train_vuln or args.train_exploit)
#     if train_all or args.train_network:
#         trainer.train_network_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
#     if train_all or args.train_vuln:
#         trainer.train_vuln_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
#     if train_all or args.train_exploit:
#         trainer.train_exploit_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
#
# if __name__ == "__main__":
#     main()
