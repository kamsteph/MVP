"""
Improved trainer with:
 - class weighting / focal loss
 - LR scheduler (ReduceLROnPlateau) + weight decay
 - early stopping + best checkpoint saving
 - metrics logging (CSV)
 - BatchPrep (NeighborLoader) integration
"""

import os
import csv
import time
import argparse
import random
import numpy as np
from datetime import datetime

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.tensorboard import SummaryWriter

# Try user imports (adjust if your repository layout differs)
try:
    from Network_Pentest.network.Brain.gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
    from Network_Pentest.network.utils.metrics import MetricsRecorder
    from back_end.database import DatabaseManager
    from Network_Pentest.network.Brain.batch_preparation import BatchPrep
    from Network_Pentest.network.db_manip.build_graph_caching import build_graph_cache
    from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
except Exception as e:
    # best-effort fallbacks (you may need to edit import paths)
    print("Import warning (adjust paths if needed):", e)
    from torch_geometric.nn import GCNConv  # fallback only to avoid hard crash

# ----------------- Hyperparameters (tweak these) -----------------
DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DEFAULT_EPOCHS = 30
DEFAULT_BATCH_SIZE = 128
DEFAULT_NUM_NEIGHBORS = [10, 5]  # NeighborLoader fanout per hop
DEFAULT_LR = 1e-3
DEFAULT_WEIGHT_DECAY = 1e-5
EARLY_STOPPING_PATIENCE = 6
LR_REDUCE_FACTOR = 0.5
LR_REDUCE_PATIENCE = 3
CSV_LOG_DIR = "./training_logs"
os.makedirs(CSV_LOG_DIR, exist_ok=True)
# -----------------------------------------------------------------

# ---------- Utilities ----------
def set_global_seed(seed: int = 42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_global_seed(2025)

def compute_class_weights_from_labels(y_tensor: torch.Tensor, num_classes: int):
    """
    Compute class weights inversely proportional to frequency.
    y_tensor: 1-D tensor of integer class labels
    """
    if y_tensor.numel() == 0:
        return None
    counts = torch.bincount(y_tensor.cpu())
    # if classes missing, pad
    if counts.numel() < num_classes:
        counts = torch.cat([counts, torch.zeros(num_classes - counts.numel(), dtype=counts.dtype)])
    weights = 1.0 / (counts.float() + 1e-6)
    weights = weights / weights.sum() * num_classes
    return weights

class FocalLoss(nn.Module):
    def __init__(self, gamma: float = 2.0, alpha: float = 1.0, reduction="mean"):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction

    def forward(self, inputs, targets):
        """
        inputs: raw logits (N, C)
        targets: long labels (N,)
        """
        ce = nn.functional.cross_entropy(inputs, targets, reduction="none")
        pt = torch.exp(-ce)
        loss = self.alpha * (1 - pt) ** self.gamma * ce
        if self.reduction == "mean":
            return loss.mean()
        elif self.reduction == "sum":
            return loss.sum()
        return loss

def safe_metrics(y_true, y_pred_probs, pos_label=1):
    """
    Compute precision, recall, f1, auc if sklearn available; otherwise basics.
    y_true: 1-D numpy array ints
    y_pred_probs: 1-D numpy array floats (or logits) for positive class
    """
    try:
        from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
        y_scores = np.array(y_pred_probs)
        # if probs are logits or multi-class, convert appropriately
        if y_scores.ndim > 1:
            # assume shape (N, C) -> take positive class as index 1 if exists
            if y_scores.shape[1] > 1:
                y_scores_pos = y_scores[:, 1]
            else:
                y_scores_pos = y_scores[:, 0]
        else:
            y_scores_pos = y_scores

        # build hard preds at 0.5
        y_pred = (y_scores_pos >= 0.5).astype(int)
        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
        auc = 0.0
        try:
            auc = float(roc_auc_score(y_true, y_scores_pos))
        except Exception:
            auc = 0.0
        return {"precision": float(prec), "recall": float(rec), "f1": float(f1), "auc": float(auc)}
    except Exception:
        # fallback simple accuracy
        y_pred = (np.array(y_pred_probs) >= 0.5).astype(int)
        acc = (y_pred == np.array(y_true)).mean() if y_pred.size else 0.0
        return {"accuracy": float(acc), "precision": 0.0, "recall": 0.0, "f1": 0.0, "auc": 0.0}

# ----------------- Training primitives -----------------
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    n = 0
    for batch in loader:
        # NeighborLoader yields a Batch object or dict-like mapping with .to()
        if hasattr(batch, "to"):
            batch = batch.to(device)
            optimizer.zero_grad()
            # many of your GNNs expect (x, edge_index, batch.batch)
            out = model(batch.x, batch.edge_index, getattr(batch, "batch", None))
            # out may be (N, C) for node-level or graph-level depending on model impl
            loss = None
            if out is None:
                continue
            if isinstance(out, dict) and "success_logits" in out:
                logits = out["success_logits"]
                # assume binary node-level
                if logits.dim() == 1:
                    logits = logits.view(-1, 1)
                # for BCEWithLogitsLoss, criterion expects (N,) or (N,1) and labels floats
                y = batch.y.float().view(-1)
                loss = criterion(logits.view(-1), y)
            else:
                # assume classification logits (N, C)
                y = batch.y.view(-1)
                loss = criterion(out, y)
        else:
            # if loader yields Data objects
            data = batch.to(device)
            optimizer.zero_grad()
            out = model(data.x, data.edge_index, getattr(data, "batch", None))
            y = data.y.view(-1)
            loss = criterion(out, y)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
        optimizer.step()

        total_loss += float(loss.item())
        n += 1
    return total_loss / max(1, n)

def eval_epoch(model, loader, device):
    model.eval()
    all_true = []
    all_scores = []
    with torch.no_grad():
        for batch in loader:
            if hasattr(batch, "to"):
                batch = batch.to(device)
                out = model(batch.x, batch.edge_index, getattr(batch, "batch", None))
                # handle dict outputs
                if isinstance(out, dict) and "success_logits" in out:
                    logits = out["success_logits"].detach().cpu().numpy().flatten()
                    scores = 1.0 / (1.0 + np.exp(-logits))  # sigmoid
                else:
                    # assume (N, C) logits; take prob of class 1
                    if out.dim() == 1:
                        scores = 1.0 / (1.0 + np.exp(-out.detach().cpu().numpy()))
                    else:
                        logits = out.detach().cpu().numpy()
                        if logits.shape[1] > 1:
                            # softmax prob for class 1
                            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
                            scores = probs[:, 1]
                        else:
                            scores = 1.0 / (1.0 + np.exp(-logits[:, 0]))
                y_true = batch.y.detach().cpu().numpy().astype(int)
                all_true.append(y_true)
                all_scores.append(scores)
            else:
                data = batch.to(device)
                out = model(data.x, data.edge_index, getattr(data, "batch", None))
                if out.dim() == 1:
                    scores = 1.0 / (1.0 + np.exp(-out.detach().cpu().numpy()))
                else:
                    logits = out.detach().cpu().numpy()
                    if logits.shape[1] > 1:
                        probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
                        scores = probs[:, 1]
                    else:
                        scores = 1.0 / (1.0 + np.exp(-logits[:, 0]))
                y_true = data.y.detach().cpu().numpy().astype(int)
                all_true.append(y_true)
                all_scores.append(scores)
    if not all_true:
        return {}
    y_true = np.concatenate(all_true, axis=0)
    y_scores = np.concatenate(all_scores, axis=0)
    return safe_metrics(y_true, y_scores)

# ------------------ Unified Trainer ------------------
class UnifiedTrainer:
    def __init__(self, device=DEFAULT_DEVICE):
        self.device = torch.device(device)
        self.db = DatabaseManager()
        try:
            self.metrics = MetricsRecorder(db_logging=True)
        except Exception:
            self.metrics = None

    def build_cache_and_loaders(self, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, k_fold_seed=2025):
        """
        Build graph cache (node features, edges, labels) then create NeighborLoader-based loaders
        Returns: (graph_cache, train_loader, val_loader, test_loader)
        """
        vendor_mgr = VendorEmbeddingManager()
        graph_cache = build_graph_cache(vendor_mgr, k_nn=num_neighbors[0], sim_threshold=0.25)  # uses your build_graph_cache

        data = graph_cache["data"]
        # create a simple node-splitting (random) for validation/testing
        N = data.num_nodes
        perm = torch.randperm(N, generator=torch.Generator().manual_seed(k_fold_seed))
        n_train = int(0.7 * N)
        n_val = int(0.15 * N)
        train_idx = perm[:n_train].tolist()
        val_idx = perm[n_train:n_train + n_val].tolist()
        test_idx = perm[n_train + n_val:].tolist()

        # attach masks to data (NeighborLoader uses `input_nodes` or mask-based indexing)
        data.train_mask = torch.zeros(N, dtype=torch.bool)
        data.val_mask = torch.zeros(N, dtype=torch.bool)
        data.test_mask = torch.zeros(N, dtype=torch.bool)
        data.train_mask[train_idx] = True
        data.val_mask[val_idx] = True
        data.test_mask[test_idx] = True

        # use BatchPrep (NeighborLoader wrapper)
        bp_train = BatchPrep(data, batch_size=batch_size, num_neighbors=num_neighbors, shuffle=True)
        bp_val = BatchPrep(data, batch_size=batch_size, num_neighbors=num_neighbors, shuffle=False)
        bp_test = BatchPrep(data, batch_size=batch_size, num_neighbors=num_neighbors, shuffle=False)

        train_loader = bp_train.get_loader()
        val_loader = bp_val.get_loader()
        test_loader = bp_test.get_loader()

        return graph_cache, train_loader, val_loader, test_loader

    def _train_model_with_advanced_features(
            self,
            model,
            train_loader,
            val_loader,
            test_loader,
            model_name: str,
            epochs: int = DEFAULT_EPOCHS,
            lr: float = DEFAULT_LR,
            weight_decay: float = DEFAULT_WEIGHT_DECAY,
            use_focal: bool = False,
            device: str = None
    ):
        device = self.device if device is None else torch.device(device)
        model = model.to(device)
        # Compute class weights from training loader if possible
        # attempt to gather training labels (might be expensive for huge datasets)
        train_labels = []
        for batch in train_loader:
            if hasattr(batch, "y"):
                train_labels.append(batch.y.view(-1).cpu())
        if train_labels:
            y_all = torch.cat(train_labels, dim=0)
            num_classes = int(y_all.max().item()) + 1 if y_all.numel() else 2
            weights = compute_class_weights_from_labels(y_all, num_classes)
            if weights is not None:
                weights = weights.to(device)
        else:
            weights = None


        if "exploit" in model_name.lower():
            pos_weight = torch.tensor([3.0], device=device)  # adjust this ratio if class imbalance changes
            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

        else:
            if use_focal:
                criterion = FocalLoss(gamma=2.0, alpha=1.0)
            elif weights is not None:
                criterion = nn.CrossEntropyLoss(weight=weights)
            else:
                criterion = nn.CrossEntropyLoss()

    # if use_focal:
        #     criterion = FocalLoss(gamma=2.0, alpha=1.0)
        # else:
        #     if weights is not None:
        #         criterion = nn.CrossEntropyLoss(weight=weights)
        #     else:
        #         criterion = nn.CrossEntropyLoss()

        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", factor=LR_REDUCE_FACTOR, patience=LR_REDUCE_PATIENCE)

        # CSV logger
        csv_path = os.path.join(CSV_LOG_DIR, f"{model_name}_training_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}.csv")
        csv_fields = ["epoch", "train_loss", "val_precision", "val_recall", "val_f1", "val_auc", "lr", "time_epoch_s"]
        with open(csv_path, "w", newline="") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
            writer.writeheader()

        best_metric = -1.0
        patience = EARLY_STOPPING_PATIENCE
        patience_counter = 0

        writer = SummaryWriter(log_dir=f"./runs/{model_name}_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}")

        for epoch in range(1, epochs + 1):
            t0 = time.time()
            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
            val_stats = eval_epoch(model, val_loader, device)  # dict: precision/recall/f1/auc
            epoch_time = time.time() - t0

            # choose validation metric to monitor: f1 preferred, fall back to auc or precision
            val_metric = val_stats.get("f1", None) or val_stats.get("auc", None) or val_stats.get("precision", 0.0)

            writer.add_scalar("Loss/train", train_loss, epoch)
            writer.add_scalar("Val/F1", val_stats.get("f1", 0.0), epoch)
            writer.add_scalar("Val/AUC", val_stats.get("auc", 0.0), epoch)
            writer.add_scalar("LR", optimizer.param_groups[0]['lr'], epoch)


            # scheduler step
            try:
                scheduler.step(val_metric if val_metric is not None else 0.0)
            except Exception:
                pass

            # checkpointing & early stopping
            if val_metric is not None and val_metric > best_metric:
                best_metric = float(val_metric)
                patience_counter = 0
                torch.save(model.state_dict(), f"best_{model_name}.pt")
                print(f"✅ Epoch {epoch}: New best val metric {best_metric:.4f} — saved best_{model_name}.pt")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"⏹️ Early stopping at epoch {epoch} (no improvement for {patience} epochs).")
                    break

            # write CSV line
            current_lr = optimizer.param_groups[0]["lr"]
            csv_line = {
                "epoch": epoch,
                "train_loss": train_loss,
                "val_precision": val_stats.get("precision", 0.0),
                "val_recall": val_stats.get("recall", 0.0),
                "val_f1": val_stats.get("f1", 0.0),
                "val_auc": val_stats.get("auc", 0.0),
                "lr": current_lr,
                "time_epoch_s": round(epoch_time, 2)
            }
            with open(csv_path, "a", newline="") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=csv_fields)
                writer.writerow(csv_line)

            print(f"[{model_name}] Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_f1 {val_stats.get('f1',0.0):.4f} | lr {current_lr:.6f} | epoch_time {epoch_time:.1f}s")
            print(f"⏱️  Epoch runtime: {epoch_time:.2f}s | Best F1 so far: {best_metric:.4f}")
            writer.add_scalar("Time/EpochSeconds", epoch_time, epoch)

            if epoch % 10 == 0:
                os.makedirs("./checkpoints", exist_ok=True)
                torch.save(model.state_dict(), f"./checkpoints/{model_name}_epoch_{epoch}.pth")

    # final test evaluation with best model if saved
        try:
            model.load_state_dict(torch.load(f"best_{model_name}.pt", map_location=device))
        except Exception:
            pass
        test_stats = eval_epoch(model, test_loader, device)
        print(f"[{model_name}] FINAL TEST: {test_stats}")
        writer.close()
        return model, test_stats

    # -------------- public train methods --------------
    def train_network_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        graph_cache, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        # infer in_channels
        data = graph_cache["data"]
        in_channels = data.x.shape[1]
        model = NetworkManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "network_gnn", epochs=epochs, use_focal=use_focal)

    def train_vuln_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        graph_cache, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = graph_cache["data"]
        in_channels = data.x.shape[1]
        model = VulnerabilityManagerGNN(in_channels=in_channels, hidden=64, num_classes=2)
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "vuln_gnn", epochs=epochs, use_focal=use_focal)

    def train_exploit_gnn(self, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH_SIZE, num_neighbors=DEFAULT_NUM_NEIGHBORS, use_focal=False):
        graph_cache, train_loader, val_loader, test_loader = self.build_cache_and_loaders(batch_size=batch_size, num_neighbors=num_neighbors)
        data = graph_cache["data"]
        in_channels = data.x.shape[1]
        model = ExploitManagerGNN(in_channels=in_channels, hidden=48, heads=6)
        # Exploit uses BCEWithLogitsLoss usually; adapt _train_model... for that if needed
        return self._train_model_with_advanced_features(model, train_loader, val_loader, test_loader, "exploit_gnn", epochs=epochs, use_focal=use_focal)

# ----------------- CLI -----------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--device", default=DEFAULT_DEVICE)
    p.add_argument("--epochs", type=int, default=DEFAULT_EPOCHS)
    p.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE)
    p.add_argument("--num-neighbors", nargs="+", type=int, default=DEFAULT_NUM_NEIGHBORS)
    p.add_argument("--train-network", action="store_true")
    p.add_argument("--train-vuln", action="store_true")
    p.add_argument("--train-exploit", action="store_true")
    p.add_argument("--use-focal", action="store_true")
    return p.parse_args()

def main():
    args = parse_args()
    set_global_seed(2025)
    trainer = UnifiedTrainer(device=args.device)

    train_all = not (args.train_network or args.train_vuln or args.train_exploit)
    if train_all or args.train_network:
        trainer.train_network_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
    if train_all or args.train_vuln:
        trainer.train_vuln_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)
    if train_all or args.train_exploit:
        trainer.train_exploit_gnn(epochs=args.epochs, batch_size=args.batch_size, num_neighbors=args.num_neighbors, use_focal=args.use_focal)

if __name__ == "__main__":
    main()


# """
# train_all_gnns.py
#
# Unified trainer for:
#   - NetworkManagerGNN
#   - VulnerabilityManagerGNN
#   - ExploitManagerGNN
#
# Now supports:
#  - building graph cache (build_graph_cache + VendorEmbeddingManager)
#  - using a BatchPrep / NeighborLoader (prebuilt loader or fallback to DB)
#  - dynamic in_channels detection from the cached graph
#  - saving models & metrics
#
# Usage examples:
#   python train_all_gnns.py            # build cache & train all (MVP flow)
#   python train_all_gnns.py --exploit  # train only exploit model
# """
#
# import os
# import argparse
#
# from torch import device
# from torch.nn.functional import cross_entropy
# import random
# import numpy as np
# import torch
# import torch.nn as nn
# from torch.optim import Adam
#
# # try to import user modules from likely locations
# try:
#     # your original models / metrics / DB manager
#     from Network_Pentest.network.Brain.gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
#     from Network_Pentest.network.utils.metrics import MetricsRecorder
#     from back_end.database import DatabaseManager
# except Exception:
#     # fallback if running from repo root (adjust if necessary)
#     from gnn_models import NetworkManagerGNN, VulnerabilityManagerGNN, ExploitManagerGNN
#     from Network_Pentest.network.utils.metrics import MetricsRecorder
#     from back_end.database import DatabaseManager
#
# # build_graph_cache and VendorEmbeddingManager - adjust import per your layout if necessary
# try:
#     from Network_Pentest.network.db_manip.build_graph_cache import build_graph_cache
#     from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
# except Exception:
#     try:
#         from Network_Pentest.network.db_manip.build_graph_cache import build_graph_cache
#         from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager
#     except Exception:
#         # Last resort: user must provide proper import paths
#         raise
#
# # BatchPrep loader (your custom class)
# try:
#     from Network_Pentest.network.Brain.batch_preparation import BatchPrep
# except Exception:
#     try:
#         from Network_Pentest.network.Brain.batch_preparation import BatchPrep
#     except Exception:
#         # If BatchPrep isn't found, fallback to a simple wrapper around torch_geometric NeighborLoader
#         BatchPrep = None
#
# # reproducibility helper
# def set_global_seed(seed: int = 42, use_cuda: bool = False):
#     os.environ['PYTHONHASHSEED'] = str(seed)
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     if use_cuda and torch.cuda.is_available():
#         torch.cuda.manual_seed(seed)
#         torch.cuda.manual_seed_all(seed)
#         torch.backends.cudnn.deterministic = True
#         torch.backends.cudnn.benchmark = False
#
# set_global_seed(2025, use_cuda=torch.cuda.is_available())
#
# # ----- utility trainers (same semantics as your original code but accept loaders) -----
# def train_epoch(model, loader, optimizer, criterion, device):
#     model.train()
#     total_loss = 0.0
#     n_batches = 0
#     for batch in loader:
#         # batch may be torch_geometric Batch object or tuple depending on BatchPrep
#         if hasattr(batch, "to"):
#             batch = batch.to(device)
#             x = batch.x
#             edge_index = batch.edge_index
#             # if graph-level classification: models might expect (x, edge_index, batch.batch)
#             optimizer.zero_grad()
#             out = model(x, edge_index, batch.batch)
#             loss = criterion(out, batch.y.view(-1))
#         else:
#             # fallback for simple DataLoader that returns Data objects
#             data = batch
#             data = data.to(device)
#             optimizer.zero_grad()
#             out = model(data.x, data.edge_index, getattr(data, "batch", None))
#             loss = criterion(out, data.y.view(-1))
#
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#         n_batches += 1
#     return total_loss / max(1, n_batches)
#
# def eval_epoch(model, loader, device):
#     model.eval()
#     correct, total = 0, 0
#     with torch.no_grad():
#         for batch in loader:
#             if hasattr(batch, "to"):
#                 batch = batch.to(device)
#                 out = model(batch.x, batch.edge_index, batch.batch)
#                 preds = out.argmax(dim=1)
#                 correct += (preds == batch.y.view(-1)).sum().item()
#                 total += batch.num_graphs
#             else:
#                 data = batch.to(device)
#                 out = model(data.x, data.edge_index, getattr(data, "batch", None))
#                 preds = out.argmax(dim=1)
#                 correct += (preds == data.y.view(-1)).sum().item()
#                 total += 1
#     return correct / total if total > 0 else 0.0
#
# # ----- UnifiedTrainer class (accepts loaders or builds them from cache) -----
# class UnifiedTrainer:
#     def __init__(self, device="cpu"):
#         self.device = torch.device(device)
#         self.db = DatabaseManager()
#         self.metrics = MetricsRecorder(db_logging=True)
#
#     # Backwards-compatible DB loader (keeps your original behavior)
#     def _load_dataset_from_db(self, collection_name, label_field="label"):
#         collection = getattr(self.db, collection_name)
#         cursor = collection.find({"nodes": {"$exists": True}})
#         data_list = []
#         from torch_geometric.data import Data
#         for doc in cursor:
#             try:
#                 x = torch.tensor(doc["nodes"], dtype=torch.float)
#                 edge_index = torch.tensor(doc["edges"], dtype=torch.long).t().contiguous()
#                 y = torch.tensor([doc.get(label_field, 0)], dtype=torch.long)
#                 data = Data(x=x, edge_index=edge_index, y=y)
#                 data_list.append(data)
#             except Exception as e:
#                 print(f"[WARN] Skipped invalid doc: {e}")
#                 continue
#         return data_list
#
#     # ---- new: utility to build loader from cache or fallback to DB ----
#     def _make_loader_from_cache_or_db(self, graph_cache, batch_size=64, num_neighbors=[10,5], shuffle=True):
#         """
#         graph_cache: dict returned by build_graph_cache() or None
#         If BatchPrep class is present, we use it; otherwise fallback to simple DataLoader over single Data object.
#         """
#         if graph_cache is None:
#             return None
#
#         data = graph_cache["data"]
#         # Try to use BatchPrep if available
#         if BatchPrep is not None:
#             bp = BatchPrep(data, num_neighbors=num_neighbors, batch_size=batch_size, shuffle=shuffle)
#             loader = bp.get_loader()  # your BatchPrep should implement get_loader()
#             return loader
#         else:
#             # fallback: create a simple DataLoader that yields full graph as single-batch items (less efficient)
#             from torch_geometric.loader import DataLoader as PyGDataLoader
#             # split the full graph into many single-data objects? simplest: return an iterator that yields the full graph
#             return [data]  # single-element loader (inefficient but safe)
#
#     # ---- Train network GNN ----
#     def train_network_gnn(self, loader=None, collection_name="adb_collection", epochs=20, lr=1e-3, batch_size=64):
#         print("🚀 Starting NetworkManagerGNN training...")
#         # If loader provided use it, else try to load graph_cache and create loader
#         if loader is None:
#             # try building a cache
#             try:
#                 vendor_mgr = VendorEmbeddingManager()
#                 graph_cache = build_graph_cache(vendor_mgr)
#                 loader = self._make_loader_from_cache_or_db(graph_cache, batch_size=batch_size)
#             except Exception as e:
#                 print(f"[WARN] Couldn't build cache ({e}), falling back to DB loader.")
#                 dataset = self._load_dataset_from_db(collection_name)
#                 from torch_geometric.loader import DataLoader as PyGDataLoader
#                 loader = PyGDataLoader(dataset, batch_size=8, shuffle=True)
#
#         # detect in_channels automatically if possible
#         sample_data = None
#         if hasattr(loader, "__iter__"):
#             # try to peek at first element
#             it = iter(loader)
#             try:
#                 sample = next(it)
#                 if hasattr(sample, "x"):
#                     in_channels = sample.x.shape[1]
#                 else:
#                     in_channels = getattr(sample, "num_node_features", 11)
#             except StopIteration:
#                 in_channels = 11
#         else:
#             in_channels = 11
#
#         model = NetworkManagerGNN(in_channels=in_channels, hidden=64, num_classes=2).to(self.device)
#         optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)
#
#         labels = torch.cat([batch.y for batch in loader.dataset])
#         class_counts = torch.bincount(labels)
#         weights = 1.0 / (class_counts.float() + 1e-6)
#         weights = weights / weights.sum()
#         criterion = nn.CrossEntropyLoss(weight=weights.to(device))
#         #criterion = nn.CrossEntropyLoss()
#
#         for epoch in range(epochs):
#             loss = train_epoch(model, loader, optimizer, criterion, self.device)
#             acc = eval_epoch(model, loader, self.device)
#             print(f"[NetworkGNN] Epoch {epoch+1:03d} | Loss {loss:.4f} | Acc {acc:.4f}")
#             # optional logging
#             try:
#                 self.metrics.classification_metrics([], [], phase="network_gnn")
#             except Exception:
#                 pass
#
#         torch.save(model.state_dict(), "network_gnn.pt")
#         print("✅ Saved network_gnn.pt")
#
#     # ---- Train vulnerability GNN ----
#     def train_vuln_gnn(self, loader=None, collection_name="adb_collection", epochs=20, lr=1e-3, batch_size=64):
#         print("🚀 Starting VulnerabilityManagerGNN training...")
#         if loader is None:
#             try:
#                 vendor_mgr = VendorEmbeddingManager()
#                 graph_cache = build_graph_cache(vendor_mgr)
#                 loader = self._make_loader_from_cache_or_db(graph_cache, batch_size=batch_size)
#             except Exception as e:
#                 print(f"[WARN] Couldn't build cache ({e}), falling back to DB loader.")
#                 dataset = self._load_dataset_from_db(collection_name)
#                 from torch_geometric.loader import DataLoader as PyGDataLoader
#                 loader = PyGDataLoader(dataset, batch_size=8, shuffle=True)
#
#         # dynamic in_channels
#         try:
#             # try infer input channels
#             it = iter(loader); sample = next(it)
#             in_channels = sample.x.shape[1]
#         except Exception:
#             in_channels = 11
#
#         model = VulnerabilityManagerGNN(in_channels=in_channels, hidden=64, num_classes=2).to(self.device)
#         optimizer = Adam(model.parameters(), lr=lr)
#         criterion = nn.CrossEntropyLoss()
#
#         for epoch in range(epochs):
#             loss = train_epoch(model, loader, optimizer, criterion, self.device)
#             acc = eval_epoch(model, loader, self.device)
#             print(f"[VulnGNN] Epoch {epoch+1:03d} | Loss {loss:.4f} | Acc {acc:.4f}")
#             try:
#                 self.metrics.classification_metrics([], [], phase="vuln_gnn")
#             except Exception:
#                 pass
#
#         torch.save(model.state_dict(), "vuln_gnn.pt")
#         print("✅ Saved vuln_gnn.pt")
#
#     # ---- Train exploit GNN ----
#     def train_exploit_gnn(self, loader=None, collection_name="adb_collection", epochs=25, lr=5e-4, batch_size=64):
#         print("🚀 Starting ExploitManagerGNN training...")
#         if loader is None:
#             try:
#                 vendor_mgr = VendorEmbeddingManager()
#                 graph_cache = build_graph_cache(vendor_mgr)
#                 loader = self._make_loader_from_cache_or_db(graph_cache, batch_size=batch_size)
#             except Exception as e:
#                 print(f"[WARN] Couldn't build cache ({e}), falling back to DB loader.")
#                 dataset = self._load_dataset_from_db(collection_name)
#                 from torch_geometric.loader import DataLoader as PyGDataLoader
#                 loader = PyGDataLoader(dataset, batch_size=8, shuffle=True)
#
#         # infer in_channels for exploit model: if features smaller than expected, use default 6
#         try:
#             it = iter(loader); sample = next(it)
#             in_channels = sample.x.shape[1]
#         except Exception:
#             in_channels = 6
#
#         model = ExploitManagerGNN(in_channels=in_channels, hidden=48, heads=6).to(self.device)
#         optimizer = Adam(model.parameters(), lr=lr)
#         criterion = nn.BCEWithLogitsLoss()
#
#         for epoch in range(epochs):
#             model.train()
#             total_loss = 0.0
#             n_batches = 0
#             for batch in loader:
#                 if hasattr(batch, "to"):
#                     batch = batch.to(self.device)
#                     optimizer.zero_grad()
#                     # ExploitManagerGNN expected to return dict with "success_logits"
#                     out = model(batch.x, batch.edge_index)
#                     logits = out.get("success_logits") if isinstance(out, dict) else out
#                     labels = batch.y.float().view(-1)
#                     loss = criterion(logits, labels)
#                 else:
#                     data = batch.to(self.device)
#                     optimizer.zero_grad()
#                     out = model(data.x, data.edge_index)
#                     logits = out.get("success_logits") if isinstance(out, dict) else out
#                     labels = data.y.float().view(-1)
#                     loss = criterion(logits, labels)
#
#                 loss.backward()
#                 optimizer.step()
#                 total_loss += loss.item()
#                 n_batches += 1
#
#             avg_loss = total_loss / max(1, n_batches)
#             print(f"[ExploitGNN] Epoch {epoch+1:03d} | Loss {avg_loss:.4f}")
#
#         torch.save(model.state_dict(), "exploit_gnn.pt")
#         print("✅ Saved exploit_gnn.pt")
#
#
# # --------- CLI / main orchestrator ----------
# def parse_args():
#     p = argparse.ArgumentParser()
#     p.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
#     p.add_argument("--epochs", type=int, default=20)
#     p.add_argument("--batch-size", type=int, default=256)
#     p.add_argument("--train-network", action="store_true")
#     p.add_argument("--train-vuln", action="store_true")
#     p.add_argument("--train-exploit", action="store_true")
#     p.add_argument("--build-cache-only", action="store_true")
#     return p.parse_args()
#
# def main():
#     args = parse_args()
#     trainer = UnifiedTrainer(device=args.device)
#
#     # Build cache once and reuse for all training runs
#     print("🧠 Building graph cache (vendor embeddings + semantic vectors)...")
#     vendor_mgr = VendorEmbeddingManager()
#     graph_cache = build_graph_cache(vendor_mgr)
#
#     # create loaders via BatchPrep if available
#     if BatchPrep is not None:
#         bp_vuln = BatchPrep(graph_cache["data"], batch_size=args.batch_size, num_neighbors=[10,5], shuffle=True)
#         bp_net = BatchPrep(graph_cache["data"], batch_size=args.batch_size, num_neighbors=[10,5], shuffle=True)
#         bp_exp = BatchPrep(graph_cache["data"], batch_size=max(64, args.batch_size//4), num_neighbors=[8,4], shuffle=True)
#
#         loader_vuln = bp_vuln.get_loader()
#         loader_net = bp_net.get_loader()
#         loader_exp = bp_exp.get_loader()
#     else:
#         # fallback single-batch loader
#         loader_vuln = [graph_cache["data"]]
#         loader_net = [graph_cache["data"]]
#         loader_exp = [graph_cache["data"]]
#
#     if args.build_cache_only:
#         print("✅ Cache built, exiting (build-cache-only).")
#         return
#
#     # Decide which models to train
#     if not any([args.train_network, args.train_vuln, args.train_exploit]):
#         # default: train all
#         args.train_network = args.train_vuln = args.train_exploit = True
#
#     if args.train_network:
#         trainer.train_network_gnn(loader=loader_net, epochs=args.epochs, batch_size=args.batch_size)
#
#     if args.train_vuln:
#         trainer.train_vuln_gnn(loader=loader_vuln, epochs=args.epochs, batch_size=args.batch_size)
#
#     if args.train_exploit:
#         trainer.train_exploit_gnn(loader=loader_exp, epochs=max(25, args.epochs), batch_size= max(64, args.batch_size//4))
#
#
# if __name__ == "__main__":
#     main()
