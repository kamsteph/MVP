# new_module/deception_gan_train_with_adb.py
"""
GAN Training Module for Honeynet Configuration Synthesis using ADB samples
"""

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

from defense.deception_learning import HoneynetGenerator, HoneynetDiscriminator
from back_end.database import DatabaseManager  # adjust import path if needed


class HoneynetGANTrainer:
    def __init__(
            self,
            noise_dim: int = 16,
            config_dim: int = 32,
            device: str = "cuda",
            db: DatabaseManager = None,
            max_samples: int = None,
    ):
        # device: CPU or CUDA
        self.device = torch.device(device)

        # model dimensions
        self.noise_dim = noise_dim
        self.config_dim = config_dim

        # initialize networks
        self.generator = HoneynetGenerator(noise_dim, config_dim).to(self.device)
        self.discriminator = HoneynetDiscriminator(config_dim).to(self.device)

        # optimizers & loss
        self.opt_G = optim.Adam(self.generator.parameters(), lr=1e-3)
        self.opt_D = optim.Adam(self.discriminator.parameters(), lr=1e-3)
        self.criterion = nn.BCELoss()

        # Database manager (use provided or create a new one)
        self.db = db if db is not None else DatabaseManager()

        # load and prepare dataset (list of numeric vectors)
        self.dataset_tensor = self._build_dataset_from_adb(max_samples=max_samples)

        # dataloader will be created lazily in train() (so you can change batch_size at call time)
        self.dataloader = None


    # ---------------------------
    # Feature extraction / dataset
    # ---------------------------
    def _doc_to_vector(self, doc: dict) -> np.ndarray:
        """
        Convert an ADB document into a numeric vector for GAN input.
        Extracts key indicators (severity, exploitability, references, etc.)
        and normalizes them.
        """

        vec = []

        # --- 1) CVSS Score ---
        cvss = 0.0
        if "cvss" in doc:
            cvss = float(doc.get("cvss") or 0.0)
        elif "metrics" in doc:  # nested structure
            metrics = doc.get("metrics", {})
            for key in ["cvssMetricV31", "cvssMetricV30", "cvssMetricV2"]:
                if key in metrics:
                    try:
                        cvss = float(metrics[key][0]["cvssData"]["baseScore"])
                        break
                    except Exception:
                        continue
        vec.append(min(cvss / 10.0, 1.0))

        # --- 2) Exploit presence ---
        has_exploit = bool(doc.get("has_exploit") or doc.get("exploit_urls"))
        vec.append(1.0 if has_exploit else 0.0)

        # --- 3) Disputed flag ---
        disputed = 1.0 if doc.get("disputed") else 0.0
        vec.append(disputed)

        # --- 4) Year normalization ---
        year = 0
        if "cve_year" in doc:
            year = int(doc["cve_year"])
        elif "cve_id" in doc and doc["cve_id"].startswith("CVE-"):
            try:
                year = int(doc["cve_id"].split("-")[1])
            except Exception:
                year = 0
        vec.append(max(0.0, min(1.0, (year - 2000) / 100.0)))

        # --- 5) References count (log-scaled) ---
        refs = doc.get("references") or []
        num_refs = len(refs) if isinstance(refs, list) else 0
        vec.append(np.log1p(num_refs) / 5.0)

        # --- 6) Affected components count ---
        affected = doc.get("affected") or []
        num_aff = len(affected) if isinstance(affected, list) else 0
        vec.append(np.tanh(num_aff / 10.0))

        # --- 7) CWE diversity (if exists) ---
        cwes = doc.get("weaknesses") or []
        num_cwes = len(cwes) if isinstance(cwes, list) else 0
        vec.append(np.tanh(num_cwes / 5.0))

        # --- 8) Description length (text info proxy) ---
        desc = doc.get("description", "")
        desc_len = len(desc) if isinstance(desc, str) else 0
        vec.append(min(desc_len / 1000.0, 1.0))

        # pad or truncate to fit config_dim
        while len(vec) < self.config_dim:
            vec.append(0.0)
        vec = vec[:self.config_dim]

        return np.array(vec, dtype=np.float32)

    def _build_dataset_from_adb(self, max_samples: int = None) -> torch.Tensor:
        """
        Query ADB (adb_collection) and convert documents into a single tensor.
        max_samples: limit number of docs for quicker prototyping
        Returns: torch.FloatTensor of shape (N, config_dim)
        """
        # fetch cursor
        cursor = self.db.get_all_threat_intel()

        vectors = []
        count = 0
        for doc in cursor:
            # optional: filter out docs missing cvss or features
            try:
                v = self._doc_to_vector(doc)
                vectors.append(v)
                count += 1
                if max_samples and count >= max_samples:
                    break
            except Exception:
                # skip malformed docs but continue
                continue

        if not vectors:
            # fallback: if DB empty, return random noise so training can still run
            print("[WARN] No ADB vectors found; using random fallback data for training.")
            vectors = [np.random.randn(self.config_dim).astype(np.float32) for _ in range(128)]

        arr = np.stack(vectors, axis=0)
        return torch.from_numpy(arr).float()

    # ---------------------------
    # Training loop
    # ---------------------------
    def train(self, epochs: int = 200, batch_size: int = 32):
        """
        Train the GAN using real samples from ADB.
        Creates a DataLoader from the in-memory dataset.
        """
        # build dataloader here, so batch_size can be passed at train time
        self.dataloader = DataLoader(
            TensorDataset(self.dataset_tensor),
            batch_size=batch_size,
            shuffle=True,
            drop_last=True,
        )

        for epoch in range(epochs):
            epoch_loss_D = 0.0
            epoch_loss_G = 0.0
            # iterate over real minibatches
            for real_batch_tuple in tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
                # real_batch_tuple is a tuple (tensor,), so get [0]
                real_data = real_batch_tuple[0].to(self.device)

                # labels
                bs = real_data.size(0)
                real_labels = torch.ones(bs, 1, device=self.device)
                fake_labels = torch.zeros(bs, 1, device=self.device)

                # -----------------
                # Train Discriminator
                # -----------------
                self.opt_D.zero_grad()

                # D(real)
                pred_real = self.discriminator(real_data)
                loss_real = self.criterion(pred_real, real_labels)

                # generate fake samples
                noise = torch.randn(bs, self.noise_dim, device=self.device)
                fake_data = self.generator(noise).detach()  # detach so D update doesn't backprop to G
                pred_fake = self.discriminator(fake_data)
                loss_fake = self.criterion(pred_fake, fake_labels)

                # total D loss
                loss_D = loss_real + loss_fake
                loss_D.backward()
                self.opt_D.step()

                # -----------------
                # Train Generator
                # -----------------
                self.opt_G.zero_grad()
                # regenerate fake (do not detach â€” G needs grads)
                noise = torch.randn(bs, self.noise_dim, device=self.device)
                fake_data = self.generator(noise)
                pred_fake_for_G = self.discriminator(fake_data)  # D's opinion about fakes
                loss_G = self.criterion(pred_fake_for_G, real_labels)  # want D to think fakes are real
                loss_G.backward()
                self.opt_G.step()

                # accumulate for logging
                epoch_loss_D += loss_D.item()
                epoch_loss_G += loss_G.item()

            # end epoch: log average losses
            avg_D = epoch_loss_D / len(self.dataloader)
            avg_G = epoch_loss_G / len(self.dataloader)
            print(f"[GAN] Epoch {epoch+1} | LossD: {avg_D:.4f} | LossG: {avg_G:.4f}")

        # save
        torch.save(self.generator.state_dict(), "honeynet_generator.pt")
        torch.save(self.discriminator.state_dict(), "honeynet_discriminator.pt")
        print(" GAN training complete and models saved.")
