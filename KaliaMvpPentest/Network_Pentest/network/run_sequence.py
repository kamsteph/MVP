"""
run_sequence.py
------------------
Unified runner for the MVP GNN pipeline:
1. Build graph cache
2. Run semantic pipeline
3. Train all GNN modules

Usage:
    python run_sequence.py --device cuda --epochs 30 --batch-size 512
"""

import os
import sys
import argparse
import logging
import subprocess
from datetime import datetime
import torch

# ----------------------------------------------------
# UTF-8 output for Windows consoles
# ----------------------------------------------------
sys.stdout.reconfigure(encoding="utf-8")
sys.stderr.reconfigure(encoding="utf-8")

# ----------------------------------------------------
# Logging setup
# ----------------------------------------------------
os.makedirs("logs", exist_ok=True)
log_file = f"logs/run_all_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("RUN_SEQUENCE")

# ----------------------------------------------------
# Define main execution
# ----------------------------------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--epochs", type=int, default=25)
    parser.add_argument("--batch-size", type=int, default=512)
    parser.add_argument("--num-neighbors", type=int, nargs="+", default=[10, 5])
    parser.add_argument("--skip-cache", action="store_true", help="Skip graph cache build step.")
    parser.add_argument("--skip-semantic", action="store_true", help="Skip semantic pipeline step.")
    parser.add_argument("--use-focal", action="store_true", help="Use focal loss instead of BCE.")
    args = parser.parse_args()

    base_dir = os.path.dirname(os.path.abspath(__file__))

    # ----------------------------------------------------
    # STEP 1: Build Graph Cache
    # ----------------------------------------------------
    graph_cache_path = os.path.join(base_dir, "cache", "network_graph.pt")
    if not os.path.exists(graph_cache_path) or args.skip_cache:
        logger.info("BUILDING GRAPH CACHE...")
        try:
            subprocess.run(
                [sys.executable, os.path.join(base_dir, "db_manip", "build_graph_caching.py")],
                check=True
            )
            open(os.path.join(base_dir, "cache", "cache_built.flag"), "w").close()
            logger.info("GRAPH CACHE BUILT SUCCESSFULLY.")
        except subprocess.CalledProcessError as e:
            logger.error(f"GRAPH CACHE BUILD FAILED: {e}")
            sys.exit(1)
    else:
        logger.info("SKIPPING GRAPH CACHE BUILD (ALREADY EXISTS).")

    # ----------------------------------------------------
    # STEP 2: Semantic Pipeline
    # ----------------------------------------------------
    semantic_flag = os.path.join(base_dir, "cache", "semantic_done.flag")
    if not os.path.exists(semantic_flag) and not args.skip_semantic:
        logger.info("RUNNING SEMANTIC PIPELINE...")
        try:
            subprocess.run(
                [sys.executable, os.path.join(base_dir, "db_manip", "semantic_compute.py")],
                check=True
            )
            open(semantic_flag, "w").close()
            logger.info("SEMANTIC PIPELINE COMPLETED SUCCESSFULLY.")
        except subprocess.CalledProcessError as e:
            logger.error(f"SEMANTIC PIPELINE FAILED: {e}")
            sys.exit(1)
    else:
        subprocess.run(["python", "db_manip/semantic_compute.py"], check=True)
        logger.info("SEMANTIC PIPELINE LAUNCHED SUCCESSFULLY.")

    # ----------------------------------------------------
    # STEP 3: Train All GNNs
    # ----------------------------------------------------
    logger.info("LAUNCHING UNIFIED GNN TRAINING...")
    cmd = [
        sys.executable,
        os.path.join(base_dir, "Brain", "train_all_gnns.py"),
        "--device", args.device,
        "--epochs", str(args.epochs),
        "--batch-size", str(args.batch_size),
        "--num-neighbors", *map(str, args.num_neighbors)
    ]
    if args.use_focal:
        cmd.append("--use-focal")

    try:
        subprocess.run(cmd, check=True)
        logger.info("GNN TRAINING FINISHED SUCCESSFULLY.")
    except subprocess.CalledProcessError as e:
        logger.error(f"GNN TRAINING FAILED: {e}")
        sys.exit(1)

    logger.info("PIPELINE COMPLETED SUCCESSFULLY.")

# ----------------------------------------------------
# Launch main
# ----------------------------------------------------
if __name__ == "__main__":
    main()
