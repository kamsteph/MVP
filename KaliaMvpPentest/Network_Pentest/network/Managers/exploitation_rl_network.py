# new_module/exploitation_agent.py
"""
DQN-style ExploitationAgent (baseline).

- Minimal DQN-like agent using PyTorch.
- Stores experiences in MongoDB via DatabaseManager.save_learning_data.
- This is a lightweight baseline; extend it for production.
"""
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import List, Dict, Any
from back_end.database import DatabaseManager
from back_end.utils.colored_logger import get_logger
from datetime import datetime

logger = get_logger(__name__, component="EXPLOITATION_AGENT", region="LEARNING")

class QNet(nn.Module):
    def __init__(self, input_dim: int, hidden: int, output_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, output_dim)
        )
    def forward(self, x):
        return self.net(x)

def default_state_encoder(scan_result: Dict[str, Any], candidates: List[Dict[str, Any]]) -> np.ndarray:
    n_ports = len(scan_result.get("open_ports", [])) if scan_result.get("open_ports") else 0
    n_vulns = len(scan_result.get("vulnerabilities", [])) if scan_result.get("vulnerabilities") else 0
    highest_cvss = 0.0
    for c in candidates:
        cv = c.get("cvss_score") or c.get("cvss", 0) or 0.0
        try:
            cv = float(cv)
        except Exception:
            cv = 0.0
        highest_cvss = max(highest_cvss, cv)
    return np.array([n_ports, n_vulns, highest_cvss], dtype=np.float32)

class ExploitationAgent:
    def __init__(self, db: DatabaseManager, input_dim: int = 4, hidden: int = 64, lr: float = 1e-3, device: str = "cpu"):
        self.db = db
        self.device = torch.device(device)
        self.qnet = QNet(input_dim, hidden, 1).to(self.device)
        self.optimizer = optim.Adam(self.qnet.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()

    def select_action(self, scan_result: Dict[str, Any], candidates: List[Dict[str, Any]], epsilon: float = 0.1) -> int:
        if not candidates:
            return -1
        if random.random() < epsilon:
            return random.randrange(len(candidates))
        state = default_state_encoder(scan_result, candidates)
        q_values = []
        for c in candidates:
            cv = float(c.get("cvss_score") or c.get("cvss", 0) or 0.0)
            inp = np.concatenate([state, np.array([cv], dtype=np.float32)])
            tensor = torch.from_numpy(inp).to(self.device)
            with torch.no_grad():
                q = self.qnet(tensor).item()
            q_values.append(q)
        return int(np.argmax(q_values))

    def store_experience(self, state_vec: List[float], action_idx: int, reward: float, next_state_vec: List[float], done: bool):
        doc = {
            "state": list(map(float, state_vec)),
            "action": int(action_idx),
            "reward": float(reward),
            "next_state": list(map(float, next_state_vec)),
            "done": bool(done),
            "ts": datetime.now().isoformat() + "Z"
        }
        try:
            self.db.save_learning_data(doc)
        except Exception as e:
            logger.warning("Failed to store experience: %s", e)

    def sample_experiences(self, batch_size: int = 32):
        try:
            total = self.db.learning_data.count_documents({})
            if total == 0:
                return []
            # use aggregation sample for random docs
            samples = list(self.db.learning_data.aggregate([{"$sample": {"size": min(batch_size, total)}}]))
            return samples
        except Exception as e:
            logger.warning("Sampling failed: %s", e)
            return []

    def learn_from_experience(self, batch_size: int = 16):
        samples = self.sample_experiences(batch_size)
        if not samples:
            return 0.0
        losses = []
        for s in samples:
            state = torch.tensor(s["state"], dtype=torch.float32).to(self.device)
            next_state = torch.tensor(s["next_state"], dtype=torch.float32).to(self.device)
            reward = float(s["reward"])
            with torch.no_grad():
                q_next = self.qnet(next_state)
                max_q_next = q_next.max().item() if q_next.numel() > 0 else 0.0
            target = torch.tensor([reward + 0.99 * max_q_next], dtype=torch.float32).to(self.device)
            q_val = self.qnet(state)
            loss = self.loss_fn(q_val, target)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            losses.append(loss.item())
        return float(sum(losses) / max(1, len(losses)))
