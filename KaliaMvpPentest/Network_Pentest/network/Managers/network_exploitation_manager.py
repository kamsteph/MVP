# file: Network_Pentest/network/Managers/exploitation_ai_agent_v2.py

import math
from datetime import datetime
from typing import List, Dict, Any, Optional

import numpy as np
import torch
import torch.nn as nn

from Network_Pentest.network.Managers.RL.exploitation_rl_network import ExploitationAgent
from Network_Pentest.network.tool.base_class import list_registered_tools
from back_end.database import DatabaseManager
from back_end.utils.colored_logger import get_logger
from Network_Pentest.network.Brain.gnn_models import ExploitManagerGNN

logger = get_logger(__name__, component="EXPLOIT_AI_AGENT_V2", region="AI")


class ExploitationAIAgentV2:
    """
    Exploit AI agent (prototype, pre-exploitation, level_1 only).

    Responsibilities:
    -----------------
    - Take level_1 candidates from VAI.
    - Use ExploitManagerGNN as a *simulation* layer (predict success_prob).
    - Persist per-candidate state in MongoDB:
        * simulation_score, simulation_success
        * production_status, production_failure_count
        * current_level (level_1 / level_2 / level_3)
    - Always attempt production (even if simulation predicted failure),
      but prioritize simulated-success candidates first.
    - On repeated production failures (>= failure_threshold),
      promote candidate/host to level_2.
    - Stop logic at level_1 compromise (no lateral movement here).
    """

    def __init__(
            self,
            db: DatabaseManager,
            model_path: Optional[str] = None,
            device: str = "cpu",
            failure_threshold: int = 2,
            sim_success_threshold: float = 0.6,
    ):
        self.db = db
        self.device = torch.device(device)
        self.failure_threshold = int(failure_threshold)
        self.sim_success_threshold = float(sim_success_threshold)

        # --- Vendor embedding setup (static, lightweight) ---
        self.vendor_to_idx = self._build_vendor_vocab()
        # reserve index 0 for "unknown"
        if "unknown" not in self.vendor_to_idx:
            self.vendor_to_idx["unknown"] = 0
        self.vendor_emb_dim = 8
        self.vendor_emb = nn.Embedding(
            num_embeddings=len(self.vendor_to_idx),
            embedding_dim=self.vendor_emb_dim
        ).to(self.device)

        # --- Context encoder (GNN) used as "simulation" --
        in_channels = 8 + self.vendor_emb_dim
        self.encoder = ExploitManagerGNN(
            in_channels=in_channels,
            hidden=48,
            heads=6
        ).to(self.device)

        if model_path:
            self.encoder.load_state_dict(torch.load(model_path, map_location=self.device,weights_only=False))

        self.encoder.eval()  # we only use it for inference in this prototype

        # --- Reinforcement learner (optional for later) ---
        # Here we use it to log experiences, but you can later extend it
        # to choose tools / sequences.
        self.policy = ExploitationAgent(
            db=db,
            input_dim=5,
            hidden=64,
            lr=1e-3,
            device=device
        )

        # --- Tool registry (not actually executed in this prototype) ---
        self.available_tools = list_registered_tools()
        logger.info(f"[EXP_V2] Available exploit tools: {list(self.available_tools.keys())}")

    # ====================================================
    # Persistent state helpers (MongoDB)
    # ====================================================
    def _make_state_key(self, task_id: str, candidate: Dict[str, Any]) -> Dict[str, Any]:
        """
        Build a deterministic key for the candidate:
        - task_id
        - ip
        - surface (if present)
        - vuln_id (cve or id or fallback)
        """
        ip = candidate.get("ip", "unknown_ip")
        surface = candidate.get("surface", "generic")
        vuln_id = candidate.get("cve") or candidate.get("id")
        if not vuln_id:
            # fallback: use port + service + score fingerprint
            vuln_id = f"{ip}:{surface}:{candidate.get('port', 'na')}:{candidate.get('service', 'na')}"
        return {
            "type": "exp_candidate_state",
            "task_id": task_id,
            "ip": ip,
            "surface": surface,
            "vuln_id": str(vuln_id),
        }

    def _load_state(self, key: Dict[str, Any]) -> Dict[str, Any]:
        """
        Load candidate state from standard_db or initialize a default one.
        """
        doc = self.db.standard_db.find_one(key)
        if not doc:
            # default state
            now = datetime.now()
            base = {
                **key,
                "level": "level_1",
                "simulation_score": 0.0,
                "simulation_success": False,
                "production_status": "not_started",  # "success" | "failed" | "not_started"
                "production_failure_count": 0,
                "history": [],
                "created_at": now,
                "updated_at": now,
            }
            return base
        return doc

    def _save_state(self, state: Dict[str, Any]):
        """
        Upsert candidate state in standard_db.
        """
        state = dict(state)
        state["updated_at"] = datetime.now()
        key = {
            "type": state["type"],
            "task_id": state["task_id"],
            "ip": state["ip"],
            "surface": state["surface"],
            "vuln_id": state["vuln_id"],
        }
        self.db.standard_db.update_one(key, {"$set": state}, upsert=True)

    def _append_history(self, state: Dict[str, Any], event: str, details: Dict[str, Any]):
        h = state.setdefault("history", [])
        h.append({
            "ts": datetime.now().isoformat(),
            "event": event,
            "details": details,
        })

    # ====================================================
    # Feature & graph construction (simulation)
    # ====================================================
    def _build_vendor_vocab(self) -> Dict[str, int]:
        """
        Build vendor vocabulary from ADB or static fallback.
        Uses `adb` property on DatabaseManager.
        """
        vendors = set()
        try:
            records = self.db.adb.find({}, {"vendor": 1})
            for r in records:
                if r.get("vendor"):
                    vendors.add(str(r["vendor"]).lower())
        except Exception as e:
            logger.warning(f"[EXP_V2] Vendor vocab fallback (db unavailable): {e}")
            vendors.update(["microsoft", "apache", "oracle", "cisco", "linux", "ibm", "unknown"])
        if not vendors:
            vendors.add("unknown")
        return {v: i for i, v in enumerate(sorted(vendors))}

    def _encode_candidate_node(self, c: Dict[str, Any]) -> torch.Tensor:
        """
        Encode a single candidate into numeric + vendor embedding vector.
        8 core features + vendor_emb_dim.
        """
        sev = self._normalize_cvss(c.get("severity", c.get("cvss", 0.0)))
        port_norm = self._normalize_port(c.get("port", 0.0))
        svc_name = str(c.get("service", "")).lower()
        is_http = 1.0 if "http" in svc_name else 0.0
        prev = 1.0 if c.get("previous_success", False) else 0.0
        age_days = float(c.get("exploit_age", 0.0) or 0.0)
        age_norm = max(0.0, min(1.0, 1.0 - (age_days / 3650.0)))  # recent=1.0, older→0
        has_exploit = 1.0 if c.get("has_exploit", False) else 0.0
        ref_cnt = self._safe_len((c.get("exploit_urls") or []) + (c.get("references") or []))
        ref_norm = min(1.0, math.log1p(ref_cnt) / 5.0)
        cwe_count = min(1.0, self._safe_len(c.get("weaknesses", [])) / 5.0)

        vendor = str(c.get("vendor", "unknown")).lower()
        vendor_idx = self.vendor_to_idx.get(vendor, self.vendor_to_idx.get("unknown", 0))
        vendor_vec = self.vendor_emb(torch.tensor(vendor_idx, device=self.device))

        node_core = torch.tensor([
            sev, port_norm, is_http, prev, age_norm,
            has_exploit, ref_norm, cwe_count
        ], dtype=torch.float, device=self.device)

        return torch.cat([node_core, vendor_vec], dim=0)

    def _build_graph_features(self, candidates: List[Dict[str, Any]], knn_k: int = 4) -> np.ndarray:
        """
        Build a small similarity graph between candidates and use the GNN to
        output a success logit per candidate (our *simulation* signal).
        """
        if not candidates:
            return np.zeros((0,), dtype=np.float32)

        # Build node feature matrix
        nodes = [self._encode_candidate_node(c) for c in candidates]
        x = torch.stack(nodes, dim=0)  # [N, in_channels]

        # Build edges by cosine similarity + shared port
        meta_np = x.detach().cpu().numpy()
        N, D = meta_np.shape
        edges = []

        if N > 1:
            norms = np.linalg.norm(meta_np, axis=1, keepdims=True)
            norms[norms == 0] = 1.0
            meta_normed = meta_np / norms
            sim = meta_normed @ meta_normed.T

            for i in range(N):
                topk = np.argsort(-sim[i])[: knn_k + 1]
                for j in topk:
                    if j == i:
                        continue
                    if sim[i, j] > 0.3:
                        edges.append((i, j))
                        edges.append((j, i))

        # Extra edges for same-port candidates
        for i in range(N):
            for j in range(i):
                if int(candidates[i].get("port", -1)) == int(candidates[j].get("port", -2)):
                    edges.append((i, j)); edges.append((j, i))

        edge_index = torch.tensor(edges, dtype=torch.long, device=self.device).t().contiguous() \
            if edges else torch.empty((2, 0), dtype=torch.long, device=self.device)

        self.encoder.eval()
        with torch.no_grad():
            out = self.encoder(x.to(self.device), edge_index)
            logits = out["success_logits"].detach().cpu().numpy().flatten()

        return logits

    # ====================================================
    # Public API: run pre-exploitation for level_1
    # ====================================================

    def run_level1_pre_exploitation(self, task_id: str, candidates: List[Dict[str, Any]]) -> Dict[str, Any]:

        """
          Main entry for this prototype:

          - Filter candidates at level_1.
          - Use GNN to simulate success likelihood.
          - Persist simulation_score & simulation_success in DB.
          - Reorder candidates:
              1) simulated_success = True
              2) simulated_success = False
          - For each candidate in that order:
              * attempt "production" (no real exploit, only prototype logic)
              * update production_status & production_failure_count
              * if failures >= threshold → promote to level_2.
          - Return a summary for UI / logging.
          """
        # Option-B integration:
        # Always attempt all level_1 candidates — but simulate-first sorting
        # determines which ones get attempted FIRST.
        level1_candidates = [c for c in candidates if c.get("level") == "level_1"]

        if not level1_candidates:
            logger.info(f"[EXP_V2] Task {task_id}: No level_1 candidates to process.")
            return {"task_id": task_id, "processed": 0, "message": "no_level1_candidates"}

            # --- Simulation (GNN-based) ---
        logits = self._build_graph_features(level1_candidates)
        probs = 1 / (1 + np.exp(-logits))  # sigmoid

        # Attach sim scores & persist
        sim_info = []
        for c, p in zip(level1_candidates, probs):
            key = self._make_state_key(task_id, c)
            state = self._load_state(key)
            state["simulation_score"] = float(p)
            state["simulation_success"] = bool(p >= self.sim_success_threshold)
            self._append_history(state, "simulation_update", {
                "simulation_score": state["simulation_score"],
                "simulation_success": state["simulation_success"],
            })
            self._save_state(state)
            sim_info.append((c, state))

        # --- Reorder: first sim_success=True, then others ---
        # --- Prioritization for Option B ---
        sim_success_list = sorted(
            [(c, st) for (c, st) in sim_info if st["simulation_success"]],
            key=lambda x: x[1]["simulation_score"],
            reverse=True
        )

        sim_fail_list = sorted(
            [(c, st) for (c, st) in sim_info if not st["simulation_success"]],
            key=lambda x: x[1]["simulation_score"],
            reverse=True
        )

        # Order:
        #   1) all simulated-success (high→low)
        #   2) all simulated-failure (high→low)


        #sim_success_list = [ (c, st) for (c, st) in sim_info if st["simulation_success"] ]
        #sim_fail_list    = [ (c, st) for (c, st) in sim_info if not st["simulation_success"] ]

        ordered = sim_success_list + sim_fail_list

        summary = {
            "task_id": task_id,
            "processed": len(ordered),
            "attempts": [],
        }

        # --- Production attempts (prototype) ---
        for candidate, state in ordered:
            # Skip if already succeeded earlier
            if state.get("production_status") == "success":
                self._append_history(state, "production_skip_already_success", {})
                self._save_state(state)
                summary["attempts"].append({
                    "ip": state["ip"],
                    "surface": state["surface"],
                    "vuln_id": state["vuln_id"],
                    "result": "skipped_already_success",
                })
                continue

            # If this candidate reached failure threshold, mark level_2
            if state.get("production_failure_count", 0) >= self.failure_threshold:
                state["level"] = "level_2"
                self._append_history(state, "promote_to_level2_due_to_failures", {
                    "failures": state["production_failure_count"]
                })
                self._save_state(state)
                summary["attempts"].append({
                    "ip": state["ip"],
                    "surface": state["surface"],
                    "vuln_id": state["vuln_id"],
                    "result": "skipped_promoted_level2",
                })
                continue

            # Perform a *prototype* production attempt:
            prod_success = self._attempt_production(candidate, state)

            if prod_success:
                state["production_status"] = "success"
                self._append_history(state, "production_success", {})
                self._save_state(state)
                summary["attempts"].append({
                    "ip": state["ip"],
                    "surface": state["surface"],
                    "vuln_id": state["vuln_id"],
                    "result": "production_success",
                })
                # NOTE: we do *not* move to level_2 here; prototype stops at level_1.
            else:
                # failure → increment counter, possibly promote
                cur_fail = int(state.get("production_failure_count", 0)) + 1
                state["production_failure_count"] = cur_fail
                self._append_history(state, "production_failure", {
                    "production_failure_count": cur_fail
                })

                # check threshold
                if cur_fail >= self.failure_threshold:
                    state["level"] = "level_2"
                    self._append_history(state, "promote_to_level2_due_to_failures", {
                        "failures": cur_fail
                    })
                    result_str = "production_failed_promoted_level2"
                else:
                    result_str = "production_failed_retry_possible"

                self._save_state(state)
                summary["attempts"].append({
                    "ip": state["ip"],
                    "surface": state["surface"],
                    "vuln_id": state["vuln_id"],
                    "result": result_str,
                })

        return summary

    # ====================================================
    # Prototype "production" attempt
    # ====================================================
    def _attempt_production(self, candidate: Dict[str, Any], state: Dict[str, Any]) -> bool:
        """
        Prototype-only *production* attempt.

        In the real system, this is where you'd:
        - choose a tool / exploit template
        - run it against the real host
        - observe success/failure
        - feed reward to RL agent
        - etc.

        Here, we simulate an outcome based on:
        - simulation_score
        - some noise
        - and we log the experience for RL.
        """
        sim_score = float(state.get("simulation_score", 0.0))

        # Simple stochastic outcome:
        #   base_prob = sim_score * 0.7 + 0.1 (so very low sim_score still has tiny chance)
        base_prob = max(0.0, min(1.0, sim_score * 0.7 + 0.1))
        outcome = np.random.rand() < base_prob

        # RL logging: state_vec = [sim_score, num_failures, is_sim_success, score, exploit_prob]
        # We try to build something simple for now.
        failures = float(state.get("production_failure_count", 0))
        is_sim_success = 1.0 if state.get("simulation_success") else 0.0
        score = float(candidate.get("score", 0.0))
        exploit_prob = float(candidate.get("exploit_prob", 0.0))

        state_vec = [sim_score, failures, is_sim_success, score, exploit_prob]
        next_state_vec = [sim_score, failures + (0 if outcome else 1), is_sim_success, score, exploit_prob]
        reward = 1.0 if outcome else -1.0

        try:
            self.policy.store_experience(
                state_vec=state_vec,
                action_idx=0,  # single abstract "exploit" action in this prototype
                reward=reward,
                next_state_vec=next_state_vec,
                done=False,
                candidate_info=candidate
            )
        except Exception as e:
            logger.warning("[EXP_V2] Failed to store RL experience: %s", e)

        return bool(outcome)

    # ====================================================
    # Utility normalizers
    # ====================================================
    @staticmethod
    def _normalize_port(p):
        try:
            p = float(p)
        except Exception:
            return 0.0
        if p <= 1023:
            return 0.25
        if p <= 49151:
            return 0.5
        return 1.0

    @staticmethod
    def _normalize_cvss(cvss):
        try:
            return max(0.0, min(1.0, float(cvss) / 10.0))
        except Exception:
            return 0.0

    @staticmethod
    def _safe_len(x):
        try:
            return len(x) if x else 0
        except Exception:
            return 0

    def extract_level1_candidates_from_honeynet(self, honeypots: List[Dict[str, Any]]):
        """
        Extract level_1 candidates from deployed honeynet metadata.
        This ensures EXP V2 works directly after DeceptionLearning.
        """
        level1 = []
        for hp in honeypots:
            # hp structure:
            #   { "honeypot_id", "ip", "level", "simulated_for" }
            if hp.get("level") == "level_1":
                level1.append({
                    "ip": hp.get("simulated_for"),
                    "surface": "generic",
                    "port": 80,
                    "service": "simulated",
                    "vendor": "unknown",
                    "score": 0.0,
                    "exploit_prob": 0.0,
                    "level": "level_1",
                })
        return level1
