import torch
import torch.nn as nn
import numpy as np
import math
from typing import List, Dict, Any

from Network_Pentest.network.Managers.exploitation_rl_network import ExploitationAgent, default_state_encoder
from Network_Pentest.network.tool.base_class import get_tool_by_name, list_registered_tools
from back_end.database import DatabaseManager
from back_end.utils.colored_logger import get_logger
from Network_Pentest.network.Brain.gnn_models import ExploitManagerGNN

logger = get_logger(__name__, component="EXPLOIT_AI_AGENT_V2", region="AI")
from Network_Pentest.network.dynamic_vendor_embedding import VendorEmbeddingManager


class ExploitationAIAgentV2:
    def __init__(self, db: DatabaseManager, model_path: str = None, device: str = "cpu"):
        self.db = db
        self.device = torch.device(device)

        # --- Vendor embedding setup ---
        self.vendor_to_idx = self._build_vendor_vocab()
        self.vendor_emb_dim = 8
        self.vendor_emb = nn.Embedding(len(self.vendor_to_idx), self.vendor_emb_dim).to(self.device)
        self.vendor_mgr = VendorEmbeddingManager(initial_vendor_list=..., emb_dim=16, sb_model_name="all-MiniLM-L6-v2", device=device)
        #vendor_vec = self.vendor_mgr.forward_vendor(vendor)

        # --- Context encoder (GNN) ---
        self.encoder = ExploitManagerGNN(in_channels=8 + self.vendor_emb_dim, hidden=48, heads=6).to(self.device)
        if model_path:
            self.encoder.load_state_dict(torch.load(model_path, map_location=self.device))
        self.encoder.eval()

        # --- Reinforcement learner ---
        self.policy = ExploitationAgent(db, input_dim=5, hidden=64, lr=1e-3, device=device)

        # --- Tool registry ---
        self.available_tools = list_registered_tools()
        logger.info(f"Available exploit tools: {list(self.available_tools.keys())}")

    # ====================================================
    # Utility normalizers
    # ====================================================
    @staticmethod
    def _normalize_port(p):
        try:
            p = float(p)
        except Exception:
            return 0.0
        if p <= 1023:
            return 0.25
        if p <= 49151:
            return 0.5
        return 1.0

    @staticmethod
    def _normalize_cvss(cvss):
        try:
            return max(0.0, min(1.0, float(cvss) / 10.0))
        except Exception:
            return 0.0

    @staticmethod
    def _safe_len(x):
        try:
            return len(x) if x else 0
        except Exception:
            return 0

    def _build_vendor_vocab(self) -> Dict[str, int]:
        """Builds vendor vocabulary from database or static fallback."""
        vendors = set()
        try:
            records = self.db.get_collection("augmented_database").find({}, {"vendor": 1})
            for r in records:
                if r.get("vendor"):
                    vendors.add(str(r["vendor"]).lower())
        except Exception as e:
            logger.warning(f"Vendor vocab fallback (db unavailable): {e}")
            vendors.update(["microsoft", "apache", "oracle", "cisco", "linux", "ibm"])
        return {v: i for i, v in enumerate(sorted(vendors))}

    # ====================================================
    # Feature & graph construction
    # ====================================================
    def build_graph_features(self, candidates: List[Dict[str, Any]], knn_k: int = 4) -> np.ndarray:
        nodes = []
        meta_embeddings = []
        edges = []

        for i, c in enumerate(candidates):
            sev = self._normalize_cvss(c.get("severity", c.get("cvss", 0.0)))
            port_norm = self._normalize_port(c.get("port", 0.0))
            is_http = 1.0 if str(c.get("service", "")).lower() == "http" else 0.0
            prev = 1.0 if c.get("previous_success", False) else 0.0
            age_days = float(c.get("exploit_age", 0.0))
            age_norm = max(0.0, min(1.0, 1.0 - (age_days / 3650.0)))  # recent=1.0
            has_exploit = 1.0 if c.get("has_exploit", False) else 0.0
            ref_cnt = min(1.0, math.log1p(self._safe_len(c.get("exploit_urls", []) + c.get("references", []))) / 5.0)
            cwe_count = min(1.0, self._safe_len(c.get("weaknesses", [])) / 5.0)

            vendor = str(c.get("vendor", "")).lower()
            vendor_idx = self.vendor_to_idx.get(vendor, 0)
            vendor_vec = self.vendor_emb(torch.tensor(vendor_idx, device=self.device))

            # numeric core features
            node_core = torch.tensor([
                sev, port_norm, is_http, prev, age_norm,
                has_exploit, ref_cnt, cwe_count
            ], dtype=torch.float, device=self.device)

            node_vec = torch.cat([node_core, vendor_vec], dim=0)
            nodes.append(node_vec)
            meta_embeddings.append(node_vec.detach().cpu().numpy())

        x = torch.stack(nodes, dim=0) if nodes else torch.zeros((0, self.encoder.in_channels), device=self.device)

        # --- Build edges ---
        meta_np = np.stack(meta_embeddings, axis=0) if meta_embeddings else np.zeros((0, x.shape[1]))
        if len(meta_np) > 1:
            norms = np.linalg.norm(meta_np, axis=1, keepdims=True)
            norms[norms == 0] = 1.0
            meta_normed = meta_np / norms
            sim = meta_normed @ meta_normed.T
            N = meta_np.shape[0]
            for i in range(N):
                topk = np.argsort(-sim[i])[: knn_k + 1]
                for j in topk:
                    if j == i:
                        continue
                    if sim[i, j] > 0.3:
                        edges.append((i, j))
                        edges.append((j, i))
        for i in range(len(candidates)):
            for j in range(i):
                if int(candidates[i].get("port", 0)) == int(candidates[j].get("port", -1)):
                    edges.append((i, j)); edges.append((j, i))

        edge_index = torch.tensor(edges, dtype=torch.long, device=self.device).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long, device=self.device)

        with torch.no_grad():
            out = self.encoder(x, edge_index)
            logits = out["success_logits"].detach().cpu().numpy().flatten()
        return logits
