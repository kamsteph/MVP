# File: new_module/exploitation_agent.py

from collections import deque
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime
from typing import List, Dict, Any

from Network_Pentest.network.Brain.hyperparams import HINT_BAYES_ALPHA, HINT_BAYES_BETA
from Network_Pentest.network.Managers.RL.exploit_selector import record_outcome
from back_end.database import DatabaseManager
from back_end.utils.colored_logger import get_logger

logger = get_logger(__name__, component="EXPLOITATION_AGENT", region="LEARNING")


class QNet(nn.Module):
    def __init__(self, input_dim: int, hidden: int, output_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, output_dim)
        )

    def forward(self, x):
        return self.net(x)


class ExploitationAgent:
    def __init__(
            self,
            db: DatabaseManager,
            input_dim: int = 4,  # must match state+action vector size
            hidden: int = 64,
            lr: float = 1e-3,
            device: str = "cpu",
            gamma: float = 0.99,
            buffer_size: int = 10000,
            min_replay_size: int = 128,
            batch_size: int = 32,
            target_update_freq: int = 1000
    ):
        self.db = db
        self.device = torch.device(device)

        # Main and target networks
        self.qnet = QNet(input_dim, hidden, 1).to(self.device)
        self.target_qnet = QNet(input_dim, hidden, 1).to(self.device)
        self.target_qnet.load_state_dict(self.qnet.state_dict())
        self.target_qnet.train()

        self.optimizer = optim.Adam(self.qnet.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        self.gamma = gamma

        # Replay buffer
        self.replay = deque(maxlen=buffer_size)
        self.min_replay_size = min_replay_size
        self.batch_size = batch_size
        self.total_steps = 0
        self.target_update_freq = target_update_freq

    # -------------------------
    # State encoding
    # -------------------------
    def default_state_encoder(self, scan_result: Dict[str, Any], candidates: List[Dict[str, Any]]) -> np.ndarray:
        n_ports = len(scan_result.get("open_ports", [])) if scan_result.get("open_ports") else 0
        n_vulns = len(scan_result.get("vulnerabilities", [])) if scan_result.get("vulnerabilities") else 0
        highest_cvss = 0.0
        for c in candidates:
            try:
                cv = float(c.get("cvss_score") or c.get("cvss") or 0.0)
            except Exception:
                cv = 0.0
            highest_cvss = max(highest_cvss, cv)
        return np.array([n_ports, n_vulns, highest_cvss], dtype=np.float32)

    # -------------------------
    # Action selection with Bayesian hints
    # -------------------------
    def select_action(self, scan_result: Dict[str, Any], candidates: List[Dict[str, Any]], epsilon: float = None) -> int:
        if not candidates:
            return -1
        eps = epsilon if epsilon is not None else self.hp["epsilon"]
        if random.random() < eps:
            return random.randrange(len(candidates))

        state = self.default_state_encoder(scan_result, candidates)
        q_values = []

        for c in candidates:
            cv = float(c.get("cvss_score") or c.get("cvss") or 0.0)
            inp = np.concatenate([state, np.array([cv], dtype=np.float32)])
            tensor = torch.from_numpy(inp).unsqueeze(0).to(self.device)
            with torch.no_grad():
                q = self.qnet(tensor).item()

            # --- Integrate Bayesian hint ---
            attack_surface = str(c.get("attack_surface", "generic"))
            tool_name = str(c.get("tool_name", "unknown"))
            # fetch the current hint score from DB (simulate it here)
            coll = self.db.db.get_collection("tools_priority")
            hint_doc = coll.find_one({"attack_surface": attack_surface, "tool": tool_name})
            hint_score = hint_doc.get("score", HINT_BAYES_ALPHA / (HINT_BAYES_ALPHA + HINT_BAYES_BETA)) if hint_doc else 0.5

            # combine Q-value + hint (linear bias, tunable)
            alpha_hint = 1.0  # weight of the hint
            q_values.append(q + alpha_hint * hint_score)

        return int(np.argmax(q_values))

    # -------------------------
    # Update Bayesian hints after storing experience
    # -------------------------
    def store_experience(self, state_vec: List[float], action_idx: int, reward: float, next_state_vec: List[float], done: bool, candidate_info: Dict[str, Any]):
        # standard RL storage
        exp = {
            "state": list(map(float, state_vec)),
            "action": int(action_idx),
            "reward": float(reward),
            "next_state": list(map(float, next_state_vec)),
            "done": bool(done),
            "ts": datetime.now().isoformat() + "Z"
        }
        self.replay.append(exp)
        try:
            self.db.save_learning_data(exp)
        except Exception as e:
            logger.warning("Failed to persist experience to DB: %s", e)

        # --- Update hint system with outcome ---
        success = reward > 0  # you can adjust this criterion
        attack_surface = str(candidate_info.get("attack_surface", "generic"))
        tool_name = str(candidate_info.get("tool_name", "unknown"))
        record_outcome(attack_surface, tool_name, success)

    def sample_experiences(self, batch_size: int = None):
        bs = batch_size or self.batch_size
        if len(self.replay) < self.min_replay_size:
            return []
        return random.sample(self.replay, min(bs, len(self.replay)))

    # -------------------------
    # Learning step
    # -------------------------
    def learn_from_experience(self, batch_size: int = None):
        samples = self.sample_experiences(batch_size)
        if not samples:
            return 0.0

        # Prepare batch tensors
        states = torch.tensor([s["state"] for s in samples], dtype=torch.float32, device=self.device)
        next_states = torch.tensor([s["next_state"] for s in samples], dtype=torch.float32, device=self.device)
        rewards = torch.tensor([s["reward"] for s in samples], dtype=torch.float32, device=self.device).unsqueeze(1)
        dones = torch.tensor([s["done"] for s in samples], dtype=torch.float32, device=self.device).unsqueeze(1)

        # Current Q-values
        q_vals = self.qnet(states)

        # Target Q-values (no grad)
        with torch.no_grad():
            q_next = self.target_qnet(next_states)
            target = rewards + (1.0 - dones) * (self.gamma * q_next)

        # Loss and backward
        loss = self.loss_fn(q_vals, target)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.qnet.parameters(), max_norm=10.0)
        self.optimizer.step()

        # Update target network periodically
        self.total_steps += 1
        if self.total_steps % self.target_update_freq == 0:
            self.target_qnet.load_state_dict(self.qnet.state_dict())
            logger.info(f"[ExploitationAgent] Target network updated at step {self.total_steps}")

        return float(loss.item())
