import torch
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Any
from torch_geometric.data import Data

from Network_Pentest.network.Brain.gnn_models import VulnerabilityManagerGNN
from Network_Pentest.network.Brain.hyperparams import W_LHF, W_SERVICES, W_CVSS
from Network_Pentest.network.Managers.RL.exploitation_rl_network import ExploitationAgent
from Network_Pentest.network.utils.low_hanging_fruits import detect_low_hanging_fruits
from back_end.database import DatabaseManager
from back_end.utils.colored_logger import get_logger

logger = get_logger(__name__, component="VULN_AI_AGENT", region="AI")

#threshold levels for system classification
THRESHOLD_LEVEL_ONE = 0.3
THRESHOLD_LEVEL_TWO = 0.7


class VulnerabilityAIAgent:
    def __init__(self, db: DatabaseManager, model_path: str = None, device: str = "cuda" if torch.cuda.is_available() else "cpu" ):
        self.db = db
        self.device = torch.device(device)
        self.encoder = VulnerabilityManagerGNN(in_channels=11, hidden=64, num_classes=2).to(self.device)
        if model_path:
            self.encoder.load_state_dict(torch.load(model_path, map_location=self.device,weights_only=False))
        self.encoder.train()  # for training
        self.policy = ExploitationAgent(db, input_dim=4, hidden=64, device=device)  # aligned input_dim

    # ----------------------------------------------------------------------
    #  INTERNAL: generic graph builder (re-used in several methods)
    # ----------------------------------------------------------------------
    def _build_graph_tensors(self, scan_results: List[Dict[str, Any]]) -> Data:
        """
        Build a PyG Data graph (x, edge_index, batch) from a list of host records.
        Each host -> one node with 11-dim features.
        Edges added if hosts share a service name.
        """
        nodes: List[List[float]] = []
        edges = set()

        for idx, r in enumerate(scan_results):
            vulns = r.get("vulnerabilities", []) or []
            services = r.get("services", []) or []

            num_vulns = len(vulns)
            max_cvss = max([v.get("cvss", 0.0) for v in vulns], default=0.0)
            has_exploit = 1 if any(v.get("exploit", False) for v in vulns) else 0
            num_services = len(services)
            risk = (r.get("risk") or "").lower()
            r_high, r_med, r_low = int(risk == "high"), int(risk == "medium"), int(risk == "low")

            nodes.append([
                float(num_vulns),
                float(max_cvss),
                float(has_exploit),
                float(num_services),
                float(r_high),
                float(r_med),
                float(r_low),
                0.0,                    # subnet_deg placeholder (not known here)
                float(num_vulns),
                float(num_services),
                0.0                     # observed_connections placeholder
            ])

            # edges based on shared service names
            for j in range(idx):
                s1 = [s.get("name") for s in services if s.get("name")]
                s2 = [s.get("name") for s in (scan_results[j].get("services", []) or []) if s.get("name")]
                if any(x in s2 for x in s1):
                    edges.add((idx, j))
                    edges.add((j, idx))

        x = torch.tensor(nodes, dtype=torch.float, device=self.device)
        if edges:
            edge_index = torch.tensor(list(edges), dtype=torch.long, device=self.device).t().contiguous()
        else:
            edge_index = torch.empty((2, 0), dtype=torch.long, device=self.device)

        batch = torch.zeros(len(nodes), dtype=torch.long, device=self.device)
        return Data(x=x, edge_index=edge_index, batch=batch)

    # ----------------------------------------------------------------------
    #  ORIGINAL: network-level exploitability via GNN
    # ----------------------------------------------------------------------
    def build_graph_features(self, scan_results: List[Dict[str, Any]], training: bool = False) -> np.ndarray:
        data = self._build_graph_tensors(scan_results)
        with torch.set_grad_enabled(training):
            logits = self.encoder(data.x, data.edge_index, data.batch)
            probs = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()
        return probs

    async def scan_target(self, target_ip: str, hints: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Main entrypoint used by ReconScanner.
        - Takes one IP.
        - Uses the recon result (passed through hints if needed).
        - Performs vuln scoring, defense scoring, exploitability prediction.
        - Returns a complete host summary.
        """
        try:
            host_record = hints.get("host_record") if isinstance(hints, dict) else None
            if host_record is None:
                return {
                    "ip": target_ip,
                    "error": "host_record_missing",
                }

            # Extract surface names
            surfaces = set()
            for s in host_record.get("services", []) or []:
                name = (s.get("name") or "").lower()
                if name:
                    surfaces.add(name)

            surface_reports = []
            for surf in surfaces:
                rep = await self.analyze_surface(
                    ip=target_ip,
                    surface=surf,
                    host_record=host_record,
                )
                surface_reports.append(rep)

            # Global host metrics
            defense_state = self.compute_defense_state(host_record)
            vuln_info = self.compute_vuln_score({}, host_record)
            exploit_probs = self.build_graph_features([host_record], training=False)
            exploit_prob = float(exploit_probs[0])

            return {
                "ip": target_ip,
                "surfaces": surface_reports,
                "defense_state": defense_state,
                "score": vuln_info["score"],
                "level": vuln_info["level"],
                "lh_tags": vuln_info["lh_tags"],
                "exploit_prob": exploit_prob,
            }

        except Exception as e:
            return {"ip": target_ip, "error": str(e)}


    # ----------------------------------------------------------------------
    #  DEFENSE STATE ESTIMATION (per host)
    # ----------------------------------------------------------------------
    def compute_defense_state(self, host_record: Dict[str, Any]) -> Dict[str, Any]:
        """
        Try to capture the *defense posture* of the host based on:
          - how exposed it is (how many services, which ones)
          - presence of critical vulns with exploits
          - any hardening signs (few exposed ports, preferring TLS, etc.)

        Returns e.g.:
          {
            "exposure_score": 0.8,
            "hardening_flags": [...],
            "misconfig_flags": [...],
            "defense_level": "weak"|"medium"|"strong"
          }
        """
        services = host_record.get("services", []) or []
        vulns = host_record.get("vulnerabilities", []) or []

        service_names = [(s.get("name") or "").lower() for s in services]
        n_services = len(services)

        # crude exposure score: how many services / 10 (clipped to [0,1])
        exposure_score = min(1.0, n_services / 10.0)

        hardening_flags = []
        misconfig_flags = []

        # Hardening hints
        # - only SSH is exposed
        if n_services == 1 and any("ssh" in n for n in service_names):
            hardening_flags.append("ssh_only_access")

        # - prefers TLS (https but no http)
        if any("https" in n for n in service_names) and not any(
                "http" in n and "https" not in n for n in service_names):
            hardening_flags.append("prefers_tls")

        # Misconfig / weak hints
        if any("telnet" in n for n in service_names):
            misconfig_flags.append("telnet_exposed")
        if any("ftp" in n for n in service_names):
            misconfig_flags.append("ftp_exposed")

        # Vuln-based hints
        has_known_exploit = any(v.get("exploit", False) for v in vulns)
        max_cvss = max([v.get("cvss", 0.0) for v in vulns], default=0.0)

        if has_known_exploit:
            misconfig_flags.append("known_exploit_available")
        if max_cvss >= 9.0:
            misconfig_flags.append("critical_vuln_exposed")

        # derive defense_level
        # weaker if high exposure & misconfigs; stronger if low exposure & some hardening
        base = 1.0 - exposure_score  # high exposure => low base
        base += 0.15 * len(hardening_flags)
        base -= 0.15 * len(misconfig_flags)
        base = max(0.0, min(1.0, base))

        if base < 0.33:
            defense_level = "weak"
        elif base < 0.66:
            defense_level = "medium"
        else:
            defense_level = "strong"

        return {
            "exposure_score": float(exposure_score),
            "hardening_flags": hardening_flags,
            "misconfig_flags": misconfig_flags,
            "defense_level": defense_level,
        }

    # ----------------------------------------------------------------------
    #  VULN SCORE + LEVEL (existing logic)
    # ----------------------------------------------------------------------
    def compute_vuln_score(self, vuln_record: Dict[str, Any], host_record: Dict[str, Any]) -> Dict[str, Any]:
        """
        Compute a vulnerability score based on:
          - Low-hanging fruits (easy indicators)
          - Service exposure
          - Average CVSS severity
        Returns dict with fields: {'score': float, 'level': str, 'lh_tags': list}
        """
        lh_tags, lhf_score = detect_low_hanging_fruits(vuln_record, host_record, db=self.db)
        # lhf_score returned in [0,1], lower = easier -> invert for weighting
        f_lhf = 1.0 - lhf_score

        # risky services fraction (HTTP, SMB, RDP, SSH etc.)
        risky_services = {"http", "https", "rdp", "smb", "ftp", "ssh", "telnet"}
        services = [s.get("name", "").lower() for s in host_record.get("services", [])]
        risky_count = sum(1 for s in services if s in risky_services)
        f_services = min(1.0, risky_count / max(1, len(services)))

        # average CVSS scaled 0→1
        vulns = host_record.get("vulnerabilities", [])
        avg_cvss = np.mean([v.get("cvss", 0.0) for v in vulns]) if vulns else 0.0
        f_cvss = avg_cvss / 10.0

        # weighted sum (lower = easier target)
        score = W_LHF * (1 - f_lhf) + W_SERVICES * f_services + W_CVSS * f_cvss
        score = max(0.0, min(1.0, score))

        level = self.assign_host_level(score)
        return {"score": score, "level": level, "lh_tags": lh_tags}

    @staticmethod
    def assign_host_level(score: float) -> str:
        if score <= THRESHOLD_LEVEL_ONE:
            return "level_1"
        elif score <= THRESHOLD_LEVEL_TWO:
            return "level_2"
        else:
            return "level_3"

    # ----------------------------------------------------------------------
    #  HIGH-LEVEL SURFACE ANALYSIS (per IP/surface)
    # ----------------------------------------------------------------------
    async def analyze_surface(self, ip: str, surface: str, host_record: dict) -> dict:
        """
        Analyze one attack surface for a host.
        - host_record must be provided by the pipeline (no DB lookup here).
        - Returns a structured dict with:
            score, level, exploit_prob, lh_tags, policy_action,
            defense_state (NEW), host_record_ref
        """
        try:
            if host_record is None:
                return {"ip": ip, "surface": surface, "error": "host_record_required"}

            services = host_record.get("services", []) or []
            vulns = host_record.get("vulnerabilities", []) or []

            # simple service-match: keep service entries whose name contains the surface token
            relevant_services = [s for s in services if surface.lower() in (s.get("name", "").lower())]

            # Small context list for GNN (existing format)
            context = [{
                "ip": ip,
                "services": relevant_services,
                "vulnerabilities": vulns,
                "risk": host_record.get("risk", "unknown")
            }]

            # GNN inference
            probs = self.build_graph_features(context, training=False)
            try:
                exploit_prob = float(probs[0]) if isinstance(probs, (list, tuple)) else float(
                    np.asarray(probs).ravel()[-1]
                )
            except Exception:
                exploit_prob = float(np.asarray(probs).ravel()[-1])

            # Vuln score + level
            vuln_info = self.compute_vuln_score({"surface": surface}, host_record)
            score = vuln_info.get("score", 0.0)
            level = vuln_info.get("level", "level_1")
            lh_tags = vuln_info.get("lh_tags", [])

            # NEW: defense state
            defense_state = self.compute_defense_state(host_record)

            # Optional: RL policy action
            policy_action = None
            try:
                if hasattr(self.policy, "select_action_for_state"):
                    state_vec = [score, exploit_prob, len(relevant_services), len(vulns)]
                    policy_action = self.policy.select_action_for_state(state_vec)
            except Exception:
                policy_action = None

            result = {
                "ip": ip,
                "surface": surface,
                "exploit_prob": exploit_prob,
                "score": score,
                "level": level,
                "lh_tags": lh_tags,
                "defense_state": defense_state,
                "policy_action": policy_action,
                "host_record_ref": {"ip": ip},
            }
            logger.info(
                f"[VAI] analyze_surface {ip}/{surface} -> "
                f"score={score:.3f} level={level} prob={exploit_prob:.3f} defense={defense_state['defense_level']}"
            )
            return result

        except Exception as e:
            logger.exception("[VAI] analyze_surface failed: %s", e)
            return {"ip": ip, "surface": surface, "error": str(e)}

    # ----------------------------------------------------------------------
    #  GROUP HOSTS INTO LEVEL_1/2/3 + DEFENSE SNAPSHOT FOR HONEYNET
    # ----------------------------------------------------------------------
    def classify_hosts_for_levels(self, hosts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Take raw host records (e.g. recon+vuln scan results) and classify them
        into level_1, level_2, level_3 for honeynet building / prioritization.

        Returns:
          {
            "level_1": [ {ip, score, defense_state, ...}, ... ],
            "level_2": [...],
            "level_3": [...],
            "counts": {"level_1": n1, "level_2": n2, "level_3": n3}
          }
        """
        buckets = {"level_1": [], "level_2": [], "level_3": []}

        for h in hosts:
            ip = h.get("ip", "unknown")
            vuln_info = self.compute_vuln_score({}, h)
            defense_state = self.compute_defense_state(h)

            # optional: network-level exploitability (single-host context)
            try:
                exploit_probs = self.build_graph_features([h], training=False)
                exploit_prob = float(np.asarray(exploit_probs).ravel()[-1])
            except Exception:
                exploit_prob = 0.0

            entry = {
                "ip": ip,
                "score": vuln_info["score"],
                "level": vuln_info["level"],
                "lh_tags": vuln_info["lh_tags"],
                "defense_state": defense_state,
                "exploit_prob": exploit_prob,
            }
            buckets[vuln_info["level"]].append(entry)

        counts = {
            "level_1": len(buckets["level_1"]),
            "level_2": len(buckets["level_2"]),
            "level_3": len(buckets["level_3"]),
        }
        logger.info(f"[VAI] classify_hosts_for_levels -> counts={counts}")
        return {"level_1": buckets["level_1"], "level_2": buckets["level_2"], "level_3": buckets["level_3"], "counts": counts}

    # ----------------------------------------------------------------------
    #  BUILD ONE GRAPH PER LEVEL (for Firecracker / DeceptionLearning)
    # ----------------------------------------------------------------------
    def build_level_graphs(self, hosts: List[Dict[str, Any]]) -> Dict[str, Data]:
        """
        Split hosts into level_1/2/3 and build a PyG graph per level
        using the same 11-dim feature representation.

        Returns:
          {
            "level_1": Data(...) or None,
            "level_2": Data(...) or None,
            "level_3": Data(...) or None,
        }
        """
        # First, bucket hosts by level
        classified = self.classify_hosts_for_levels(hosts)
        graphs: Dict[str, Data] = {}

        for level_key in ("level_1", "level_2", "level_3"):
            entries = classified[level_key]
            if not entries:
                graphs[level_key] = None
                continue

            # Recover host_record structures based on IP (your pipeline can pass them directly instead)
            # Here we assume `hosts` list is the richer structure, entries only carry summary.
            level_hosts = []
            ips_for_level = {e["ip"] for e in entries}
            for h in hosts:
                if h.get("ip") in ips_for_level:
                    level_hosts.append(h)

            if not level_hosts:
                graphs[level_key] = None
                continue

            graphs[level_key] = self._build_graph_tensors(level_hosts)
            logger.info(f"[VAI] build_level_graphs -> {level_key} has {len(level_hosts)} nodes")

        return graphs

    # ----------------------------------------------------------------------
    #  LEVEL ADJUSTMENT WHEN EXP FAILS IN PROD
    # ----------------------------------------------------------------------
    def adjust_level_after_failure(self, current_level: str) -> str:
        """
        Simple rule: if exploitation fails in production despite simulation success,
        we treat the host as 'harder' and bump its level upwards.

        level_1 -> level_2
        level_2 -> level_3
        level_3 -> level_3 (max)
        """
        if current_level == "level_1":
            return "level_2"
        if current_level == "level_2":
            return "level_3"
        return "level_3"


# import torch
# import torch.nn.functional as F
# import numpy as np
# from typing import List, Dict, Any
# from torch_geometric.data import Data
#
# from Network_Pentest.network.Brain.gnn_models import VulnerabilityManagerGNN
# from Network_Pentest.network.Brain.hyperparams import W_LHF, W_SERVICES, W_CVSS
# from Network_Pentest.network.Managers.RL.exploitation_rl_network import ExploitationAgent
# from Network_Pentest.network.utils.low_hanging_fruits import detect_low_hanging_fruits
# from back_end.database import DatabaseManager
# from back_end.utils.colored_logger import get_logger
#
# logger = get_logger(__name__, component="VULN_AI_AGENT", region="AI")
# THRESHOLD_LEVEL_ONE=0.3
# THRESHOLD_LEVEL_TWO=0.7
# class VulnerabilityAIAgent:
#     def __init__(self, db: DatabaseManager, model_path: str = None, device: str = "cuda"):
#         self.db = db
#         self.device = torch.device(device)
#         self.encoder = VulnerabilityManagerGNN(in_channels=11, hidden=64, num_classes=2).to(self.device)
#         if model_path:
#             self.encoder.load_state_dict(torch.load(model_path, map_location=self.device))
#         self.encoder.train()  # for training
#         self.policy = ExploitationAgent(db, input_dim=4, hidden=64, device=device)  # aligned input_dim
#
#     def build_graph_features(self, scan_results: List[Dict[str, Any]], training: bool = False) -> np.ndarray:
#         nodes, edges = [], set()
#         for idx, r in enumerate(scan_results):
#             vulns = r.get("vulnerabilities", [])
#             services = r.get("services", [])
#             num_vulns = len(vulns)
#             max_cvss = max([v.get("cvss", 0.0) for v in vulns], default=0.0)
#             has_exploit = 1 if any(v.get("exploit", False) for v in vulns) else 0
#             num_services = len(services)
#             risk = r.get("risk", "").lower()
#             r_high, r_med, r_low = int(risk=="high"), int(risk=="medium"), int(risk=="low")
#
#             nodes.append([
#                 float(num_vulns), float(max_cvss), float(has_exploit), float(num_services),
#                 float(r_high), float(r_med), float(r_low),
#                 0.0, float(num_vulns), float(num_services), 0.0
#             ])
#
#             # simple edge based on shared services
#             for j in range(idx):
#                 s1 = [s.get("name") for s in r.get("services", []) if s.get("name")]
#                 s2 = [s.get("name") for s in scan_results[j].get("services", []) if s.get("name")]
#                 if any(x in s2 for x in s1):
#                     edges.add((idx, j)); edges.add((j, idx))
#
#         x = torch.tensor(nodes, dtype=torch.float, device=self.device)
#         edge_index = torch.tensor(list(edges), dtype=torch.long, device=self.device).t().contiguous() if edges else torch.empty((2,0),dtype=torch.long,device=self.device)
#         data = Data(x=x, edge_index=edge_index, batch=torch.zeros(len(nodes),dtype=torch.long,device=self.device))
#
#         with torch.set_grad_enabled(training):
#             logits = self.encoder(data.x, data.edge_index, data.batch)
#             probs = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()
#         return probs
#
#     def compute_vuln_score(self, vuln_record: Dict[str, Any], host_record: Dict[str, Any]) -> Dict[str, Any]:
#         """
#         Compute a vulnerability score based on:
#           - Low-hanging fruits (easy indicators)
#           - Service exposure
#           - Average CVSS severity
#         Returns dict with fields: {'score': float, 'level': str, 'lh_tags': list}
#         """
#         lh_tags, lhf_score = detect_low_hanging_fruits(vuln_record, host_record, db=self.db)
#         # lhf_score returned in [0,1], lower = easier -> invert for weighting
#         f_lhf = 1.0 - lhf_score
#
#         # risky services fraction (HTTP, SMB, RDP, SSH etc.)
#         risky_services = {"http", "https", "rdp", "smb", "ftp", "ssh", "telnet"}
#         services = [s.get("name", "").lower() for s in host_record.get("services", [])]
#         risky_count = sum(1 for s in services if s in risky_services)
#         f_services = min(1.0, risky_count / max(1, len(services)))
#
#         # average CVSS scaled 0→1
#         vulns = host_record.get("vulnerabilities", [])
#         avg_cvss = np.mean([v.get("cvss", 0.0) for v in vulns]) if vulns else 0.0
#         f_cvss = avg_cvss / 10.0
#
#         # weighted sum (lower = easier target)
#         score = W_LHF * (1 - f_lhf) + W_SERVICES * f_services + W_CVSS * f_cvss
#         score = max(0.0, min(1.0, score))
#
#         level = self.assign_host_level(score)
#
#         return {"score": score, "level": level, "lh_tags": lh_tags}
#
#         # ------------------------------------------------------------
#         # NEW: level assignment helper
#         # ------------------------------------------------------------
#     @staticmethod
#     def assign_host_level(score: float) -> str:
#         if score <= THRESHOLD_LEVEL_ONE:
#             return "level_1"
#         elif score <= THRESHOLD_LEVEL_TWO:
#             return "level_2"
#         else:
#             return "level_3"
#
#
# # inside VulnerabilityAIAgent
#
#     async def analyze_surface(self, ip: str, surface: str, host_record: dict) -> dict:
#         """
#         Analyze one attack surface for a host.
#         - host_record must be provided by the pipeline (no DB lookup here).
#         - Returns a structured dict with score, level, exploit_prob, lh_tags, and optional policy_action.
#         """
#         try:
#             if host_record is None:
#                 return {"ip": ip, "surface": surface, "error": "host_record_required"}
#
#             # Prepare filtered host view for this surface
#             services = host_record.get("services", []) or []
#             vulns = host_record.get("vulnerabilities", []) or []
#             # simple service-match: keep service entries whose name contains the surface token
#             relevant_services = [s for s in services if surface.lower() in (s.get("name","").lower())]
#
#             # Small context list for GNN (your existing format)
#             context = [{
#                 "ip": ip,
#                 "services": relevant_services,
#                 "vulnerabilities": vulns,
#                 "risk": host_record.get("risk", "unknown")
#             }]
#
#             # GNN inference (no-grad by default)
#             probs = self.build_graph_features(context, training=False)
#             # build_graph_features returns numpy array; interpret safely:
#             try:
#                 exploit_prob = float(probs[0]) if isinstance(probs, (list, tuple)) else float(np.asarray(probs).ravel()[-1])
#             except Exception:
#                 exploit_prob = float(np.asarray(probs).ravel()[-1])
#
#             # Compute vulnerability score + level using existing method
#             vuln_info = self.compute_vuln_score({"surface": surface}, host_record)
#             score = vuln_info.get("score", 0.0)
#             level = vuln_info.get("level", "level_1")
#             lh_tags = vuln_info.get("lh_tags", [])
#
#             # Ask RL policy for recommended action (adapt to your policy API)
#             # If your ExploitationAgent exposes select_action(scan_result, candidates),
#             # you may instead call a helper that maps this small state to an action index.
#             policy_action = None
#             try:
#                 # example: if you have a select_action_for_state helper; if not, skip
#                 if hasattr(self.policy, "select_action_for_state"):
#                     state_vec = [score, exploit_prob, len(relevant_services), len(vulns)]
#                     policy_action = self.policy.select_action_for_state(state_vec)
#             except Exception:
#                 policy_action = None
#
#             result = {
#                 "ip": ip,
#                 "surface": surface,
#                 "exploit_prob": exploit_prob,
#                 "score": score,
#                 "level": level,
#                 "lh_tags": lh_tags,
#                 "policy_action": policy_action,
#                 "host_record_ref": {"ip": ip}  # keep as pointer for audits
#             }
#             logger.info(f"[VAI] analyze_surface {ip}/{surface} -> score={score:.3f} prob={exploit_prob:.3f}")
#             return result
#
#         except Exception as e:
#             logger.exception("[VAI] analyze_surface failed: %s", e)
#             return {"ip": ip, "surface": surface, "error": str(e)}
